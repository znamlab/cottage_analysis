{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d7cce3",
   "metadata": {},
   "source": [
    "# Visual stimulation monitor synchronisation\n",
    "\n",
    "Bonsai controls visual display and saves a log everytime it asks for a frame to be rendered. However there is an unknown delay (~2 frames) between this render frame event and the actual display time. Furthermore some frames are skipped.\n",
    "\n",
    "To figure out which frame is displayed when we put a photodiode in front of the monitor and display a pseudo-random sequence of alternating grey value. \n",
    "\n",
    "This notebook show how we use the frame logger and the photodiode signal to determine exact frame identity at each point of time. It has 3 main steps\n",
    "\n",
    "**1. Detect frames on photodiode signal**\n",
    "\n",
    "**2. Cross-correlate frame with sequence to find expected lag**\n",
    "\n",
    "**3. Match cross correlation results to frame logger**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64fe5d3",
   "metadata": {},
   "source": [
    "\n",
    "###Â Load example data\n",
    "\n",
    "This next section just loads one example recording to be used for the rest of the notebook.\n",
    "\n",
    "Define the session we want:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90e896",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c3cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"hey2_3d-vision_foodres_20220101\"\n",
    "MOUSE = \"PZAH8.2i\"\n",
    "SESSION = \"S20230209\"\n",
    "RECORDING = \"R174123_SpheresPermTubeRewardPlayback\"\n",
    "PROTOCOL = \"SpheresPermTubeReward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08035942",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"blota_onix_pilote\"\n",
    "MOUSE = \"BRYA142.5d\"\n",
    "SESSION = \"S20231005\"\n",
    "PROTOCOL = \"SpheresPermTubeReward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_onix = True\n",
    "visstim_in_harp = False\n",
    "photodiode_protocol = 5\n",
    "sync_kwargs = dict(frame_detection_height=0.05)\n",
    "conflicts = \"overwrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flexiznam as flz\n",
    "from cottage_analysis.preprocessing.synchronisation import find_monitor_frames\n",
    "\n",
    "flm_sess = flz.get_flexilims_session(PROJECT)\n",
    "sess = flz.get_entity(flexilims_session=flm_sess, name=f\"{MOUSE}_{SESSION}\")\n",
    "harp_recording = flz.get_entity(\n",
    "    origin_id=sess.id,\n",
    "    datatype=\"recording\",\n",
    "    query_key=\"protocol\",\n",
    "    query_value=\"harpdata\",\n",
    "    flexilims_session=flm_sess,\n",
    ")\n",
    "if visstim_in_harp:\n",
    "    vis_stim_recording = harp_recording\n",
    "else:\n",
    "    vis_stim_recording = flz.get_entity(\n",
    "        origin_id=sess.id,\n",
    "        datatype=\"recording\",\n",
    "        query_key=\"protocol\",\n",
    "        query_value=PROTOCOL,\n",
    "        flexilims_session=flm_sess,\n",
    "    )\n",
    "if use_onix:\n",
    "    onix_recording = flz.get_entity(\n",
    "        origin_id=sess.id,\n",
    "        datatype=\"recording\",\n",
    "        query_key=\"protocol\",\n",
    "        query_value=\"onix\",\n",
    "        flexilims_session=flm_sess,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf24fdb",
   "metadata": {},
   "source": [
    "Load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63be585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "from cottage_analysis.utilities.misc import get_str_or_recording\n",
    "from cottage_analysis.io_module.harp import load_harpmessage\n",
    "from cottage_analysis.io_module import onix as onix_io\n",
    "from cottage_analysis.io_module.visstim import get_frame_log\n",
    "from cottage_analysis.io_module.spikes import load_kilosort_folder\n",
    "from cottage_analysis.preprocessing import onix as onix_prepro\n",
    "\n",
    "\n",
    "project = None\n",
    "assert conflicts in [\"skip\", \"overwrite\", \"abort\"]\n",
    "if flexilims_session is None:\n",
    "    assert project is not None, \"project must be provided if flexilims_session is None\"\n",
    "    flexilims_session = flz.get_flexilims_session(project_id=project)\n",
    "\n",
    "vis_stim_recording = get_str_or_recording(vis_stim_recording, flexilims_session)\n",
    "if harp_recording is None:\n",
    "    harp_recording = vis_stim_recording\n",
    "else:\n",
    "    harp_recording = get_str_or_recording(harp_recording, flexilims_session)\n",
    "    onix_recording = get_str_or_recording(onix_recording, flexilims_session)\n",
    "\n",
    "assert harp_recording is not None, \"harp_recording must be provided\"\n",
    "assert onix_recording is not None, \"onix_recording must be provided\"\n",
    "assert vis_stim_recording is not None, \"vis_stim_recording must be provided\"\n",
    "# Get frame log\n",
    "frame_log = get_frame_log(\n",
    "    flexilims_session,\n",
    "    harp_recording=harp_recording,\n",
    "    vis_stim_recording=vis_stim_recording,\n",
    ")\n",
    "\n",
    "# Create output and reload\n",
    "monitor_frames_ds = flz.Dataset.from_origin(\n",
    "    origin_id=vis_stim_recording[\"id\"],\n",
    "    dataset_type=\"monitor_frames\",\n",
    "    flexilims_session=flexilims_session,\n",
    "    conflicts=conflicts,\n",
    ")\n",
    "if monitor_frames_ds.flexilims_status() != \"not online\" and conflicts == \"skip\":\n",
    "    print(\"Loading existing monitor frames...\")\n",
    "    monitor_frames_ds = pd.read_pickle(monitor_frames_ds.path_full)\n",
    "    print(\"Done.\")\n",
    "\n",
    "monitor_frames_ds.path = monitor_frames_ds.path.parent / f\"monitor_frames_df.pickle\"\n",
    "\n",
    "# Get photodiode\n",
    "raw = flz.get_data_root(\"raw\", flexilims_session=flexilims_session)\n",
    "harp_message, harp_ds = load_harpmessage(\n",
    "    recording=harp_recording,\n",
    "    flexilims_session=flexilims_session,\n",
    "    conflicts=\"skip\",\n",
    ")\n",
    "if onix_recording is None:\n",
    "    # get the photodiode from harp directly\n",
    "    photodiode = harp_message[\"photodiode\"]\n",
    "    analog_time = harp_message[\"analog_time\"]\n",
    "else:\n",
    "    onix_ds = flz.get_datasets(\n",
    "        flexilims_session=flexilims_session,\n",
    "        origin_name=onix_recording.name,\n",
    "        dataset_type=\"onix\",\n",
    "        allow_multiple=False,\n",
    "    )\n",
    "    breakout = onix_io.load_breakout(raw / onix_recording.path)\n",
    "    onix_data = onix_prepro.preprocess_onix_recording(\n",
    "        dict(breakout_data=breakout), harp_message=harp_message\n",
    "    )\n",
    "    if \"aio_mapping\" in onix_ds.extra_attributes:\n",
    "        ch_pd = onix_ds.extra_attributes[\"aio_mapping\"][\"photodiode\"]\n",
    "    else:\n",
    "        ch_pd = onix_prepro.ANALOG_INPUTS.index(\"photodiode\")\n",
    "    photodiode = onix_data[\"breakout_data\"][\"aio\"][ch_pd, :]\n",
    "    analog_time = onix_data[\"onix2harp\"](onix_data[\"breakout_data\"][\"aio-clock\"])\n",
    "    # to make it faster, decimate the photodiode signal\n",
    "    if False:  ### X TEMP TO DEBUG\n",
    "        photodiode = scipy.signal.decimate(photodiode, 5)\n",
    "        analog_time = analog_time[::5]\n",
    "\n",
    "\n",
    "recording_duration = frame_log.HarpTime.values[-1] - frame_log.HarpTime.values[0]\n",
    "frame_rate = 1 / frame_log.HarpTime.diff().median()\n",
    "print(f\"Recording is {recording_duration:.0f} s long.\")\n",
    "print(f\"Frame rate is {frame_rate:.0f} Hz.\")\n",
    "\n",
    "# Get frames from photodiode trace, depending on the photodiode protocol is 2 or 5\n",
    "diagnostics_folder = monitor_frames_ds.path_full.parent / \"diagnostics\" / \"frame_sync\"\n",
    "diagnostics_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0880150",
   "metadata": {},
   "outputs": [],
   "source": [
    "onix_ds = flz.get_datasets(\n",
    "    origin_id=onix_recording[\"id\"],\n",
    "    dataset_type=\"onix\",\n",
    "    flexilims_session=flexilims_session,\n",
    "    allow_multiple=False,\n",
    ")\n",
    "\n",
    "from cottage_analysis.io_module.onix import load_breakout\n",
    "\n",
    "breakout = load_breakout(onix_ds.path_full, num_ai_chan=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a43bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visstim_ds.extra_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "visstim_ds = flz.get_datasets(\n",
    "    origin_id=vis_stim_recording[\"id\"],\n",
    "    dataset_type=\"visstim\",\n",
    "    flexilims_session=flexilims_session,\n",
    "    allow_multiple=False,\n",
    ")\n",
    "csvs = visstim_ds.extra_attributes[\"csv_files\"]\n",
    "param_log = pd.read_csv(visstim_ds.path_full / csvs[\"ParamLog\"])\n",
    "frame_log = pd.read_csv(visstim_ds.path_full / csvs[\"FrameLog\"])\n",
    "reward_log = pd.read_csv(visstim_ds.path_full / csvs[\"RewardLog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30922b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c53e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "b, e = 2.012e7 + np.array([2618.7, 2619.25])\n",
    "v = (harp_message[\"analog_time\"] < e) & (harp_message[\"analog_time\"] > b)\n",
    "ax.plot(harp_message[\"analog_time\"][v], harp_message[\"photodiode\"][v])\n",
    "v = (analog_time < e) & (analog_time > b)\n",
    "ax1 = plt.subplot(2, 1, 2, sharex=ax)\n",
    "m = photodiode[v].mean()\n",
    "M = photodiode[v].max()\n",
    "normed = (photodiode[v] - m) / (M - m)\n",
    "ax1.plot(analog_time[v], normed)\n",
    "v = frame_log.HarpTime < e\n",
    "ax1.plot(\n",
    "    frame_log.HarpTime[v] - 30e-3,\n",
    "    frame_log[\"PhotoQuadColor\"][v],\n",
    "    drawstyle=\"steps-post\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a5f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cottage_analysis.utilities import continuous_data_analysis as cda\n",
    "\n",
    "sampling = 1 / np.diff(analog_time).mean()\n",
    "filt_pd = cda.filter(\n",
    "    photodiode, sampling, lowcut=None, highcut=700, design=\"butter\", axis=-1\n",
    ")\n",
    "v = (analog_time < e) & (analog_time > b)\n",
    "plt.figure(figsize=(20, 2))\n",
    "plt.plot(analog_time[v], filt_pd[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd2b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_pd = np.array(filt_pd, dtype=float)\n",
    "normed_pd -= np.quantile(normed_pd, 0.01)\n",
    "normed_pd /= np.quantile(normed_pd, 0.99)\n",
    "\n",
    "valid = analog_time < frame_log.HarpTime.values[-1]\n",
    "from cottage_analysis.preprocessing.find_frames import *\n",
    "\n",
    "# First step: Frame detection\n",
    "time_column = \"HarpTime\"\n",
    "frame_detection_height = 0.05\n",
    "do_plot = True\n",
    "save_folder = None\n",
    "verbose = True\n",
    "ignore_errors = False\n",
    "last_frame_delay = 100\n",
    "frames_df, db_dict, figs = create_frame_df(\n",
    "    frame_log=frame_log,\n",
    "    photodiode_time=analog_time[valid],\n",
    "    photodiode_signal=normed_pd[valid],\n",
    "    time_column=time_column,\n",
    "    frame_rate=frame_rate,\n",
    "    height=frame_detection_height,\n",
    "    do_plot=do_plot,\n",
    "    verbose=verbose,\n",
    "    debug=True,\n",
    "    save_folder=save_folder,\n",
    "    ignore_errors=ignore_errors,\n",
    "    last_frame_delay=last_frame_delay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ampl = normed_pd[valid][frames_df.peak_sample.values]\n",
    "amp_t = frames_df.peak_time.values\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "_ = ax.hist(ampl, bins=1000)\n",
    "borders = [-1, 0.04, 0.1, 0.19, 0.4, 1.1]\n",
    "for b in borders[1:-1]:\n",
    "    ax.axvline(b, color=\"grey\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f350706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_bined = (np.digitize(ampl, borders)).astype(float) - 1\n",
    "amp_bined /= amp_bined.max()\n",
    "v, c = np.unique(amp_bined, return_counts=True)\n",
    "c = c / len(amp_bined)\n",
    "{v0: np.round(c0, 3) for v0, c0 in zip(v, c)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = np.loadtxt(\n",
    "    \"/nemo/lab/znamenskiyp/home/shared/transfer/random_sequence_5values_alternate.csv\"\n",
    ")\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_seq = np.tile(seq, 1 + len(amp_bined) // len(seq))[: len(amp_bined)]\n",
    "\n",
    "\n",
    "def shift_diff(lag):\n",
    "    # shift the sequence by lag and compute the sum of square difference\n",
    "    shifted = np.roll(long_seq, lag)\n",
    "    return np.sum((amp_bined - shifted) ** 2)\n",
    "\n",
    "\n",
    "# minimize the sum of square difference\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "res = minimize(shift_diff, (0,), method=\"Nelder-Mead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "corr = signal.correlate(seq, amp_bined, mode=\"valid\")\n",
    "lags = signal.correlation_lags(len(amp_bined), len(seq), mode=\"valid\")\n",
    "\n",
    "plt.plot(lags, corr)\n",
    "max_lag = lags[np.argmax(corr)]\n",
    "print(max_lag)\n",
    "plt.xlim(max_lag - 100, max_lag + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(harp_recording.name)\n",
    "plt.plot(harp_message[\"analog_time\"], harp_message[\"photodiode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618df54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f4cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(10e6)\n",
    "for i in range(breakout[\"aio\"].shape[0]):\n",
    "    plt.plot(np.arange(n)[::10], breakout[\"aio\"][i, :n:10] + i * 10000, label=str(i))\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40725701",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(onix_recording.name)\n",
    "plt.plot(analog_time[::10] - analog_time[0], photodiode[::10])\n",
    "plt.xlim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae8f2f",
   "metadata": {},
   "source": [
    "# Normal usage\n",
    "\n",
    "This is using the main master function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cottage_analysis.preprocessing import find_frames\n",
    "\n",
    "processed = flz.get_data_root(\"processed\", project=PROJECT)\n",
    "params = dict(\n",
    "    time_column=\"HarpTime\",\n",
    "    sequence_column=\"PhotoQuadColor\",\n",
    "    num_frame_to_corr=6,\n",
    "    maxlag=3.0 / frame_rate,\n",
    "    expected_lag=2.0 / frame_rate,\n",
    "    frame_rate=frame_rate,\n",
    "    correlation_threshold=0.8,\n",
    "    relative_corr_thres=0.02,\n",
    "    frame_detection_height=0.1,\n",
    "    minimum_lag=1.0 / frame_rate,\n",
    "    do_plot=True,\n",
    "    save_folder=diagnostics_folder,\n",
    "    verbose=True,\n",
    "    ignore_errors=False,\n",
    ")\n",
    "if sync_kwargs is not None:\n",
    "    params.update(sync_kwargs)\n",
    "\n",
    "if False:\n",
    "    frames_df, db_dict = find_frames.sync_by_correlation(\n",
    "        frame_log,\n",
    "        analog_time,\n",
    "        photodiode,\n",
    "        **params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e05db",
   "metadata": {},
   "source": [
    "#Â Detailled description\n",
    "\n",
    "How does it work? The alignment is made in 3 steps:\n",
    "\n",
    "- detect frames\n",
    "- crosscorrelated with expected sequence\n",
    "- align results\n",
    "\n",
    "##Â Detect frames\n",
    "\n",
    "The frame detection is simple: filter a bit to smooth local extrema, `diff` to find fast changes and detect peaks on that `diff` trace. This should detect all frame borders. In between these borders, look for the `diff` minimum to find the frame peak (be it a maximum or a minium)\n",
    "\n",
    "Detection can be done independently using `detect_frame_onset`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c13854",
   "metadata": {},
   "source": [
    "## Create frame df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fecea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in params.items():\n",
    "    print(f\"{k}={v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c62fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column = \"HarpTime\"\n",
    "sequence_column = \"PhotoQuadColor\"\n",
    "num_frame_to_corr = 6\n",
    "maxlag = 0.02102399244904518\n",
    "expected_lag = 0.014015994966030123\n",
    "frame_rate = 142.6941151767892\n",
    "correlation_threshold = 0.8\n",
    "relative_corr_thres = 0.02\n",
    "frame_detection_height = 0.05\n",
    "minimum_lag = 0.007007997483015061\n",
    "do_plot = True\n",
    "save_folder = \"/camp/lab/znamenskiyp/home/shared/projects/blota_onix_pilote/BRYA142.5d/S20231010/R142857_SpheresPermTubeReward/diagnostics/frame_sync\"\n",
    "verbose = True\n",
    "ignore_errors = False\n",
    "debug = True\n",
    "last_frame_delay = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "photodiode_time = analog_time\n",
    "photodiode_signal = photodiode\n",
    "\n",
    "from cottage_analysis.preprocessing.find_frames import *\n",
    "\n",
    "pd_sampling = 1 / np.mean(np.diff(photodiode_time))\n",
    "\n",
    "# Normalise photodiode signal\n",
    "normed_pd = np.array(photodiode_signal, dtype=float)\n",
    "normed_pd -= np.quantile(normed_pd, 0.01)\n",
    "normed_pd /= np.quantile(normed_pd, 0.99)\n",
    "\n",
    "# First step: Frame detection\n",
    "frames_df, db_dict, figs = create_frame_df(\n",
    "    frame_log=frame_log,\n",
    "    photodiode_time=photodiode_time,\n",
    "    photodiode_signal=normed_pd,\n",
    "    time_column=time_column,\n",
    "    frame_rate=frame_rate,\n",
    "    height=frame_detection_height,\n",
    "    do_plot=do_plot,\n",
    "    verbose=verbose,\n",
    "    debug=debug,\n",
    "    save_folder=save_folder,\n",
    "    ignore_errors=ignore_errors,\n",
    "    last_frame_delay=last_frame_delay,\n",
    ")\n",
    "ndetected = len(frames_df)\n",
    "npresented = len(frame_log)\n",
    "if npresented < ndetected:\n",
    "    msg = (\n",
    "        f\"Detected more frames ({ndetected}) than presented ({npresented})\"\n",
    "        \"\\n Check create_frame_df parameters\"\n",
    "    )\n",
    "elif npresented > ndetected * 2:\n",
    "    msg = (\n",
    "        f\"Dropped more than half of the frames ({npresented - ndetected} dropped)\"\n",
    "        \"\\n Check create_frame_df parameters\"\n",
    "    )\n",
    "else:\n",
    "    msg = None\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if db_dict is not None:\n",
    "    db_dict[\"normed_pd\"] = normed_pd\n",
    "\n",
    "if figs is not None:\n",
    "    fig_dict = dict(frame_dection=figs)\n",
    "else:\n",
    "    fig_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6632c2c",
   "metadata": {},
   "source": [
    "##Â Do the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fe907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pd_sampling is None:\n",
    "    pd_sampling = 1 / np.mean(np.diff(photodiode_time))\n",
    "out_dict = {}\n",
    "frame_onsets = frames_df[\"onset_sample\"].values\n",
    "# make lags into samples\n",
    "maxlag_samples = int(np.round(maxlag * pd_sampling))\n",
    "expected_lag_samples = int(np.round(expected_lag * pd_sampling))\n",
    "\n",
    "# make an idealised photodiode signal\n",
    "ideal_time, ideal_seqi_trace, ideal_pd = ideal_photodiode(\n",
    "    frame_log,\n",
    "    sampling_rate=pd_sampling,\n",
    "    sequence_column=\"PhotoQuadColor\",\n",
    "    time_column=\"HarpTime\",\n",
    "    pad_frames=(maxlag + num_frame_to_corr) * 2,\n",
    "    highcut=150,\n",
    ")\n",
    "if debug:\n",
    "    out_dict[\"ideal_photodiode_trace\"] = ideal_pd\n",
    "    out_dict[\"ideal_time\"] = ideal_time\n",
    "    out_dict[\"ideal_seqi_trace\"] = ideal_seqi_trace\n",
    "\n",
    "# find the closest switch time for each frame according to computer time\n",
    "real_switch_times = frame_log[time_column].values\n",
    "closest_switch = np.clip(\n",
    "    real_switch_times.searchsorted(photodiode_time[frame_onsets]),\n",
    "    0,\n",
    "    len(real_switch_times) - 1,\n",
    ")\n",
    "frames_df[\"closest_frame_log_index\"] = closest_switch\n",
    "# and the corresponding ideal photodiode sample\n",
    "ideal_onset = frame_log[\"ideal_switch_samples\"].iloc[closest_switch].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "dt = np.diff(ideal_time).mean()\n",
    "p1 = np.searchsorted(ideal_time, [10, 20])\n",
    "p2 = np.searchsorted(photodiode_time - t0, [10, 20])\n",
    "out = signal.correlate(ideal_pd[p1[0] : p1[1]], normed_pd[p2[0] : p2[1]], mode=\"same\")\n",
    "lags = signal.correlation_lags(\n",
    "    ideal_pd[p1[0] : p1[1]].size, normed_pd[p2[0] : p2[1]].size, mode=\"same\"\n",
    ")\n",
    "plt.plot(lags * dt, out)\n",
    "plt.xlim(-0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f79104",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ideal_time, ideal_pd)\n",
    "plt.xlim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_onset_samples = ideal_onset\n",
    "ideal_photodiode_trace = ideal_pd\n",
    "ideal_frame_index = ideal_seqi_trace\n",
    "expected_lag = expected_lag_samples\n",
    "maxlag = maxlag_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_sampling = 1 / np.mean(np.diff(photodiode_time))\n",
    "\n",
    "# define the 3 correlation windows, bef, center and aft\n",
    "window = [\n",
    "    np.array([-1, 1]) * maxlag\n",
    "    + np.array(w * num_frame_to_corr / frame_rate * pd_sampling, dtype=\"int\")\n",
    "    for w in [np.array([-1, 0]), np.array([-0.5, 0.5]), np.array([0, 1])]\n",
    "]\n",
    "# for bef window, we add 1.5 frame to have half of the current frame included\n",
    "window[0] += int(1.5 / frame_rate * pd_sampling)\n",
    "# for center window, we shift by 0.5 frame to center\n",
    "window[1] += int(0.5 / frame_rate * pd_sampling)\n",
    "\n",
    "if verbose:\n",
    "    start = time.time()\n",
    "    print(\"Starting crosscorrelation\", flush=True)\n",
    "cc_mat = np.zeros((len(window), len(frame_onsets), maxlag * 2)) + np.nan\n",
    "eq_ind = np.zeros((len(window), len(frame_onsets), maxlag * 2), dtype=\"int\") - 1\n",
    "residuals = np.zeros((len(window), len(frame_onsets))) + np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d32bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iframe = 50000\n",
    "foi = frame_onsets[iframe]\n",
    "for iw, win in enumerate(window):\n",
    "    if (win[0] + foi) < 0:\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Frame %d at sample %d is too close from start of recording\"\n",
    "                % (iframe, foi)\n",
    "            )\n",
    "        crash\n",
    "    elif (win[1] + foi) > (len(photodiode_signal) - expected_lag):\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Frame %d at sample %d is too close from end of recording\"\n",
    "                % (iframe, foi)\n",
    "            )\n",
    "        crash\n",
    "    elif (win[0] + ideal_onset_samples[iframe] - expected_lag) < 0:\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Frame %d at sample %d is too close from start of ideal pd\"\n",
    "                % (iframe, foi)\n",
    "            )\n",
    "        crash\n",
    "    elif (win[1] + ideal_onset_samples[iframe] - expected_lag) > len(\n",
    "        ideal_photodiode_trace\n",
    "    ):\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Frame %d at sample %d is too close from end of ideal pd\"\n",
    "                % (iframe, foi)\n",
    "            )\n",
    "        crash\n",
    "    # ideal_pd is drifting, so we need to look for the closest computer time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5966a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_t = ideal_frame_index[slice(*win + ideal_onset_samples[iframe] - expected_lag)]\n",
    "# we want the middle \"maxlag * 2\" samples, which is where correlation can\n",
    "# be done\n",
    "eq_ind[iw, iframe] = id_t[int(len(id_t) / 2 - maxlag) : int(len(id_t) / 2 + maxlag)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d85b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = frame_log.HarpTime.values[0]\n",
    "plt.plot(analog_time[::5] - t0, normed_pd[::5])\n",
    "plt.axvline(frame_log.HarpTime.values[0] - t0, color=\"r\")\n",
    "plt.axvline(frame_log.HarpTime.values[-1] - t0, color=\"r\")\n",
    "plt.plot(ideal_time[::10], ideal_pd[::10])\n",
    "plt.xlim(-0.05, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(normed_pd[slice(*win * 15 + foi)])\n",
    "plt.plot(\n",
    "    ideal_photodiode_trace[\n",
    "        slice(*win * 15 + ideal_onset_samples[iframe] - expected_lag)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_skip = np.diff(frame_borders) > pd_sampling / frame_rate * 1.5\n",
    "frames_df = pd.DataFrame(\n",
    "    dict(\n",
    "        onset_sample=frame_borders[:-1],\n",
    "        offset_sample=frame_borders[1:],\n",
    "        peak_sample=peak_index,\n",
    "        include_skip=frame_skip,\n",
    "    )\n",
    ")\n",
    "frames_df[\"onset_time\"] = photodiode_time[frames_df.onset_sample]\n",
    "frames_df[\"offset_time\"] = photodiode_time[frames_df.offset_sample]\n",
    "frames_df[\"peak_time\"] = photodiode_time[frames_df.peak_sample]\n",
    "\n",
    "# check if frames are detected after the presentation is over\n",
    "after_last = frames_df[\"onset_time\"] >= frame_log[time_column].iloc[-1]\n",
    "print(f\"Framed detected after the presentation is over: {after_last.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_frames.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_presentation = frame_log[time_column].iloc[-1]\n",
    "last_frame = frames_df[\"onset_time\"].iloc[-1]\n",
    "print(f\"Presentation ends at {end_of_presentation:.2f} s\")\n",
    "print(f\"Last frame detected at {last_frame:.2f} s\")\n",
    "\n",
    "delay = frames_df[\"onset_time\"].iloc[-1] - frame_log[time_column].iloc[-1]\n",
    "print(f\"{after_last.sum()} frames detected after the last render time.\")\n",
    "\n",
    "b, e = photodiode_time.searchsorted([end_of_presentation - 1, last_frame + 20])\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_title(\"Recording ends before last frame detected\")\n",
    "ax.plot(\n",
    "    photodiode_time[b:e] - end_of_presentation,\n",
    "    photodiode_signal[b:e],\n",
    "    label=\"photodiode\",\n",
    ")\n",
    "ax.axvline(0, color=\"k\", label=\"end of presentation\")\n",
    "late_frames = frames_df[frames_df.onset_time > end_of_presentation]\n",
    "ax.scatter(\n",
    "    late_frames.peak_time - end_of_presentation,\n",
    "    photodiode_signal[late_frames.peak_sample],\n",
    "    label=\"late frame\",\n",
    "    color=\"purple\",\n",
    ")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"Time relative to end of presentation (s)\")\n",
    "ax.set_ylabel(\"Photodiode signal (a.u.)\")\n",
    "save_folder = None\n",
    "if save_folder is not None:\n",
    "    fig.savefig(Path(save_folder) / f\"presentation_end_issue.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e02230",
   "metadata": {},
   "source": [
    "You can get an example of detection using `plot_frame_detection_report`. This will give you `num_examples * 2` figures. Half of them are selected on random frames, half are centered around a frame drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_errors = True\n",
    "ndetected = len(frames_df)\n",
    "npresented = len(frame_log)\n",
    "if npresented < ndetected:\n",
    "    msg = (\n",
    "        f\"Detected more frames ({ndetected}) than presented ({npresented})\"\n",
    "        \"\\n Check create_frame_df parameters\"\n",
    "    )\n",
    "elif npresented > ndetected * 2:\n",
    "    msg = (\n",
    "        f\"Dropped more than half of the frames ({npresented - ndetected} dropped)\"\n",
    "        \"\\n Check create_frame_df parameters\"\n",
    "    )\n",
    "else:\n",
    "    msg = None\n",
    "if msg is not None:\n",
    "    if ignore_errors:\n",
    "        warnings.warn(msg)\n",
    "    else:\n",
    "        raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc9fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_window = np.array([-7.5, 7.5]) / frame_rate * pd_sampling\n",
    "figs = find_frames.plot_frame_detection_report(\n",
    "    border_index=frame_borders,\n",
    "    peak_index=peak_index,\n",
    "    debug_dict=db_dict,\n",
    "    num_examples=1,\n",
    "    plot_window=plot_window,\n",
    "    photodiode=photodiode,\n",
    "    frame_rate=frame_rate,\n",
    "    photodiode_sampling=pd_sampling,\n",
    "    highcut=frame_rate * 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e4728",
   "metadata": {},
   "source": [
    "#Â Crosscorrelation\n",
    "\n",
    "After having detected frames we will try to find where each of them falls in the photodiode sequence. To do that, we start by normalising the photodiode signal between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2637d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_df, db_di = find_frames.run_cross_correlation(\n",
    "    frames_df,\n",
    "    frame_log,\n",
    "    photodiode_time,\n",
    "    normed_pd,\n",
    "    time_column,\n",
    "    sequence_column,\n",
    "    num_frame_to_corr,\n",
    "    maxlag,\n",
    "    expected_lag,\n",
    "    frame_rate,\n",
    "    verbose=True,\n",
    "    debug=True,\n",
    "    pd_sampling=pd_sampling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_df = find_frames._match_fit_to_logger(\n",
    "    frames_df,\n",
    "    correlation_threshold=correlation_threshold,\n",
    "    relative_corr_thres=relative_corr_thres,\n",
    "    minimum_lag=minimum_lag,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Then interpolate the missing frames\n",
    "find_frames.interpolate_sync(frames_df, verbose=True)\n",
    "# and remove the last double detected frames\n",
    "frames_df = find_frames._remove_double_frames(frames_df, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dict.update(db_di)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "find_frames.plot_crosscorr_matrix(\n",
    "    ax, db_dict[\"cc_dict\"], db_dict[\"lags_sample\"], frames_df\n",
    ")\n",
    "ax = fig.add_subplot(2, 1, 2)\n",
    "find_frames.plot_crosscorr_matrix(\n",
    "    ax, db_dict[\"cc_dict\"], db_dict[\"lags_sample\"], frames_df\n",
    ")\n",
    "xl = ax.get_xlim()\n",
    "mid = (xl[0] + xl[1]) / 2\n",
    "ax.set_xlim(mid - 100, mid + 100)\n",
    "fig.savefig(save_folder / \"crosscorr_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(frames_df.closest_frame_log_index.values)\n",
    "\n",
    "ok = frames_df.closest_frame_log_index.values >= (\n",
    "    frames_df.closest_frame_log_index.iloc[-1] - 1\n",
    ")\n",
    "\n",
    "# plt.plot(frames_df.closest_frame_log_index.values)\n",
    "ok.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = frames_df.index[919623 - 150]\n",
    "fig = find_frames.plot_one_frame_check(\n",
    "    frame,\n",
    "    frames_df,\n",
    "    frame_log,\n",
    "    real_time=photodiode_time,\n",
    "    normed_pd=normed_pd,\n",
    "    ideal_time=db_dict[\"ideal_time\"],\n",
    "    ideal_pd=db_dict[\"ideal_photodiode_trace\"],\n",
    "    ideal_seqi=db_dict[\"ideal_seqi_trace\"],\n",
    "    num_frame_to_corr=None,\n",
    ")\n",
    "fig.suptitle(\n",
    "    f\"Frame {frame} matching frame log \"\n",
    "    f\"{frames_df.loc[frame, 'closest_frame']}\\n\"\n",
    "    f\"Is interpolated: {not frames_df.loc[frame, 'interpolation_seeds']}\"\n",
    ")\n",
    "fig.savefig(save_folder / f\"frame_{frame}_check.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54044fba",
   "metadata": {},
   "source": [
    "### Idealised photodiode\n",
    "\n",
    "Then we generate an idealised version of what the photodiode signal should be (had their been no frame drops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b32dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_trace, ideal_pd = find_frames.ideal_photodiode(\n",
    "    time_base=ao_time,\n",
    "    switch_time=frame_log[\"HarpTime\"].values,\n",
    "    sequence=frame_log[\"PhotoQuadColor\"].values,\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "w = np.array([10000, 10100])\n",
    "t0 = ao_time[w[0]]\n",
    "plt.plot(ao_time[slice(*w)] - t0, normed_pd[slice(*w)], label=\"Normed photodiode\")\n",
    "plt.plot(\n",
    "    ao_time[slice(*w)] - t0,\n",
    "    seq_trace[slice(*w)],\n",
    "    label=\"Sequence\",\n",
    "    color=\"grey\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.plot(ao_time[slice(*w)] - t0, ideal_pd[slice(*w)], label=\"Filtered sequence\")\n",
    "l = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75593f0d",
   "metadata": {},
   "source": [
    "### Data chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13059ae",
   "metadata": {},
   "source": [
    "Now we want to run the crosscorrelation around each frame.\n",
    "\n",
    "We need to take a chunk of data that is big enough but short enough. Five or 6 frames seems to get good unique match with the sequence. Use `num_frame_to_corr` to set that.\n",
    "\n",
    "Then we need to shift the photodiode by a given lag and cut the same chunk of data to correlate. There is no point in testing all the shifts, we now it will be about 2 frames. So we have `expected_lag ~= int(2/frame_rate*ao_sampling)` (in samples). \n",
    "\n",
    "To make things reasonably fast we also limit the search to a 3 frames of lag (+/- around expected_lag). With `maxlag ~= int(3/frame_rate*ao_sampling)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frame_to_corr = 6\n",
    "maxlag_samples = int(np.round(3 / frame_rate * ao_sampling))  # make it into samples\n",
    "expected_lag_samples = int(\n",
    "    np.round(2 / frame_rate * ao_sampling)\n",
    ")  # make it into samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db73bc",
   "metadata": {},
   "source": [
    "Finally we need to decide if we take the chunk of data before the frame, centered on the frame or after the frame. The best choice depends on if there was a frame drop recently or not. So let's just do the 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = [\n",
    "    np.array([-1, 1]) * maxlag_samples\n",
    "    + np.array(w * num_frame_to_corr / frame_rate * ao_sampling, dtype=\"int\")\n",
    "    for w in [np.array([-1, 0]), np.array([-0.5, 0.5]), np.array([0, 1])]\n",
    "]\n",
    "# for bef window, we add 1 frame to have the current frame included\n",
    "window[0] += int(1 / frame_rate * ao_sampling)\n",
    "# for center window, we shift by 0.5 frame to center\n",
    "window[1] += int(0.5 / frame_rate * ao_sampling)\n",
    "\n",
    "example_frame = 5234\n",
    "frame_sample = frame_borders[example_frame]\n",
    "lab = [\"bef\", \"center\", \"aft\"]\n",
    "t0 = ao_time[frame_sample]\n",
    "for iw, w in enumerate(window):\n",
    "    part = slice(*w + frame_sample)\n",
    "    plt.plot(\n",
    "        ao_time[part][maxlag_samples : -maxlag_samples + 1] - t0,\n",
    "        normed_pd[part][maxlag_samples : -maxlag_samples + 1] + iw * 0.5,\n",
    "        label=lab[iw],\n",
    "    )\n",
    "plt.axvspan(\n",
    "    ao_time[frame_sample] - t0,\n",
    "    ao_time[frame_borders[example_frame + 1]] - t0,\n",
    "    alpha=0.5,\n",
    ")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5771c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlag_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.hist(\n",
    "    frames_df.lag.values * 1000,\n",
    "    bins=np.arange(frames_df.lag.min() * 1000, frames_df.lag.max() * 1000),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb39769",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = np.diff(frames_df.closest_frame.values) < 1\n",
    "frames_df.iloc[1:][bad]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bca33c3",
   "metadata": {},
   "source": [
    "Add that to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dad53a",
   "metadata": {},
   "source": [
    "## Match cross correlation results to frame logger\n",
    "\n",
    "Ideally, if there is no frame drop, it does not matter if we look at the frames perceeding or following the frame we want to sync. That should be most of the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66934d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db_dict[\"debug_info\"]\n",
    "normed_pd = np.array(photodiode, dtype=float)\n",
    "normed_pd -= np.quantile(normed_pd, 0.01)\n",
    "normed_pd /= np.quantile(normed_pd, 0.99)\n",
    "db.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2de6eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(102)\n",
    "w = frames_df[frames_df.sync_reason == \"photodiode matching\"].index\n",
    "random_select = [w[i] for i in rng.integers(len(w), size=10)]\n",
    "bad = np.diff(frames_df.closest_frame.values) < 1\n",
    "badi = np.where(bad)[0]\n",
    "random_select = frames_df.iloc[badi[20] + np.array([0, 1, 2], dtype=int)].index\n",
    "labels = [\"bef\", \"center\", \"aft\"]\n",
    "num_frame_to_corr = 5\n",
    "maxlag = int(5.0 / frame_rate * ao_sampling)\n",
    "expected_lag = int(2.0 / frame_rate * ao_sampling)\n",
    "window = [\n",
    "    np.array([-1, 1]) * maxlag\n",
    "    + np.array(w * num_frame_to_corr / frame_rate * ao_sampling, dtype=\"int\")\n",
    "    for w in [np.array([-1, 0]), np.array([-0.5, 0.5]), np.array([0, 1])]\n",
    "]\n",
    "seq_trace = db[\"seq_trace\"]\n",
    "\n",
    "for frame in random_select:\n",
    "    # frame = frames_df[~good].index[num]\n",
    "    # frame = frames_df.index[num]\n",
    "    fseries = frames_df.loc[frame]\n",
    "    on_s = fseries.onset_sample\n",
    "    off_s = fseries.offset_sample\n",
    "    on_t = fseries.onset_time\n",
    "    off_t = fseries.offset_time\n",
    "    w = np.array([-50, 50])\n",
    "    vfdf = frames_df[\n",
    "        (frames_df.onset_sample > w[0] + on_s)\n",
    "        & (frames_df.offset_sample < w[1] + off_s)\n",
    "    ]\n",
    "    qc = np.array([fseries[[\"quadcolor_%s\" % w for w in labels]]])\n",
    "    best = fseries.crosscorr_picked\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    plt.gca().get_yaxis().set_visible(False)\n",
    "\n",
    "    col = dict(bef=\"r\", center=\"g\", aft=\"b\")\n",
    "    for i in range(3):\n",
    "        label = \"Photodiode\" if i == 1 else None\n",
    "        plt.plot(\n",
    "            ao_time[slice(*w + on_s)] - on_t,\n",
    "            normed_pd[slice(*w + on_s)] + i,\n",
    "            label=label,\n",
    "            color=\"purple\",\n",
    "        )\n",
    "        label = \"Frame #%d\" % frame if i == 1 else None\n",
    "        plt.axvspan(0, off_t - on_t, color=\"purple\", alpha=0.2, label=label)\n",
    "        plt.plot(fseries.peak_time - on_t, fseries.photodiode, \"o\", color=\"purple\")\n",
    "\n",
    "    vlog = frame_log[\n",
    "        (frame_log.HarpTime > w[0] / ao_sampling + on_t - fseries.lag_bef)\n",
    "        & (frame_log.HarpTime < w[1] / ao_sampling + off_t)\n",
    "    ]\n",
    "    plt.plot(\n",
    "        vlog.HarpTime.values - on_t,\n",
    "        vlog.PhotoQuadColor - 1.5,\n",
    "        drawstyle=\"steps-post\",\n",
    "        label=\"Render frame\",\n",
    "    )\n",
    "\n",
    "    i = 0\n",
    "    for win, lab in zip(window, [\"bef\", \"center\", \"aft\"]):\n",
    "        cut_win = win + maxlag * np.array([1, -1], dtype=int)\n",
    "        l = fseries[\"lag_%s\" % lab]\n",
    "        part = seq_trace[slice(*win + on_s)]\n",
    "        cut_part = seq_trace[slice(*cut_win + on_s)]\n",
    "        x = normed_pd[slice(*win + on_s)][maxlag : -maxlag + 1]\n",
    "\n",
    "        plt.plot(\n",
    "            ao_time[slice(*win + on_s)] - on_t + l,\n",
    "            part + i,\n",
    "            alpha=0.75,\n",
    "            lw=2,\n",
    "            color=col[lab],\n",
    "        )\n",
    "        plt.plot(\n",
    "            ao_time[slice(*win + on_s)][maxlag : -maxlag + 1] - on_t,\n",
    "            x + i,\n",
    "            alpha=0.5,\n",
    "            lw=4,\n",
    "            ls=\"--\",\n",
    "            color=col[lab],\n",
    "        )\n",
    "\n",
    "        cl = fseries[\"closest_frame_%s\" % lab]\n",
    "        plt.plot(\n",
    "            frame_log.iloc[cl].HarpTime - on_t,\n",
    "            frame_log.iloc[cl].PhotoQuadColor - 1.5 + i / 6,\n",
    "            \"o\",\n",
    "            color=col[lab],\n",
    "        )\n",
    "        if lab == best:\n",
    "            plt.plot(\n",
    "                frame_log.iloc[cl].HarpTime - on_t,\n",
    "                frame_log.iloc[cl].PhotoQuadColor - 1.5 + i / 6,\n",
    "                \"o\",\n",
    "                mfc=\"None\",\n",
    "                mec=\"k\",\n",
    "                ms=10,\n",
    "                mew=2,\n",
    "            )\n",
    "            plt.plot(\n",
    "                frame_log.iloc[cl].HarpTime - on_t + l,\n",
    "                frame_log.iloc[cl].PhotoQuadColor + i,\n",
    "                \"o\",\n",
    "                color=\"k\",\n",
    "            )\n",
    "            plt.plot(\n",
    "                ao_time[slice(*cut_win + on_s)] - on_t + l,\n",
    "                cut_part + i,\n",
    "                alpha=1,\n",
    "                lw=1,\n",
    "                color=\"k\",\n",
    "                label=\"Selected match\",\n",
    "            )\n",
    "\n",
    "        i += 1\n",
    "        plt.title(\"%s\" % fseries.onset_sample)\n",
    "\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4230d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77caab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find what is the actual photodiode value and how does it depend on previous value\n",
    "\n",
    "df = pd.DataFrame(frames_df.iloc[1:][[\"quadcolor\", \"photodiode\"]]).reset_index()\n",
    "bef = pd.DataFrame(frames_df.iloc[:-1][[\"quadcolor\", \"photodiode\"]]).reset_index()\n",
    "df[\"quadcolor_before\"] = bef[\"quadcolor\"]\n",
    "df[\"photodiode_before\"] = bef[\"photodiode\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_data = (\n",
    "    df.groupby([\"quadcolor_before\", \"quadcolor\"]).aggregate(np.nanmean).photodiode\n",
    ")\n",
    "n_data = df.groupby([\"quadcolor_before\", \"quadcolor\"]).aggregate(len).photodiode\n",
    "m = mat_data.values.reshape((5, 5))\n",
    "n = n_data.values.reshape((5, 5))\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(m.T, origin=\"lower\")\n",
    "cm = plt.colorbar()\n",
    "plt.xlabel(\"quad n-1\")\n",
    "plt.ylabel(\"quad n\")\n",
    "plt.title(\"Photodiode\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Difference\")\n",
    "plt.imshow((m - np.linspace(0, 1, 5)).T, origin=\"lower\", cmap=\"RdBu_r\")\n",
    "plt.xlabel(\"quad n-1\")\n",
    "plt.ylabel(\"quad n\")\n",
    "cm = plt.colorbar()\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"N transitions\")\n",
    "plt.imshow(n.T, origin=\"lower\")\n",
    "plt.xlabel(\"quad n-1\")\n",
    "plt.ylabel(\"quad n\")\n",
    "cm = plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e70e6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac69145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d09668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7beb44f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c1be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742d37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7864b0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda39a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fd0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362fd146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f489290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebacb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313ba96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914de51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff837f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd1199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8f89f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "fseries = frames_df.loc[4713]\n",
    "fseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = fseries.offset_time\n",
    "frame_log[\"HarpTime\"][1940:1955] - fseries.lag_aft - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b43e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_df.sync_reason.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ac6fb",
   "metadata": {},
   "source": [
    "# Divers stuff\n",
    "\n",
    "Figures to explain things for my lab meeting (09/11/2022)\n",
    "\n",
    "##Â Sequence principle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "b = 1000\n",
    "w = 100\n",
    "shift = 1\n",
    "0\n",
    "seq = seq_trace[b : b + w]\n",
    "bad_seq = np.array(seq_trace[b + shift : b + w + shift])\n",
    "bad_seq[int(w / 3) : int(w / 3 + w / 3 * 0.6)] = bad_seq[int(w / 3)]\n",
    "ax.plot((ao_time[b : b + w] - ao_time[b]) * 1e3, bad_seq)\n",
    "ax.plot((ao_time[b : b + w] - ao_time[b]) * 1e3, seq + 1)\n",
    "ax.set_xlabel(\"Time (ms)\")\n",
    "ax.yaxis.set_visible(False)\n",
    "for w in [\"top\", \"left\", \"right\"]:\n",
    "    ax.spines[w].set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
