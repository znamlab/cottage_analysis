{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the position of the mouse over time and generating a video\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that, I must: \n",
    "\n",
    "- Calibrate the extrinsics of the fm cameras in reference to the calibrador. It will probably be slightly off focus, because the headstage is not in the same position, so I should calculate the offset. \n",
    "- Figure out how to plot points on that space on the camera\n",
    "- Figure out how to translate the points of the lighthouse positioning into the camera frame (should be simple, just offset)\n",
    "- Figure out how to make a video"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import flexiznam as flz\n",
    "import numpy as np\n",
    "import cottage_analysis.io_module.onix as onix\n",
    "import cottage_analysis.io_module.harp as harp\n",
    "import cottage_analysis.ephys.preprocessing as prp\n",
    "import cottage_analysis.utilities.plot_utils as plut\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import znamcalib.calibrate_lighthouse as light"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrating the extrinsics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. The intrinsic calibration of the cameras used is S20220627, and the extrinsic is S20230509"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with OpenCV: project 3d point to frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is documented as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python: cv.ProjectPoints2(objectPoints, rvec, tvec, cameraMatrix, distCoeffs, imagePoints, dpdrot=None, dpdt=None, dpdf=None, dpdc=None, \n",
    "    dpddist=None) → None\n",
    "\n",
    "Parameters:\t\n",
    "- objectPoints – Array of object points, 3xN/Nx3 1-channel or 1xN/Nx1 3-channel (or vector<Point3f> ), where N is the number of points in the view.\n",
    "- rvec – Rotation vector. See Rodrigues() for details.\n",
    "- tvec – Translation vector.\n",
    "- cameraMatrix – Camera matrix A = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1} .\n",
    "- distCoeffs – Input vector of distortion coefficients (k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6]]) of 4, 5, or 8 elements. If the vector is \n",
    "    NULL/empty, the zero distortion coefficients are assumed.\n",
    "- imagePoints – Output array of image points, 2xN/Nx2 1-channel or 1xN/Nx1 2-channel, or vector<Point2f> .\n",
    "- jacobian – Optional output 2Nx(10+<numDistCoeffs>) jacobian matrix of derivatives of image points with respect to components of the \n",
    "    rotation vector, translation vector, focal lengths, coordinates of the principal point and the distortion coefficients. In the old \n",
    "    interface different components of the jacobian are returned via different output parameters.\n",
    "- aspectRatio – Optional “fixed aspect ratio” parameter. If the parameter is not 0, the function assumes that the aspect ratio (fx/fy) is fixed \n",
    "    and correspondingly adjusts the jacobian matrix.\n",
    "\n",
    "\n",
    "The function computes projections of 3D points to the image plane given intrinsic and extrinsic camera parameters. Optionally, the function \n",
    "computes Jacobians - matrices of partial derivatives of image points coordinates (as functions of all the input parameters) with respect to the \n",
    "particular parameters, intrinsic and/or extrinsic. The Jacobians are used during the global optimization in calibrateCamera(), solvePnP(), and \n",
    "stereoCalibrate() . The function itself can also be used to compute a re-projection error given the current intrinsic and extrinsic parameters.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we must get for each (static) plot: \n",
    "\n",
    "- A set of position points (from the Lighthouse tracking)\n",
    "- The rvec and tvec of the extrinsic calibration\n",
    "- The cameraMatrix and distCoeffs\n",
    "- what does imagePoints mean?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is all done for cam2_camera only\n",
    "\n",
    "DATA_PATH = Path('/camp/lab/znamenskiyp/data/instruments/raw_data/projects/blota_onix_pilote')\n",
    "PROCESSED_PATH = Path('/camp/lab/znamenskiyp/home/shared/projects/blota_onix_pilote')\n",
    "INTRINSICS_PATH = Path('/camp/lab/znamenskiyp/home/shared/projects/blota_onix_calibration/camera_intrinsics')\n",
    "EXTRINSICS_PATH = Path('/camp/lab/znamenskiyp/home/shared/projects/blota_onix_calibration/arena_extrinsics')\n",
    "INTRINSICS_SESSION = 'S20220627'\n",
    "EXTRINSICS_SESSION = 'S20230509'\n",
    "\n",
    "MOUSE = 'BRAC7448.2d'\n",
    "SESSION = 'S20230412'\n",
    "CAMERA = 'cam2_camera'\n",
    "\n",
    "#Temporary and painful\n",
    "\n",
    "ephys = 'R163257'\n",
    "camera = 'R162624_freelymoving'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formulating complete paths\n",
    "\n",
    "data_path = DATA_PATH / MOUSE / SESSION / ephys\n",
    "camera_path = DATA_PATH / MOUSE / SESSION / camera\n",
    "processed_path = PROCESSED_PATH / MOUSE / SESSION\n",
    "intrinsics_path = INTRINSICS_PATH / INTRINSICS_SESSION / CAMERA\n",
    "extrinsics_path = EXTRINSICS_PATH / EXTRINSICS_SESSION / CAMERA / 'cam2_camera_snapshot_0_extrinsics_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Lighthouse data\n",
    "processed_photodiode = onix.load_ts4231(data_path)\n",
    "diode3 = processed_photodiode[3]\n",
    "calibration = Path('/camp/lab/znamenskiyp/data/instruments/raw_data/projects/blota_onix_calibration/lighthouse_calibration/S20230412')\n",
    "transform_matrix = light.calibrate_session(calibration, 20)\n",
    "trans_data = light.transform_data(diode3, transform_matrix)\n",
    "light.plot_single_occupancy(trans_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading extrinsics data\n",
    "\n",
    "#Will create a function that loads the .npz files for each marker. \n",
    "\n",
    "def load_extrinsics(extrinsics_path):\n",
    "    \"\"\"\n",
    "    Inside of the .npz files you find a dictionary with the following keys: \n",
    "    ['rvec', 'tvec', 'corners', 'r2m_rvec', 'r2m_tvec', 'm2r_rvec', 'm2r_tvec']\n",
    "\n",
    "    \"\"\"\n",
    "    marker_files = list(extrinsics_path.glob('*.npz'))\n",
    "    out = dict()\n",
    "    for i, marker in enumerate(marker_files): \n",
    "        marker_path = extrinsics_path / marker\n",
    "        out[f'{str(marker.name)[0:7]}'] = np.load(marker_path)\n",
    "    return(out)\n",
    "\n",
    "extrinsics = load_extrinsics(extrinsics_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvec = extrinsics['marker4']['rvec']\n",
    "tvec = extrinsics['marker4']['tvec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading intrinsics data\n",
    "\n",
    "def load_intrinsics(intrinsics_path):\n",
    "    \"\"\"\n",
    "    outputs a dictionary with the following set of keys: \n",
    "    ['ret', 'mtx', 'dist', 'rvecs', 'tvecs', 'mean_error']\n",
    "    where mtx is the cameraMatrix and dist are the distCoeffs\n",
    "    \n",
    "    \"\"\"\n",
    "    files = list(intrinsics_path.glob('*.npz'))\n",
    "    intrinsics = np.load(files[0])\n",
    "    return(intrinsics)\n",
    "\n",
    "intrinsics = load_intrinsics(intrinsics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameraMatrix = intrinsics['mtx']\n",
    "distCoeffs = intrinsics['dist']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to load a frame of the video at a specific onix time, so that we are able to plot the position of the mouse on that frame. To do that, loading metadata of the cameras, setting the video and grabbing the frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuck, Antonin changed the structure of the cameras!\n",
    "\n",
    "camera_dir = camera_path\n",
    "acquisition= 'freely_moving'\n",
    "\n",
    "def load_camera_times(camera_dir, acquisition):\n",
    "    if acquisition == \"freely_moving\":\n",
    "        camlist = [\"cam1_camera\", \n",
    "                   \"cam2_camera\", \n",
    "                   \"cam3_camera\"]\n",
    "    if acquisition == \"headfixed\":\n",
    "        camlist = [\n",
    "            \"letfeye_camera\",\n",
    "            \"righteye_camera\",\n",
    "            \"face_camera\",\n",
    "            \"behaviour_camera\",\n",
    "        ]\n",
    "    folder = Path(camera_dir)\n",
    "    if not folder.is_dir():\n",
    "        raise IOError(\"%s is not a directory\" % folder)\n",
    "    output = dict()\n",
    "    for cam in camlist:\n",
    "        valid_files = list(folder.glob(f\"{cam}*timestamps*\"))\n",
    "        if not len(valid_files):\n",
    "            raise IOError(f\"Could not find any timestamp files in {folder}\")\n",
    "        for possible_file in valid_files:\n",
    "            possible_file = str(possible_file)\n",
    "            if cam in possible_file:\n",
    "                output[cam] = pd.read_csv(possible_file)\n",
    "    return output\n",
    "\n",
    "camera_metadata = load_camera_times(camera_dir, acquisition)\n",
    "print(camera_metadata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translating camera clock into onix clock. This is tthe dirty way, but should probably work. Now, we've got the HARP. \n",
    "\n",
    "processed_breakout = onix.load_breakout(data_path)\n",
    "dio = processed_breakout['dio']\n",
    "\n",
    "filtered_dio = dict()\n",
    "filtered_dio['Clock'], filtered_dio['DI0'] = prp.clean_di_channel(dio['Clock'], dio['DI0'])\n",
    "\n",
    "cam2_metadata = camera_metadata['cam2_camera']\n",
    "camera_frames = len(cam2_metadata['frame_id'])\n",
    "breakout_frames = (len(filtered_dio['DI0']))/2\n",
    "\n",
    "print(f'cam2 has {camera_frames} frames')\n",
    "print(f'The breakout board accounts for {breakout_frames} frames')\n",
    "\n",
    "def cam2onix(filtered_dio, cam2_metadata):\n",
    "\n",
    "    onix_first = filtered_dio['Clock'][filtered_dio['DI0']==True][0]\n",
    "    onix_last = filtered_dio['Clock'][filtered_dio['DI0']==True][-1]\n",
    "    camera_first = cam2_metadata['camera_timestamp'].iloc[0]\n",
    "    camera_last = cam2_metadata['camera_timestamp'].iloc[-1] \n",
    "\n",
    "    #y=intercept+slope*x\n",
    "\n",
    "    intercept = onix_first\n",
    "    slope = (onix_last-onix_first)/(camera_last-camera_first)\n",
    "\n",
    "    onix_timestamp = intercept+slope*(cam2_metadata['camera_timestamp']-camera_first)\n",
    "\n",
    "    return onix_timestamp\n",
    "\n",
    "cam2_metadata['onix_timestamp'] = cam2onix(filtered_dio, cam2_metadata)\n",
    "cam2_metadata['onix_timestamp']\n",
    "\n",
    "plt.plot(filtered_dio['Clock'][filtered_dio['DI0']==True], color = 'blue')\n",
    "plt.plot(cam2_metadata['onix_timestamp'], color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding a frame at a specific onix time\n",
    "\n",
    "def find_nearest(Array, x):\n",
    "    dif_Array = np.absolute(Array-x) # use of absolute() function to find the difference \n",
    "    index = dif_Array.argmin() # find the index of minimum difference element\n",
    "    return Array[index]\n",
    "\n",
    "onix_time = trans_data['clock'][20000]\n",
    "\n",
    "video_path = '/camp/lab/znamenskiyp/data/instruments/raw_data/projects/blota_onix_pilote/BRAC7448.2d/S20230412/R162624_freelymoving/cam2_camera_2023-04-12T16_26_24.mp4'\n",
    "\n",
    "\n",
    "#We want to normalise luminance to the first frame\n",
    "\n",
    "def get_avg_first_frame(video_path):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, 1)\n",
    "    ret, frame_start = video.read()\n",
    "    frame_start = cv2.cvtColor(frame_start, cv2.COLOR_BGR2GRAY)\n",
    "    avg_first_frame = frame_start.mean()\n",
    "    return(avg_first_frame)\n",
    "\n",
    "\n",
    "def get_onix_frame(video_path, onix_time, avg_first_frame=None, show=False, normalise = False):\n",
    "    start_frame_onix = find_nearest(cam2_metadata['onix_timestamp'], onix_time)\n",
    "    start_frame = cam2_metadata[cam2_metadata['onix_timestamp']==start_frame_onix]\n",
    "    start_frame = start_frame.index[0]\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    ret, frame_start = video.read()\n",
    "    if normalise==True:\n",
    "        frame_start = frame_start*(avg_first_frame / frame_start.mean())\n",
    "    if show==True:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(50, 50))\n",
    "        fig.suptitle(f'Frame {start_frame}', size = 40)\n",
    "        axs.imshow(frame_start)\n",
    "    return frame_start\n",
    "\n",
    "frame_start = get_onix_frame(video_path, onix_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_first_frame = get_avg_first_frame(video_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the transformation\n",
    "\n",
    "In the current calibration, marker 4 is the reference. By looking at the pictures, z points upwards in the arena, x points left in the arena, y points downwards in the arena, from the reference point of an observer opening the door and looking inside. I will use marker4 as the source of calibration data. \n",
    "\n",
    "As far as I've seen: rvec and tvec are the translation and rotation vectors that show the position of the marker in the frame of reference of the camera. That is: z points form the camera forwards, x points from the center of the camera frame to the right, y points upwards. The metadata from the extrinsics calibration show that the calibration was done in mm, and the 0,0 of the lihgthouse calibration is 4cm further along the y axis than the centre of the Aruco. Thus, I'll convert the LIghthouse coordinates to mm and substract 6cm from each point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '/camp/lab/znamenskiyp/data/instruments/raw_data/projects/blota_onix_pilote/BRAC7448.2d/S20230412/R162624_freelymoving/cam2_camera_2023-04-12T16_26_24.mp4'\n",
    "\n",
    "def lighthouse2aruco(trans_data):\n",
    "    #transforming coordinates\n",
    "    proj_data = trans_data.copy(deep=True)\n",
    "    proj_data['x'] = (trans_data['x']*10)\n",
    "    proj_data['y'] = ((trans_data['y'])*10)+(44.5) #25 for half an aruco, 25 for the other half, 15 until the connector, 4.5 for one connector and a half\n",
    "    proj_data['z'] = trans_data['z']*10 \n",
    "    return proj_data\n",
    "\n",
    "proj_data = lighthouse2aruco(trans_data)\n",
    "\n",
    "def _plot_onix_time(onix_time, video_path, proj_data, rvec, tvec, cameraMatrix, distCoeffs):\n",
    "    \"\"\"\n",
    "    Same as plot_onix_time, but for efficiency I took out the transformation step to use it in generating the videos, so it works directly with\n",
    "    proj_data. \n",
    "    \"\"\"\n",
    "\n",
    "    #obtaining the closest frame\n",
    "    frame_start = get_onix_frame(video_path, onix_time)\n",
    "    #selecting index\n",
    "    lighthouse_index = find_nearest(proj_data['clock'], onix_time)\n",
    "    index = proj_data[proj_data['clock']==lighthouse_index].index[0]\n",
    "    objectPoints = np.array([proj_data['x'][index], proj_data['y'][index], proj_data['z'][index]])\n",
    "    objectPoints = np.transpose(objectPoints)\n",
    "    #transforming and plotting\n",
    "    imagePoints, jacobian = cv2.projectPoints(objectPoints, rvec, tvec, cameraMatrix, distCoeffs)\n",
    "    coordinates = imagePoints[0][0]\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(50, 50))\n",
    "    axs.imshow(frame_start)\n",
    "    axs.scatter(coordinates[0], coordinates[1], marker='+', s=15e3)\n",
    "    return fig\n",
    "\n",
    "def plot_onix_time(onix_time, video_path, trans_data, rvec, tvec, cameraMatrix, distCoeffs):\n",
    "    #obtaining the closest frame\n",
    "    frame_start = get_onix_frame(video_path, onix_time)\n",
    "    #transforming coordinates\n",
    "    proj_data = lighthouse2aruco(trans_data)\n",
    "    #selecting index\n",
    "    lighthouse_index = find_nearest(proj_data['clock'], onix_time)\n",
    "    index = proj_data[proj_data['clock']==lighthouse_index].index[0]\n",
    "    objectPoints = np.array([proj_data['x'][index], proj_data['y'][index], proj_data['z'][index]])\n",
    "    objectPoints = np.transpose(objectPoints)\n",
    "    #transforming and plotting\n",
    "    imagePoints, jacobian = cv2.projectPoints(objectPoints, rvec, tvec, cameraMatrix, distCoeffs)\n",
    "    coordinates = imagePoints[0][0]\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(50, 50))\n",
    "    axs.imshow(frame_start)\n",
    "    axs.scatter(coordinates[0], coordinates[1], marker='+', s=15e3)\n",
    "    return fig\n",
    "    \n",
    "\n",
    "time_in_s = 1400\n",
    "\n",
    "onix_time = trans_data['clock'][0]+(time_in_s*250e6)\n",
    "\n",
    "#plot_onix_time(onix_time, video_path, trans_data, rvec, tvec, cameraMatrix, distCoeffs)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interval_s(start, end, video_path, trans_data, rvec, tvec, cameraMatrix, distCoeffs):\n",
    "\n",
    "    frame_times_s = np.arange(start, end, (1/15))\n",
    "\n",
    "    # Define the output file name, codec, frame rate, and dimensions\n",
    "    output_file = f'output_video_{str(start)}_{str(end)}.mp4'\n",
    "    codec = cv2.VideoWriter_fourcc(*'mp4v')  # Choose the appropriate codec\n",
    "    frame_rate = 15  # Desired frame rate\n",
    "    frame_width = 3600 #as measured from current matplotlib output, maybe better to change programatically to ensure robustness when changing\n",
    "    #plotting function?\n",
    "    frame_height = 3600\n",
    "\n",
    "    # Create the video writer\n",
    "    video_writer = cv2.VideoWriter(output_file, codec, frame_rate, (frame_width, frame_height))\n",
    "\n",
    "    proj_data = lighthouse2aruco(trans_data)\n",
    "\n",
    "    for i in frame_times_s:\n",
    "        time_in_s = i\n",
    "        onix_time = proj_data['clock'][0]+(time_in_s*250e6)\n",
    "        fig = _plot_onix_time(onix_time, video_path, proj_data, rvec, tvec, cameraMatrix, distCoeffs)\n",
    "        image = plut.get_img_from_fig(fig)\n",
    "        plt.close(fig)\n",
    "        bgr_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        # Write the frame to the video\n",
    "        video_writer.write(bgr_image)\n",
    "\n",
    "    video_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "start = 1\n",
    "end = 20\n",
    "\n",
    "#plot_interval_s(start, end, video_path, trans_data, rvec, tvec, cameraMatrix, distCoeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = proj_data['clock'][len(proj_data['clock'])-1]-proj_data['clock'][0]\n",
    "duration_s = duration/250e6\n",
    "duration_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a diagonal\n",
    "\n",
    "diagonal = np.arange(0, 70, 0.5)\n",
    "z = np.arange(0,40,(40/140))\n",
    "zeros = np.zeros(140)\n",
    "seventies = np.array([70]*140)\n",
    "diagonal_points = np.zeros((140, 3))\n",
    "diagonal_points[:,0]=(seventies*10)-25\n",
    "diagonal_points[:,1]=(seventies*10)-25\n",
    "diagonal_points[:,2]=z*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_data = lighthouse2aruco(trans_data)\n",
    "whole_traj = np.zeros((len(proj_data), 3))\n",
    "whole_traj[:,0] = pd.array(proj_data['x'])\n",
    "whole_traj[:,1] = pd.array(proj_data['y'])\n",
    "whole_traj[:,2] = pd.array(proj_data['z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a grid\n",
    "grid_points = np.zeros((200, 3))\n",
    "o = -1\n",
    "for i in np.arange(0, 70, 5):\n",
    "    for e in np.arange(0, 70, 5):\n",
    "        o = o+1\n",
    "        grid_points[o, 0] = (i*10)-25\n",
    "        grid_points[o, 1] = (e*10)-25\n",
    "grid_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_point_series(series, frame_start, rvec, tvec, cameraMatrix, distCoeffs):\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(50, 50))\n",
    "    axs.imshow(frame_start)\n",
    "    for i in series[:,]:\n",
    "        objectPoints = np.transpose(i)\n",
    "        #transforming and plotting\n",
    "        imagePoints, jacobian = cv2.projectPoints(objectPoints, rvec, tvec, cameraMatrix, distCoeffs)\n",
    "        coordinates = imagePoints[0][0]\n",
    "        axs.scatter(coordinates[0], coordinates[1], marker='.', s=90)\n",
    "    return fig\n",
    "\n",
    "#plot_point_series(grid_points, frame_start, rvec, tvec, cameraMatrix, distCoeffs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with warping the image to make it square. \n",
    "\n",
    "Alternatively, we can plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting orientation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bno = onix.load_bno055(data_path)\n",
    "processed_bno.keys()\n",
    "\n",
    "quaternion = processed_bno['quaternion']\n",
    "#quaternion = quaternion/2**14 but the rest make sense (???)\n",
    "quaternion = quaternion\n",
    "linear = processed_bno['linear']\n",
    "linear = linear \n",
    "euler = processed_bno['euler']\n",
    "euler = euler\n",
    "gravity = processed_bno['gravity']\n",
    "gravity = gravity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heading(processed_bno):\n",
    "    quaternion = processed_bno['quaternion']\n",
    "    Qw = np.transpose(quaternion[:,0])\n",
    "    Qx = np.transpose(quaternion[:,1])\n",
    "    Qy = np.transpose(quaternion[:,2])\n",
    "    Qz = np.transpose(quaternion[:,3])\n",
    "    heading = np.zeros(len(Qw))\n",
    "    for i in np.arange(0, len(Qw)):\n",
    "        ith_quaternion = [[Qw[i], Qx[i], Qy[i], Qz[i]]]\n",
    "        norm_quaternion = ith_quaternion / np.linalg.norm([Qw[i], Qx[i], Qy[i], Qz[i]])\n",
    "        norm_quaternion = norm_quaternion[0]\n",
    "        heading[i] = _get_heading(norm_quaternion[0], norm_quaternion[1],norm_quaternion[2],norm_quaternion[3])\n",
    "    return heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The quaternion is supposed to output stuff in the order w, x, y, z\n",
    "# It's an axes*values matrix\n",
    "\n",
    "Qw = np.transpose(quaternion[:,0])\n",
    "Qx = np.transpose(quaternion[:,1])\n",
    "Qy = np.transpose(quaternion[:,2])\n",
    "Qz = np.transpose(quaternion[:,3])\n",
    "\n",
    "def _get_heading(Qw, Qx, Qy, Qz):\n",
    "    heading = math.atan2(2.0 * (Qz * Qw + Qx * Qy) , - 1.0 + 2.0 * (Qw * Qw + Qx * Qx))\n",
    "    return heading\n",
    "\n",
    "\n",
    "heading = np.zeros(len(Qw))\n",
    "\n",
    "for i in np.arange(0, len(Qw)):\n",
    "    ith_quaternion = [[Qw[i], Qx[i], Qy[i], Qz[i]]]\n",
    "    norm_quaternion = ith_quaternion / np.linalg.norm([Qw[i], Qx[i], Qy[i], Qz[i]])\n",
    "    norm_quaternion = norm_quaternion[0]\n",
    "    heading[i] = _get_heading(norm_quaternion[0], norm_quaternion[1],norm_quaternion[2],norm_quaternion[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_onix_time_orientation(onix_time, video_path, trans_data, processed_bno, heading, rvec, tvec, cameraMatrix, distCoeffs):\n",
    "    #obtaining the closest frame\n",
    "    frame_start = get_onix_frame(video_path, onix_time)\n",
    "    #transforming coordinates\n",
    "    proj_data = lighthouse2aruco(trans_data)\n",
    "    #selecting index\n",
    "    lighthouse_time = find_nearest(proj_data['clock'], onix_time)\n",
    "    index = proj_data[proj_data['clock']==lighthouse_time].index[0]\n",
    "    objectPoints = np.array([proj_data['x'][index], proj_data['y'][index], proj_data['z'][index]])\n",
    "    objectPoints = np.transpose(objectPoints)\n",
    "    #Obtaining heading\n",
    "    heading_time = find_nearest(processed_bno['onix_time'], onix_time)\n",
    "    heading_index = processed_bno['onix_time'].index[processed_bno['onix_time'].eq(heading_time)][0]\n",
    "    current_heading = heading[heading_index]\n",
    "    #Obtaining heading point\n",
    "    heading_point_x = objectPoints[0]+100*math.cos(current_heading)\n",
    "    heading_point_y = objectPoints[1]+100*math.sin(current_heading)\n",
    "    heading_point = np.transpose(np.array([heading_point_x, heading_point_y, 0]))\n",
    "    #transforming and plotting\n",
    "    imagePoints, jacobian = cv2.projectPoints(objectPoints, rvec, tvec, cameraMatrix, distCoeffs)\n",
    "    headingPoints, jacobian = cv2.projectPoints(heading_point, rvec, tvec, cameraMatrix, distCoeffs)\n",
    "    coordinates, heading_coordinates = imagePoints[0][0], headingPoints[0][0]\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(50, 50))\n",
    "    axs.imshow(frame_start)\n",
    "    axs.scatter(coordinates[0], coordinates[1], marker='+', s=15e3)\n",
    "    print(heading_coordinates)\n",
    "    axs.plot([coordinates[0], heading_coordinates[0]], [coordinates[1], heading_coordinates[1]], lw=12, linestyle='solid', color = 'red')\n",
    "    return fig\n",
    "\n",
    "#plot_onix_time_orientation(onix_time, video_path, trans_data, rvec, tvec, cameraMatrix, distCoeffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_onix_time_orientation(onix_time, video_path, avg_first_frame, proj_data, processed_bno, heading, rvec, tvec, cameraMatrix, distCoeffs, normalise=False):\n",
    "    \"\"\"\n",
    "    Same as plot_onix_time, but for efficiency I took out the transformation step to use it in generating the videos, so it works directly with\n",
    "    proj_data. \n",
    "    \"\"\"\n",
    "\n",
    "    #obtaining the closest frame\n",
    "    frame_start = get_onix_frame(video_path, onix_time, avg_first_frame = avg_first_frame, normalise=normalise)\n",
    "    #selecting index\n",
    "    lighthouse_index = find_nearest(proj_data['clock'], onix_time)\n",
    "    index = proj_data[proj_data['clock']==lighthouse_index].index[0]\n",
    "    objectPoints = np.array([proj_data['x'][index], proj_data['y'][index], proj_data['z'][index]])\n",
    "    objectPoints = np.transpose(objectPoints)\n",
    "    objectPoints[2]==0\n",
    "    #Obtaining heading\n",
    "    heading_time = find_nearest(processed_bno['onix_time'], onix_time)\n",
    "    heading_index = processed_bno['onix_time'].index[processed_bno['onix_time'].eq(heading_time)][0]\n",
    "    if heading[heading_index]>0:\n",
    "        current_heading = heading[heading_index]-(math.pi+0.236332)#The offset looks like pi and 12 degrees, by eye\n",
    "    else:\n",
    "        current_heading = heading[heading_index]+(math.pi-0.236332)\n",
    "    #Obtaining heading point\n",
    "    heading_point_x = objectPoints[0]+100*math.cos(current_heading)\n",
    "    heading_point_y = objectPoints[1]+100*math.sin(current_heading)\n",
    "    heading_point = np.transpose(np.array([heading_point_x, heading_point_y, 0]))\n",
    "    #transforming and plotting\n",
    "    imagePoints, jacobian = cv2.projectPoints(objectPoints, rvec, tvec, cameraMatrix, distCoeffs)\n",
    "    headingPoints, jacobian = cv2.projectPoints(heading_point, rvec, tvec, cameraMatrix, distCoeffs)\n",
    "    coordinates, heading_coordinates = imagePoints[0][0], headingPoints[0][0]\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(50, 50))\n",
    "    axs.imshow(frame_start)\n",
    "    axs.scatter(coordinates[0], coordinates[1], marker='+', s=15e3)\n",
    "    axs.plot([coordinates[0], heading_coordinates[0]], [coordinates[1], heading_coordinates[1]], lw=12, linestyle='solid', color = 'red')\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interval_orientation_s(start, end, video_path, trans_data, processed_bno, heading, rvec, tvec, cameraMatrix, distCoeffs, normalise=False):\n",
    "\n",
    "    frame_times_s = np.arange(start, end, (1/15))\n",
    "\n",
    "    # Define the output file name, codec, frame rate, and dimensions\n",
    "    output_file = f'output_orient_video_{str(start)}_{str(end)}.mp4'\n",
    "    codec = cv2.VideoWriter_fourcc(*'mp4v')  # Choose the appropriate codec\n",
    "    frame_rate = 15  # Desired frame rate\n",
    "    frame_width = 3600 #as measured from current matplotlib output, maybe better to change programatically to ensure robustness when changing\n",
    "    #plotting function?\n",
    "    frame_height = 3600\n",
    "    avg_first_frame = get_avg_first_frame(video_path)\n",
    "\n",
    "    # Create the video writer\n",
    "    video_writer = cv2.VideoWriter(output_file, codec, frame_rate, (frame_width, frame_height))\n",
    "\n",
    "    proj_data = lighthouse2aruco(trans_data)\n",
    "\n",
    "    for i in frame_times_s:\n",
    "        time_in_s = i\n",
    "        onix_time = proj_data['clock'][0]+(time_in_s*250e6)\n",
    "        fig = _plot_onix_time_orientation(onix_time, video_path, avg_first_frame, proj_data, processed_bno, heading, rvec, tvec, cameraMatrix, distCoeffs, normalise = normalise)\n",
    "        image = plut.get_img_from_fig(fig)\n",
    "        plt.close(fig)\n",
    "        bgr_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        # Write the frame to the video\n",
    "        video_writer.write(bgr_image)\n",
    "\n",
    "    video_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "start = 30\n",
    "end = 60\n",
    "\n",
    "plot_interval_orientation_s(start, end, video_path, trans_data, processed_bno, heading, rvec, tvec, cameraMatrix, distCoeffs)\n",
    "\n",
    "start = 1603\n",
    "end = 1633\n",
    "\n",
    "plot_interval_orientation_s(start, end, video_path, trans_data, processed_bno, heading, rvec, tvec, cameraMatrix, distCoeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clock_s = (trans_data['clock']-trans_data['clock'][0])/250e6\n",
    "dif_clock = np.diff(clock_s)\n",
    "plt.hist(dif_clock)\n",
    "print(f'The median period is {np.median(dif_clock)}s, therefore the median frequency is {1/(np.median(dif_clock))}Hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clock_s = (processed_bno['onix_time']-processed_bno['onix_time'][0])/250e6\n",
    "dif_clock = np.diff(clock_s)\n",
    "plt.hist(dif_clock)\n",
    "print(f'The median period is {np.median(dif_clock)}s, therefore the median frequency is {1/(np.median(dif_clock))}Hz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cottage_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99c5ab3b34a62a7741b597fad5e753313eb10ed9c721f639f5c87868832eb072"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
