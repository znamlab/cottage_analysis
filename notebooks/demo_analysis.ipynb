{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import functools\n",
    "print = functools.partial(print, flush=True)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import flexiznam as flz\n",
    "from cottage_analysis.io_module import harp\n",
    "from cottage_analysis.preprocessing import find_frames\n",
    "from cottage_analysis.imaging.common import find_frames as find_img_frames\n",
    "from cottage_analysis.filepath import generate_filepaths\n",
    "from cottage_analysis.imaging.common import imaging_loggers_formatting as format_loggers\n",
    "from cottage_analysis.preprocessing import synchronisation\n",
    "from cottage_analysis.analysis import find_depth_neurons, fit_gaussian_blob, common_utils\n",
    "from cottage_analysis.stimulus_structure import spheres_tube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example session\n",
    "project = 'hey2_3d-vision_foodres_20220101'\n",
    "mouse = 'PZAH8.2f'\n",
    "session = 'S20230126'\n",
    "RECORDING = 'R144331_SpheresPermTubeReward'\n",
    "protocol = 'SpheresPermTubeReward'\n",
    "MESSAGES = 'harpmessage.bin'\n",
    "flexilims_session = flz.get_flexilims_session(project_id=project)\n",
    "# all_protocol_recording_entries = generate_filepaths.get_all_recording_entries(project=project, \n",
    "#                                                                               mouse=mouse, \n",
    "#                                                                               session=session, \n",
    "#                                                                               protocol=protocol, \n",
    "#                                                                               flexilims_session=flexilims_session)\n",
    "\n",
    "# # DO NOT RUN THIS FUNCTION (TAKES 2hrs ish): to find monitor frames from photodiode signal\n",
    "# find_monitor_frames(project=project, \n",
    "#                     mouse=mouse, \n",
    "#                     session=session, \n",
    "#                     protocol=protocol, \n",
    "#                     all_protocol_recording_entries=None, \n",
    "#                     irecording=None, \n",
    "#                     flexilims_session=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synchronisation dataframes\n",
    "vs_df =  synchronisation.generate_vs_df(project=project, \n",
    "                                        mouse=mouse, \n",
    "                                        session=session, \n",
    "                                        protocol=protocol, \n",
    "                                        irecording=0)\n",
    "trials_df, imaging_df = synchronisation.generate_trials_df(project=project, \n",
    "                                                           mouse=mouse, \n",
    "                                                           session=session, \n",
    "                                                           protocol=protocol, \n",
    "                                                           vs_df=vs_df, \n",
    "                                                           irecording=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find depth neurons and fit preferred depth\n",
    "neurons_df = find_depth_neurons.find_depth_neurons(\n",
    "    project=project, \n",
    "    mouse=mouse, \n",
    "    session=session, \n",
    "    protocol=\"SpheresPermTubeReward\", \n",
    "    rs_thr=0.2\n",
    ")\n",
    "\n",
    "neurons_df = find_depth_neurons.fit_preferred_depth(\n",
    "    project=project,\n",
    "    mouse=mouse,\n",
    "    session=session,\n",
    "    protocol=\"SpheresPermTubeReward\",\n",
    "    depth_min=0.02,\n",
    "    depth_max=20,\n",
    "    batch_num=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit gaussian blob to neuronal activity (THIS WILL ALSO TAKE QUITE LONG! SO STOP EARLIER AND CHECK A FEW CELLS)\n",
    "neurons_df = fit_gaussian_blob.fit_gaussian_blob(project=project, \n",
    "                  mouse=mouse, \n",
    "                  session=session, \n",
    "                  protocol=\"SpheresPermTubeReward\", \n",
    "                  rs_thr=0.01, \n",
    "                  param_range={'rs_min':0.005, 'rs_max':5, 'of_min':0.03, 'of_max':3000}, \n",
    "                  batch_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate sphere stimuli\n",
    "def regenerate_stimuli(project,mouse,session,protocol):\n",
    "    def regenerate_stimuli_each_recording(project,mouse,session,protocols,protocol,irecording,nrecordings,):\n",
    "        (\n",
    "            rawdata_folder,\n",
    "            protocol_folder,\n",
    "            _,\n",
    "            _,\n",
    "            _,\n",
    "        ) = generate_filepaths.generate_file_folders(\n",
    "            project=project,\n",
    "            mouse=mouse,\n",
    "            session=session,\n",
    "            protocol=protocol,\n",
    "            all_protocol_recording_entries=None,\n",
    "            recording_no=0,\n",
    "        )\n",
    "\n",
    "        param_log = pd.read_csv(rawdata_folder / \"NewParams.csv\")\n",
    "        param_log = param_log.rename(\n",
    "            columns={\"Radius\": \"Depth\"}\n",
    "        )\n",
    "\n",
    "        with open(protocol_folder/\"sync/imaging_df.pickle\",\"rb\") as handle:\n",
    "            imaging_df = pickle.load(handle)\n",
    "        with open(protocol_folder/\"sync/vs_df.pickle\",\"rb\") as handle:\n",
    "            vs_df = pickle.load(handle)\n",
    "        with open(protocol_folder/\"sync/trials_df.pickle\",\"rb\") as handle:\n",
    "            trials_df = pickle.load(handle)\n",
    "        output = spheres_tube.regenerate_frames(\n",
    "            frame_times=imaging_df['harptime_imaging_trigger'].values, #using imaging frames as the list of timepoints to reconstruct stimuli\n",
    "            trials_df=trials_df,\n",
    "            vs_df=vs_df,\n",
    "            param_logger=param_log,\n",
    "            time_column=\"HarpTime\",\n",
    "            resolution=1,\n",
    "            sphere_size=10,\n",
    "            azimuth_limits=(-120, 120),\n",
    "            elevation_limits=(-40, 40),\n",
    "            verbose=True,\n",
    "            output_datatype=\"int16\",\n",
    "            output=None,\n",
    "        )\n",
    "\n",
    "        np.save(protocol_folder/'stimuli.npy', output)\n",
    "        \n",
    "        return output \n",
    "    \n",
    "    outputs = []\n",
    "    output = common_utils.loop_through_recordings(project=project,\n",
    "                                                  mouse=mouse,\n",
    "                                                  session=session,\n",
    "                                                  protocol=protocol, \n",
    "                                                  func=regenerate_stimuli_each_recording)\n",
    "    outputs.append(output)\n",
    "    outputs = np.stack(outputs)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "\n",
    "outputs=regenerate_stimuli(project,mouse,session,protocol)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.load(protocol_folder/'stimuli.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iroi=3\n",
    "\n",
    "reconstructed_frames = outputs[0]\n",
    "frame_times = imaging_df['harptime_imaging_trigger'].values\n",
    "frame_rate = 15\n",
    "delays=np.array([-1,0,1])\n",
    "spk_per_frame=np.stack(imaging_df['dffs'].values)[:,iroi]\n",
    "verbose=True\n",
    "\n",
    "\"\"\"Spike triggered average of reconstructed frames by depth\n",
    "\n",
    "Delays, in second, are delay applied to the stimulus sequence. If delay is -100,\n",
    "that means that spikes were triggered by stimulus 100ms before them.\n",
    "\n",
    "Args:\n",
    "    trials_df (pd.DataFrame): stimulus structure with each row as a trial.\n",
    "    reconstructed_frames (np.array): n frames x n elev x n azim binary array of\n",
    "                                        stimuli\n",
    "    frame_times (np.array): time of each frame, same unit as corridor_df.start_time\n",
    "    frame_rate (float): frame rate to calculate delays. 144 for monitor frames, 15 for imaging frames.\n",
    "    delays (np.array): array of delays in seconds\n",
    "    spk_per_frame (np.array): spike for each frame, use to weight average. If None\n",
    "                                will do simple average\n",
    "    verbose (bool): print progress\n",
    "\n",
    "Returns:\n",
    "    sta (np.array): n depth x n delay x n elev x n azim weighted average\n",
    "    nspkes (np.array): n depth vector of number of spikes\n",
    "    depths (np.array): ordered depths corresponding to first sta dimension\n",
    "    delays (np.array): ordered delays corresponding to second sta dimension\n",
    "\"\"\"\n",
    "if delays is None:\n",
    "    delays = [0]\n",
    "if spk_per_frame is None:\n",
    "    spk_per_frame = np.ones(reconstructed_frames.shape[0])\n",
    "\n",
    "depths = np.sort(trials_df.depth.unique())\n",
    "full_sta = np.zeros((len(depths), len(delays), *reconstructed_frames.shape[1:]))\n",
    "nspks = np.zeros(len(depths))\n",
    "\n",
    "for idepth, depth in enumerate(depths):\n",
    "    if verbose:\n",
    "        print(f\"... doing depth {depth*100} cm\")\n",
    "    depth_df = trials_df[trials_df.depth == depth]\n",
    "    # find frames at this depth\n",
    "    # starts = depth_df.imaging_frame_stim_start.values\n",
    "    # ends = depth_df.imaging_frame_stim_stop.values\n",
    "    starts = frame_times.searchsorted(depth_df.harptime_stim_start)\n",
    "    ends = frame_times.searchsorted(depth_df.harptime_stim_stop)\n",
    "    ends = ends[: len(starts)]\n",
    "    frame_index = np.hstack(\n",
    "        [np.arange(s, e, dtype=int) for s, e in zip(starts, ends)]\n",
    "    )\n",
    "    # keep non-shifted spikes for all delay\n",
    "    # do it like that to look for valid frames only once\n",
    "    spk_per_frame_at_depth = spk_per_frame[frame_index]\n",
    "    nspks[idepth] = np.sum(spk_per_frame_at_depth)\n",
    "    valid_frames = spk_per_frame_at_depth != 0\n",
    "    for idelay, delay in enumerate(delays):\n",
    "        if verbose:\n",
    "            print(f\"... ... doing delay {delay * 1000} ms\")\n",
    "        shift = int(delay * frame_rate)\n",
    "        # shift the stim\n",
    "        shifted_frames = np.clip(\n",
    "            frame_index[valid_frames] + shift, 0, len(frame_times)\n",
    "        )\n",
    "        stims = reconstructed_frames[shifted_frames].reshape(\n",
    "            len(shifted_frames), -1\n",
    "        )\n",
    "        sta = np.dot(stims.T, spk_per_frame_at_depth[valid_frames])\n",
    "        sta = sta.reshape(reconstructed_frames.shape[1:])\n",
    "        full_sta[idepth, idelay] = sta\n",
    "        # !! Needs to add normalized STA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iroi=3\n",
    "\n",
    "reconstructed_frames = outputs[0]\n",
    "frame_times = imaging_df['harptime_imaging_trigger'].values\n",
    "frame_rate = 15\n",
    "delays=np.array([-1,0,1])\n",
    "spk_per_frame=np.stack(imaging_df['dffs'].values)[:,iroi]\n",
    "verbose=True\n",
    "\n",
    "\"\"\"Spike triggered average of reconstructed frames by depth\n",
    "\n",
    "Delays, in second, are delay applied to the stimulus sequence. If delay is -100,\n",
    "that means that spikes were triggered by stimulus 100ms before them.\n",
    "\n",
    "Args:\n",
    "    trials_df (pd.DataFrame): stimulus structure with each row as a trial.\n",
    "    reconstructed_frames (np.array): n frames x n elev x n azim binary array of\n",
    "                                        stimuli\n",
    "    frame_times (np.array): time of each frame, same unit as corridor_df.start_time\n",
    "    frame_rate (float): frame rate to calculate delays. 144 for monitor frames, 15 for imaging frames.\n",
    "    delays (np.array): array of delays in seconds\n",
    "    spk_per_frame (np.array): spike for each frame, use to weight average. If None\n",
    "                                will do simple average\n",
    "    verbose (bool): print progress\n",
    "\n",
    "Returns:\n",
    "    sta (np.array): n depth x n delay x n elev x n azim weighted average\n",
    "    nspkes (np.array): n depth vector of number of spikes\n",
    "    depths (np.array): ordered depths corresponding to first sta dimension\n",
    "    delays (np.array): ordered delays corresponding to second sta dimension\n",
    "\"\"\"\n",
    "if delays is None:\n",
    "    delays = [0]\n",
    "if spk_per_frame is None:\n",
    "    spk_per_frame = np.ones(reconstructed_frames.shape[0])\n",
    "\n",
    "depths = np.sort(trials_df.depth.unique())\n",
    "full_sta = np.zeros((len(depths), len(delays), *reconstructed_frames.shape[1:]))\n",
    "nspks = np.zeros(len(depths))\n",
    "\n",
    "for idepth, depth in enumerate(depths):\n",
    "    if verbose:\n",
    "        print(f\"... doing depth {depth*100} cm\")\n",
    "    depth_df = trials_df[trials_df.depth == depth]\n",
    "    # find frames at this depth\n",
    "    # starts = depth_df.imaging_frame_stim_start.values\n",
    "    # ends = depth_df.imaging_frame_stim_stop.values\n",
    "    starts = frame_times.searchsorted(depth_df.harptime_stim_start)\n",
    "    ends = frame_times.searchsorted(depth_df.harptime_stim_stop)\n",
    "    ends = ends[: len(starts)]\n",
    "    frame_index = np.hstack(\n",
    "        [np.arange(s, e, dtype=int) for s, e in zip(starts, ends)]\n",
    "    )\n",
    "    # keep non-shifted spikes for all delay\n",
    "    # do it like that to look for valid frames only once\n",
    "    spk_per_frame_at_depth = spk_per_frame[frame_index]\n",
    "    nspks[idepth] = np.sum(spk_per_frame_at_depth)\n",
    "    valid_frames = spk_per_frame_at_depth != 0\n",
    "    for idelay, delay in enumerate(delays):\n",
    "        if verbose:\n",
    "            print(f\"... ... doing delay {delay * 1000} ms\")\n",
    "        shift = int(delay * frame_rate)\n",
    "        # shift the stim\n",
    "        shifted_frames = np.clip(\n",
    "            frame_index[valid_frames] + shift, 0, len(frame_times)\n",
    "        )\n",
    "        stims = reconstructed_frames[shifted_frames].reshape(\n",
    "            len(shifted_frames), -1\n",
    "        )\n",
    "        sta = np.dot(stims.T, spk_per_frame_at_depth[valid_frames])\n",
    "        sta = sta.reshape(reconstructed_frames.shape[1:])\n",
    "        full_sta[idepth, idelay] = sta\n",
    "        # !! Needs to add normalized STA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/camp/home/hey2/.conda/envs/2p_analysis_cottage/lib/python3.9/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Process protocol 1/1---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nemo/lab/znamenskiyp/home/users/hey2/codes/cottage_analysis/cottage_analysis/filepath/generate_filepaths.py:160: DeprecationWarning: flexilims_session will become mandatory\n",
      "  sess_children = get_session_children(\n",
      "/nemo/lab/znamenskiyp/home/users/hey2/codes/cottage_analysis/cottage_analysis/filepath/generate_filepaths.py:165: DeprecationWarning: flexilims_session will become mandatory\n",
      "  recording_entries, recording_path = get_recording_entries(\n",
      "/nemo/lab/znamenskiyp/home/users/hey2/codes/cottage_analysis/cottage_analysis/filepath/generate_filepaths.py:90: UserWarning: all_protocol_recording_entries not given, assume only one recording for protocol SpheresPermTubeReward in this session.\n",
      "             The last recording that matches this protocoll will be returned.\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-cf6f6d280d12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcottage_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregenerate_stimuli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmouse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nemo/lab/znamenskiyp/home/users/hey2/codes/cottage_analysis/cottage_analysis/analysis/sta.py\u001b[0m in \u001b[0;36mregenerate_stimuli\u001b[0;34m(project, mouse, session, protocol)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     output = common_utils.loop_through_recordings(\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mmouse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmouse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nemo/lab/znamenskiyp/home/users/hey2/codes/cottage_analysis/cottage_analysis/analysis/common_utils.py\u001b[0m in \u001b[0;36mloop_through_recordings\u001b[0;34m(project, mouse, session, protocol, func, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# For each recording, synchronise and produce frames_df, trials_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mirecording\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_protocol_recording_entries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             return func(\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mmouse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmouse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nemo/lab/znamenskiyp/home/users/hey2/codes/cottage_analysis/cottage_analysis/analysis/sta.py\u001b[0m in \u001b[0;36mregenerate_stimuli_each_recording\u001b[0;34m(project, mouse, session, protocols, protocol, irecording, nrecordings)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol_folder\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"sync/imaging_df.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mimaging_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol_folder\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"sync/vs_df.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mvs_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/2p_analysis_cottage/lib/python3.9/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mnew_block\u001b[0;34m(values, placement, ndim, klass)\u001b[0m\n\u001b[1;32m   1929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mnew_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlockPlacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from cottage_analysis.analysis import sta\n",
    "outputs = sta.regenerate_stimuli(project, mouse, session, protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/camp/home/hey2/.conda/envs/2p_analysis_cottage/lib/python3.9/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11795"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2p_analysis_cottage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
