{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and synchronizing ephys data: an example notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to be able to take all the output files that are generated by the ephys setup and Ronan cottage and synchronizing all of them to the same clock.\n",
    "\n",
    "We are using session S20230323 from BRAC4779.2a [HELMET], the first session we recorded. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running list of things ot improve on\n",
    "\n",
    "- As a small note, we have to re-write the bonsai workflow to record the cameras in the arena in a convincing way, so that it inherits the name from the stimulus workflow ideally, like in Yiran's case. Work on that. \n",
    "- The outputs should all go to the same recording folder if they're part of the same recording. Like in Yiran's case. \n",
    "- We want to save the raw output from the chip, so we will have to find a way to remap it later, I guess. DONE\n",
    "- We want to use a bonsai workflow for the stimuli that is able to save a NewParams matrix. DONE\n",
    "- HF cameras need to be timestamped. DONE\n",
    "- Do not name the animal 'cowboy' ever again\n",
    "- The camera is called letfeye_camera instead of lefteye_camera\n",
    "\n",
    "The exposures of all the cameras are 7000 micros. That's not the reason for the frame drop in face cam. \n",
    "I'm trying for 20230406 to only visualize one cam, behaviour_cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cottage_analysis.io_module.onix as onix\n",
    "import cottage_analysis.io_module.harp as harp\n",
    "import cottage_analysis.ephys.preprocessing as prp\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from scipy import stats\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which files were outputted this time?\n",
    "\n",
    "- Three different folders and some loose files \n",
    "    - Outputs of the AcquireEphys\n",
    "    - Outputs of stimulus\n",
    "    - Outputs of HF cameras\n",
    "    - Outputs of FM cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = Path('/camp/lab/znamenskiyp/home/shared/projects/blota_onix_pilote/')\n",
    "data_path = Path('/camp/lab/znamenskiyp/data/instruments/raw_data/projects/blota_onix_pilote/')\n",
    "mouse = 'BRAC7448.2d'\n",
    "session = 'S20230404'\n",
    "session_path = data_path / mouse / session\n",
    "\n",
    "processed_dir = processed_path/mouse/session\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ephys = 'R110119'\n",
    "stimulus = 'R110246_SpheresPermTubeReward'\n",
    "headfixed_cam = '110357' \n",
    "\n",
    "ephys_path = session_path / ephys\n",
    "stimulus_path = session_path / stimulus\n",
    "headfixed_cam_path = session_path / headfixed_cam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the ephys outputs (rhd2164)\n",
    "\n",
    "Ephys outputs start with rhd2164, which is the intan digital electrophysiology interface chip on the headstage. There are three files created, one is aux, one is clock, one is ephys and one is first_time. \n",
    "\n",
    "Antonin wrote a function which takes them all together and makes a memmap object with them, so that they can be easily accessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ephys = onix.load_rhd2164(ephys_path)\n",
    "processed_ephys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attempt to read the ephys outputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import os\n",
    "import spikeinterface.extractors as se\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import spikeinterface as si\n",
    "\n",
    "num_channels = 64\n",
    "sampling_frequency = 30000\n",
    "gain_to_uV = 0.195\n",
    "offset_to_uV = -2**15*gain_to_uV\n",
    "dtype=\"uint16\"#This could be sensitive: our chip writes in uint16, had to change to int16 for kilosort (?)\n",
    "time_axis = 0\n",
    "\n",
    "path_to_openephys = '/camp/lab/znamenskiyp/data/instruments/raw_data/projects/blota_onix_pilote/BRAC7449.2a/S20230323/R115806/rhd2164-ephys_2023-03-23T11_58_06.raw'\n",
    "\n",
    "recording = si.read_binary(path_to_openephys, num_chan=num_channels, sampling_frequency=sampling_frequency,\n",
    "                            dtype=dtype, gain_to_uV=gain_to_uV, offset_to_uV=offset_to_uV,\n",
    "                            time_axis=time_axis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import spikeinterface.widgets as sw\n",
    "\n",
    "#trace_snippet = recording.get_traces(start_frame=int(fs*0), end_frame=int(fs*2))\n",
    "#print('Traces shape:', trace_snippet.shape)\n",
    "\n",
    "import scipy.signal\n",
    "from spikeinterface.preprocessing import (bandpass_filter, notch_filter, common_reference,\n",
    "                                          remove_artifacts, preprocesser_dict)\n",
    "recording_bp = bandpass_filter(recording, freq_min=200, freq_max=10000)\n",
    "\n",
    "recording_cmr = common_reference(recording_bp, reference='global', operator='median')\n",
    "\n",
    "\n",
    "channel_ids = range(30, 40)\n",
    "channel_ids\n",
    "\n",
    "w_ts = sw.plot_timeseries(recording_cmr, channel_ids=channel_ids, time_range=(2000.8, 2001), return_scaled=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading lighthouse data (ts4231)\n",
    "\n",
    "Three different lighthouse-generated files, one per diode. ts4231 is the name of the photodiode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_photodiode = onix.load_ts4231(ephys_path)\n",
    "processed_photodiode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize = (9, 7))\n",
    "\n",
    "fig.suptitle('Value of x for the three photodiodes in the headstage')\n",
    "axs[0].plot(processed_photodiode[1]['clock'], processed_photodiode[1]['x'])\n",
    "axs[1].plot(processed_photodiode[2]['clock'], processed_photodiode[2]['x'])\n",
    "axs[2].plot(processed_photodiode[3]['clock'], processed_photodiode[3]['x'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading IMU data (bno055)\n",
    "\n",
    "We have nothing to load the IMU data. Should I write it? Wrote it on onix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bno = onix.load_bno055(ephys_path)\n",
    "processed_bno.keys()\n",
    "\n",
    "quaternion = processed_bno['quaternion']\n",
    "quaternion = quaternion/2**14\n",
    "linear = processed_bno['linear']\n",
    "linear = linear / 100\n",
    "euler = processed_bno['euler']\n",
    "euler = euler / 16\n",
    "gravity = processed_bno['gravity']\n",
    "gravity = gravity / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how they look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minutes = len(linear[:,1])/100/60\n",
    "\n",
    "print(f'The recording was {minutes} min in length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(9, 9))\n",
    "\n",
    "fig.suptitle('BNO055 (timestamps 20000-201000)')\n",
    "\n",
    "timeslice=range(170000, 170100)\n",
    "\n",
    "axs[0, 0].plot(quaternion[timeslice, 3])\n",
    "axs[0, 0].plot(quaternion[timeslice, 0])\n",
    "axs[0, 0].plot(quaternion[timeslice, 1])\n",
    "axs[0, 0].plot(quaternion[timeslice, 2])\n",
    "axs[0, 0].set_title('Quaternion')\n",
    "\n",
    "axs[0, 1].plot(linear[timeslice, 0])\n",
    "axs[0, 1].plot(linear[timeslice, 1])\n",
    "axs[0, 1].plot(linear[timeslice, 2])\n",
    "axs[0, 1].set_title('Linear acceleration')\n",
    "\n",
    "axs[1, 1].plot(euler[timeslice, 0])\n",
    "axs[1, 1].plot(euler[timeslice, 1])\n",
    "axs[1, 1].plot(euler[timeslice, 2])\n",
    "axs[1, 1].set_title('Euler Vector')\n",
    "\n",
    "axs[1, 0].plot(gravity[timeslice, 0])\n",
    "axs[1, 0].plot(gravity[timeslice, 1])\n",
    "axs[1, 0].plot(gravity[timeslice, 2])\n",
    "axs[1, 0].set_title('Gravity vector')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extract the heading from this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import atan2\n",
    "\n",
    "fq0=quaternion[:,0]\n",
    "fq1=quaternion[:,1]\n",
    "fq2=quaternion[:,2]\n",
    "fq3=quaternion[:,3]\n",
    "heading = np.zeros(len(fq0))\n",
    "#heading[sample] = [atan2((2*(quaternion[0]*quaternion[3]+quaternion[1]*quaternion[2])), (1-2*(quaternion[2]**2+quaternion[3]**2))) for sample in quaternion[0]]\n",
    "\n",
    "for i in range(len(fq0)):\n",
    "    q0, q1, q2, q3 = fq0[i], fq1[i], fq2[i], fq3[i]\n",
    "    heading[i]=atan2((2*(q0*q3+q1*q2)), (1-2*(q2**2+q3**2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(heading[timeslice])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bno['real_time'] = (processed_bno['onix_time']-processed_bno['onix_time'][0])/250e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the stimulus logs and synchronizing them to the ephys clock\n",
    "\n",
    "There is no NewParams matrix, but there are other stuff which I can try to sync, like the rotary encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_file =  stimulus_path / 'NewParams.csv'\n",
    "processed_stimulus = pd.read_csv(param_file)\n",
    "processed_stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the HARP\n",
    "\n",
    "Impossible to do it de novo in jupyter. See the scrap in the notebook folder for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_messages = processed_path / mouse / session / stimulus / 'processed_harp.npz'\n",
    "\n",
    "if Path(processed_messages).is_file():\n",
    "    harp_message = dict(np.load(processed_messages));\n",
    "else:\n",
    "    print('Impossible to read a processed HARP file. If this is  anew recording and you are on a notebook use the scrap on the notebooks folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "harp_message.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synchronizing the HARP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harp_message, harp2onix = prp.sync_harp2onix(harp_message)\n",
    "processed_stimulus['onix_time'] = harp2onix(processed_stimulus['HarpTime'])\n",
    "#Attention: this function takes a harp time and outputs a time NOT IN ONIX TIME but in s since the harp started counting\n",
    "# make a speed out of rotary increament\n",
    "mvt = np.diff(harp_message['rotary']) #The rotary shows position of the machine. One has to play with it to turn it into something that can be actual speed.\n",
    "rollover = np.abs(mvt > 40000) #Create a mask to find the points in which the wheel has made a turn, thus the difference is huge\n",
    "mvt[rollover] -= 2**16 * np.sign(mvt[rollover]) #substracts the value of the rollover from the difference at the points where there's a rollover.\n",
    "# The rotary count decreases when the mouse goes forward\n",
    "mvt *= -1 #inverts the direction of the mouse.\n",
    "harp_message['mouse_speed'] = np.hstack([0, mvt])  # 0-padding to keep constant length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_stimulus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the breakout board \n",
    "\n",
    "Analog and digital inputs to the board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_breakout = onix.load_breakout(ephys_path)\n",
    "print(processed_breakout.keys())\n",
    "processed_breakout['aio']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_breakout['dio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_breakout['dio']\n",
    "print(set(processed_breakout['dio']['Buttons']))\n",
    "start_headfixed = np.where(processed_breakout['dio']['Buttons'] == 8)\n",
    "start_headfixed = processed_breakout['dio']['Clock'][start_headfixed[0][0]]\n",
    "print(f'The head-fixed recording started at clock time {start_headfixed}')\n",
    "end_headfixed = np.where(processed_breakout['dio']['Buttons'] == 16)\n",
    "end_headfixed = processed_breakout['dio']['Clock'][end_headfixed[0][0]]\n",
    "print(f'The head-fixed recording ended at clock time {end_headfixed}')\n",
    "start_freely_moving = np.where(processed_breakout['dio']['Buttons'] == 32)\n",
    "start_freely_moving = processed_breakout['dio']['Clock'][start_freely_moving[0][0]]\n",
    "print(f'The freely moving recording started at clock time {start_freely_moving}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_breakout['dio']['real_time'] = (processed_breakout['dio']['Clock']-processed_breakout['dio']['Clock'][0])/250e6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the freely moving cameras and synchronizing them to the ephys clock"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the clock they have. Read the timestamp dataframes as pandas. Have a look to see if the vseod have the same number of frames as the cameras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_dir = session_path / 'R123342_cowboy'\n",
    "acquisition='freely_moving'\n",
    "\n",
    "def load_camera_times(camera_dir, acquisition):\n",
    "    if acquisition == 'freely_moving':\n",
    "        camlist = ['cam1_camera', 'cam2_camera', 'cam3_camera']\n",
    "    if acquisition == 'headfixed':\n",
    "        camlist = ['letfeye_camera', 'righteye_camera', 'face_camera', 'behaviour_camera']\n",
    "    folder = Path(camera_dir)\n",
    "    if not folder.is_dir():\n",
    "        raise IOError('%s is not a directory' % folder)\n",
    "    output = dict()\n",
    "    for cam in camlist:\n",
    "        cam_folder = folder/cam\n",
    "        valid_files = list(cam_folder.glob('*timestamp*'))\n",
    "        if not len(valid_files):\n",
    "            raise IOError(f'Could not find any timestamp files in {folder}')\n",
    "        for possible_file in valid_files:\n",
    "            possible_file = str(possible_file)\n",
    "            if cam in possible_file:\n",
    "                output[cam] = pd.read_csv(possible_file)\n",
    "    return(output)\n",
    "\n",
    "processed_fm = load_camera_times(camera_dir, acquisition)\n",
    "processed_fm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera synchronization\n",
    "\n",
    "Have to find when FM cameras come online with the DI0 in the digital inputs of the breakout board. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_triggers = np.where(processed_breakout['dio']['DI0']==1)\n",
    "t0 = processed_breakout['dio']['Clock'][fm_triggers[0][0]]\n",
    "fm_trigger_clock = list(processed_breakout['dio']['Clock'][processed_breakout['dio']['DI0']==1])\n",
    "fm_camera_clock = list(processed_fm['cam1_camera']['camera_timestamp'])\n",
    "#I'm doing : trigger_clock=constant+camera_clock*slope\n",
    "slope = (fm_trigger_clock[-1]-fm_trigger_clock[0])/(fm_camera_clock[-1]-fm_camera_clock[0])\n",
    "def cam2onix(data, slope = slope, t0 = t0):\n",
    "    return t0+(slope*data)\n",
    "camlist = ['cam1_camera', 'cam2_camera', 'cam3_camera']\n",
    "for cam in camlist:\n",
    "    processed_fm[cam]['onix_time'] = cam2onix(processed_fm[cam]['camera_timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(fm_trigger_clock))\n",
    "print(len(fm_camera_clock))\n",
    "print(slope)\n",
    "print(t0)\n",
    "plt.plot(processed_breakout['dio']['Clock'], processed_breakout['dio']['DI2'])\n",
    "plt.plot(processed_breakout['dio']['Clock'], processed_breakout['dio']['DI0'])\n",
    "#plt.plot(list(processed_fm['cam1_camera']['onix_time']), np.ones_like(list(processed_fm['cam1_camera']['onix_time'])))\n",
    "plt.axvline(start_freely_moving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_breakout['dio']['Clock'][fm_triggers[0][-1]]\n",
    "fm_trigger_clock\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the camera triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dio = processed_breakout['dio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger = np.array(dio['DI0'])\n",
    "is_noise = np.diff(dio['Clock'])<3000\n",
    "print(is_noise)\n",
    "#plt.hist(np.diff(dio['Clock'][1:][is_noise]), bins = np.linspace(0,200,100))\n",
    "\n",
    "trigger[1:][is_noise] = trigger[:-1][is_noise]\n",
    "#plt.hist(np.diff(is_noise)) \n",
    "\n",
    "on_trigger = dio['Clock'][1:][np.diff(trigger)==1]\n",
    "on_dio = dio['Clock'][1:][np.diff(dio['DI0'])==1]\n",
    "\n",
    "print(len(on_trigger))\n",
    "print(len(on_dio))\n",
    "\n",
    "bins = np.arange(15)\n",
    "\n",
    "\n",
    "plt.hist(np.diff(on_dio)/250e3, bins=bins, histtype = 'step', label = 'dio', lw = 2)\n",
    "plt.hist(np.diff(on_trigger)/250e3, bins = bins, histtype = 'step', label = 'trigger')\n",
    "plt.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to find a way to filter the DI0 output, which is made of a bunch of 7 microseconds sequence of 1s, so that I can get rid of high-frequency noise\n",
    "\n",
    "The onix clock fires at 256e6 Hz, while the digital inputs fire at 5e6 Hz. \n",
    "\n",
    "How does the distribution of delays between turning-on events (from 0 to 1) look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_triggers = dio['DI0']\n",
    "\n",
    "on_triggers = dio['Clock'][1:][np.diff(fm_triggers)==1]\n",
    "off_triggers = dio['Clock'][1:][np.diff(fm_triggers)==255]\n",
    "\n",
    "print(f'The number of turn-on events in dio is {len(on_triggers)}')\n",
    "print(f'The number of turn-off events in dio is {len(off_triggers)}')\n",
    "\n",
    "import cv2\n",
    "\n",
    "camera_dir ='/camp/lab/znamenskiyp/data/instruments/raw_data/projects/blota_onix_pilote/BRAC7448.2d/S20230404/R123342_cowboy/cam3_camera/cam3_camera_2023-04-04T12_33_42.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(camera_dir)\n",
    "\n",
    "n_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "print(f'There are {n_frames} frames in the acquired video, a difference of {n_frames-len(on_triggers)} with the turn-on events')\n",
    "\n",
    "cam1 = processed_fm['cam1_camera']\n",
    "\n",
    "n_metadata_frames = len(cam1['camera_timestamp'])\n",
    "\n",
    "print(f'The number of frames in the camera metadata is {n_metadata_frames}')\n",
    "\n",
    "frame_ids = cam1['frame_id'].values[-1]-cam1['frame_id'].values[0]\n",
    "\n",
    "print(f'However, the difference between the first and last frame_id in the camera is {frame_ids}')\n",
    "print(f'This is a difference of {n_frames-frame_ids} with the frames in the video and {len(on_triggers)-frame_ids} with the turn-on events')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hunting for noise in the DIO signal to discard fake turn-on events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((off_triggers.values-on_triggers.values)/250e3)\n",
    "plt.xlabel('Length (ms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('How long is a DI0 pulse?')\n",
    "longpulses = np.sum(((off_triggers.values-on_triggers.values)/250e3)>3)\n",
    "print(f'There are {longpulses} pulses over 3ms long. This is {(100*longpulses)/len(on_triggers)}% of turn-on events')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a group of very short pulses, and a large group of roughly 6ms pulses, which is our exposition value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.diff(on_triggers), bins = np.linspace(0,500,100))\n",
    "plt.xlabel('Length (onix oscillations)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('How long is a short (fake) DI0 pulse? Very short')\n",
    "\n",
    "print(np.sum((np.diff(on_triggers)/250e6)>0.002))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then. What is a reasonable, however costly, algorithm to get rid of this noise? Two steps: \n",
    "\n",
    "- For every value, if most of the values in a +-400 onix oscillations window from it are different from it, it switches to the other value. \n",
    "- This step gets rid of values that are in the middle of either a pulse or a silent period, not the ones at the edges between a pulse or a silent period. But for those values, it's enough to filter pulses so that we only keep the ones longer than 3ms, because they will for sure be tiny. \n",
    "\n",
    "Alternatively, Antonin suggests median filter with a 400 window. Let's try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import scipy.signal\n",
    "\n",
    "DI_list = ['DI0', 'DI1', 'DI2', 'DI3', 'DI4', 'DI5', 'DI6', 'DI7']\n",
    "\n",
    "filtered_dio = dio\n",
    "\n",
    "for DI in DI_list:\n",
    "    filtered_DI = scipy.signal.medfilt(dio[DI], 399)\n",
    "    filtered_dio[DI] = filtered_DI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 399\n",
    "dio_clock = dio['Clock']\n",
    "start_batch = dio_clock.searchsorted(dio_clock-window)\n",
    "#pandas searchsorted method = Find the indices into a sorted Series self such that, \n",
    "#if the corresponding elements in value were inserted before the indices, the order of self would be preserved.\n",
    "#end_batch = np.clip(dio_clock.searchsorted(dio_clock+window), 0, len(dio_clock))\n",
    "end_batch = dio_clock.searchsorted(dio_clock+window)\n",
    "batch_size = end_batch - start_batch\n",
    "batch_ones = [np.sum(dio['DI0'][s:e]) for s,e in zip(start_batch, end_batch)]\n",
    "median_filtered_1 = batch_ones > batch_size/2\n",
    "median_filtered_0 = batch_ones < batch_size/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_filtered_0 = batch_ones < batch_size/2\n",
    "filtered_DI0_1 = np.where(median_filtered_1==True, 1, dio['DI0'])\n",
    "filtered_DI0 = np.where(median_filtered_0==True, 0, filtered_DI0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dio = dio\n",
    "filtered_dio['DI0'] = filtered_DI0\n",
    "\n",
    "fm_triggers = filtered_dio['DI0']\n",
    "\n",
    "on_triggers = filtered_dio['Clock'][1:][np.diff(fm_triggers)==1]\n",
    "off_triggers = filtered_dio['Clock'][1:][np.diff(fm_triggers)==255]\n",
    "\n",
    "\n",
    "print(f'The number of turn-on events in dio is {len(on_triggers)}')\n",
    "print(f'The number of turn-off events in dio is {len(off_triggers)}')\n",
    "\n",
    "plt.hist((off_triggers.values-on_triggers.values)/250e3)\n",
    "plt.xlabel('Length (ms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('How long is a DI0 pulse?')\n",
    "longpulses = np.sum(((off_triggers.values-on_triggers.values)/250e3)>3)\n",
    "print(f'There are {longpulses} pulses over 3ms long. This is {(100*longpulses)/len(on_triggers)}% of turn-on events')\n",
    "print(f'There are {n_frames} frames in the acquired video, a difference of {n_frames-len(on_triggers)} with the turn-on events')\n",
    "\n",
    "cam1 = processed_fm['cam2_camera']\n",
    "\n",
    "frame_ids = cam1['frame_id'].values[-1]-cam1['frame_id'].values[0]\n",
    "\n",
    "print(f'However, the difference between the first and last frame_id in the camera is {frame_ids}')\n",
    "print(f'This is a difference of {n_frames-frame_ids} with the frames in the video and {len(on_triggers)-frame_ids} with the turn-on events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag between two camera timestamps: \n",
    "\n",
    "About the clock. If 1.022*10^7 are the oscillations between two frames, and one frame is 1/100s, then the clock has 1002243200.0 cycles/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(8, 13))\n",
    "\n",
    "fig.suptitle('Time between two frames (s)')\n",
    "\n",
    "camlist = ['cam1_camera', 'cam2_camera', 'cam3_camera']\n",
    "\n",
    "for i, cam in enumerate(camlist):\n",
    "    lag = np.diff(processed_fm[cam]['camera_timestamp'])\n",
    "    lag=lag/1002243200\n",
    "    axs[i].hist(lag)\n",
    "    axs[i].set_title(cam)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between two frames (should be always one, otherwise there is frame drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(8, 13))\n",
    "\n",
    "camlist = ['cam1_camera', 'cam2_camera', 'cam3_camera']\n",
    "fig.suptitle('Space between two frame_ids')\n",
    "\n",
    "for i, cam in enumerate(camlist):\n",
    "    lag = np.diff(processed_fm[cam]['frame_id'])\n",
    "    axs[i].hist(lag)\n",
    "    axs[i].set_title(cam)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of camera timestamps between two frames to number of triggers between two frames. Should be the frame rate (timestamps/frame), except if frame drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(8, 13))\n",
    "\n",
    "fig.suptitle('Frame rate (seconds/triggered frame)')\n",
    "\n",
    "camlist = ['cam1_camera', 'cam2_camera', 'cam3_camera']\n",
    "\n",
    "for i, cam in enumerate(camlist):\n",
    "    lag = np.diff(processed_fm[cam]['camera_timestamp'])/1002243200\n",
    "    lag_frame = np.diff(processed_fm[cam]['frame_id'])\n",
    "    lag=lag/lag_frame\n",
    "    print(stats.mode(lag))\n",
    "    axs[i].hist(lag)\n",
    "    axs[i].set_title(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "camera_dir ='/camp/lab/znamenskiyp/data/instruments/raw_data/projects/blota_onix_pilote/BRAC7448.2d/S20230404/R123342_cowboy/cam1_camera/cam1_camera_2023-04-04T12_33_42.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(camera_dir)\n",
    "\n",
    "n_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "n_frames\n",
    "\n",
    "print(len(processed_fm['cam1_camera']['camera_timestamp']))\n",
    "print(n_frames)\n",
    "\n",
    "fm_triggers = processed_breakout['dio']['DI0']\n",
    "\n",
    "fm_triggers.value_counts()\n",
    "\n",
    "on_triggers = processed_breakout['dio']['Clock'][1:][np.diff(fm_triggers)==1]\n",
    "off_triggers = processed_breakout['dio']['Clock'][1:][np.diff(fm_triggers)==255]\n",
    "\n",
    "\n",
    "print(len(on_triggers))\n",
    "print(len(off_triggers))\n",
    "\n",
    "\n",
    "cam1 = processed_fm['cam1_camera']\n",
    "\n",
    "cam1['frame_id'].values[-1]-cam1['frame_id'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((off_triggers.values-on_triggers.values)/250e3)\n",
    "np.sum(((off_triggers.values-on_triggers.values)/250e3)>3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.diff(on_triggers), bins = np.linspace(0,500,100))\n",
    "\n",
    "print(np.sum((np.diff(on_triggers)/250e6)>0.002))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the head-fixed cameras and synchronizing them to the ephys clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_dir = session_path / 'R110708_cowboy'\n",
    "acquisition='headfixed'\n",
    "\n",
    "def load_camera_times(camera_dir, acquisition):\n",
    "    if acquisition == 'freely_moving':\n",
    "        camlist = ['cam1_camera', 'cam2_camera', 'cam3_camera']\n",
    "    if acquisition == 'headfixed':\n",
    "        camlist = ['letfeye_camera', 'righteye_camera', 'face_camera', 'behaviour_camera']\n",
    "    folder = Path(camera_dir)\n",
    "    if not folder.is_dir():\n",
    "        raise IOError('%s is not a directory' % folder)\n",
    "    output = dict()\n",
    "    for cam in camlist:\n",
    "        cam_folder = folder/cam\n",
    "        valid_files = list(cam_folder.glob('*timestamp*'))\n",
    "        if not len(valid_files):\n",
    "            raise IOError(f'Could not find any timestamp files in {folder}')\n",
    "        for possible_file in valid_files:\n",
    "            possible_file = str(possible_file)\n",
    "            if cam in possible_file:\n",
    "                output[cam] = pd.read_csv(possible_file)\n",
    "    return(output)\n",
    "\n",
    "processed_hf = load_camera_times(camera_dir, acquisition)\n",
    "processed_hf\n",
    "#ignore index or keep it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 1, figsize=(10, 18))\n",
    "\n",
    "fig.suptitle('Time between two frames (s)')\n",
    "\n",
    "camlist = ['letfeye_camera', 'righteye_camera', 'face_camera', 'behaviour_camera']\n",
    "\n",
    "for i, cam in enumerate(camlist):\n",
    "    lag = np.diff(processed_hf[cam]['camera_timestamp'])\n",
    "    lag=lag/1002243200\n",
    "    axs[i].hist(lag)\n",
    "    axs[i].set_title(cam)\n",
    "\n",
    "#face exposure?? run only this one to see if it's still losing it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot what I did\n",
    "harp_onix_clock = harp_message['onix_clock']\n",
    "real_time = np.arange(len(harp_onix_clock)) * 10e-3  # assume a perfect 100Hz\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "ax.clear()\n",
    "ax.hist(np.diff(harp_onix_clock)*1000)\n",
    "ax.set_xlabel('Intertrigger time (ms)')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "ax.clear()\n",
    "ax.plot(harp_onix_clock, real_time, '.')\n",
    "ax.set_xlabel('Harp time')\n",
    "ax.set_ylabel('Onix time')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "ax.clear()\n",
    "ax.plot(real_time, (harp2onix(harp_onix_clock) - real_time) * 1000, 'o')\n",
    "ax.set_xlabel('Onix time')\n",
    "ax.set_ylabel('Error (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_trace = processed_stimulus.Depth.values\n",
    "corridor_starts, = np.where(np.hstack([True, np.diff(depth_trace) > 4000]))\n",
    "corridor_ends, = np.where(np.hstack([False, np.diff(depth_trace) < -4000]))\n",
    "# make a simpler dataframe\n",
    "corridor_df = []\n",
    "for s, e in zip(corridor_starts, corridor_ends):\n",
    "    corridor_df.append(dict(start_index=s, start_time=processed_stimulus.onix_time.iloc[s],\n",
    "                            end_index=e, end_time=processed_stimulus.onix_time.iloc[e],\n",
    "                            depth=processed_stimulus.Depth[s]))\n",
    "corridor_df = pd.DataFrame(corridor_df)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.clf()\n",
    "fig.set_size_inches(18.5, 18.5)\n",
    "ax0 = fig.add_subplot(5, 1, 1)\n",
    "ax0.plot(processed_stimulus.onix_time.values, processed_stimulus.Depth.values)\n",
    "depths = corridor_df.depth.unique()\n",
    "cmap = plt.get_cmap('Set2', len(depths))\n",
    "depth_color = {d: cmap(i) for i, d in enumerate(depths)}\n",
    "ax0.set_ylabel('Depth (cm)')\n",
    "ax0.set_ylim(0, 630)\n",
    "ax1 = fig.add_subplot(5, 1, 2, sharex=ax0)\n",
    "#ax1.plot(harp_message['analog_onix_time'], harp_message['mouse_speed'], alpha=0.2)\n",
    "# make a moving average on a 100ms window (assuming 1kHz sampling)\n",
    "w = int(0.5 * 1000)\n",
    "cs = np.cumsum(harp_message['mouse_speed'])\n",
    "smooth_speed = (cs[w:] - cs[:-w]) / w\n",
    "ax1.plot(harp_message['analog_time_onix'][int(w/2):-int(w/2)], smooth_speed)\n",
    "ax1.set_ylabel('Running speed')\n",
    "ax1.set_ylim([-2, 3])\n",
    "ax0.set_xlim([0, 6800])\n",
    "for x in [ax0, ax1]:\n",
    "    for i_c, cdf in corridor_df.iterrows():\n",
    "        x.axvspan(cdf.start_time, cdf.end_time, alpha=0.2, color=depth_color[cdf.depth])\n",
    "ax2 = fig.add_subplot(5, 1, 3, sharex=ax0)\n",
    "ax2.plot(processed_bno['real_time'], linear[0:len(processed_bno['real_time']),1])\n",
    "ax2.plot(processed_bno['real_time'], linear[0:len(processed_bno['real_time']),2])\n",
    "ax2.plot(processed_bno['real_time'], linear[0:len(processed_bno['real_time']),0])\n",
    "ax2.set_ylabel('Linear acceleration')\n",
    "ax3 = fig.add_subplot(5, 1, 4, sharex=ax0)\n",
    "ax3.plot(processed_bno['real_time'], gravity[0:len(processed_bno['real_time']),1])\n",
    "ax3.plot(processed_bno['real_time'], gravity[0:len(processed_bno['real_time']),2])\n",
    "ax3.plot(processed_bno['real_time'], gravity[0:len(processed_bno['real_time']),0])\n",
    "ax3.set_ylabel('Gravity vector')\n",
    "ax4 = fig.add_subplot(5, 1, 5, sharex = ax0)\n",
    "ax4.plot(processed_breakout['dio']['real_time'], processed_breakout['dio']['DI2'])\n",
    "ax4.plot(processed_breakout['dio']['real_time'], processed_breakout['dio']['DI0'])\n",
    "ax4.set_ylabel('Camera triggers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_breakout['dio']['Clock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(9, 9))\n",
    "\n",
    "fig.suptitle('BNO055 (timestamps 20000-201000)')\n",
    "\n",
    "timeslice=range(170000, 170100)\n",
    "\n",
    "axs[0, 0].plot(quaternion[timeslice, 3])\n",
    "axs[0, 0].plot(quaternion[timeslice, 0])\n",
    "axs[0, 0].plot(quaternion[timeslice, 1])\n",
    "axs[0, 0].plot(quaternion[timeslice, 2])\n",
    "axs[0, 0].set_title('Quaternion')\n",
    "\n",
    "axs[0, 1].plot(linear[timeslice, 0])\n",
    "axs[0, 1].plot(linear[timeslice, 1])\n",
    "axs[0, 1].plot(linear[timeslice, 2])\n",
    "axs[0, 1].set_title('Linear acceleration')\n",
    "\n",
    "axs[1, 1].plot(euler[timeslice, 0])\n",
    "axs[1, 1].plot(euler[timeslice, 1])\n",
    "axs[1, 1].plot(euler[timeslice, 2])\n",
    "axs[1, 1].set_title('Euler Vector')\n",
    "\n",
    "axs[1, 0].plot(gravity[timeslice, 0])\n",
    "axs[1, 0].plot(gravity[timeslice, 1])\n",
    "axs[1, 0].plot(gravity[timeslice, 2])\n",
    "axs[1, 0].set_title('Gravity vector')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cottage_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68c78f8a4a4531c50d9d98d9d580a9dd67d4e2e39ba1bb3e7bf67526fb2ff967"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
