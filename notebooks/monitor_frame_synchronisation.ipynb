{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d7cce3",
   "metadata": {},
   "source": [
    "# Visual stimulation monitor synchronisation\n",
    "\n",
    "Bonsai controls visual display and saves a log everytime it asks for a frame to be rendered. However there is an unknown delay (~2 frames) between this render frame event and the actual display time. Furthermore some frames are skipped.\n",
    "\n",
    "To figure out which frame is displayed when we put a photodiode in front of the monitor and display a pseudo-random sequence of alternating grey value. \n",
    "\n",
    "This notebook show how we use the frame logger and the photodiode signal to determine exact frame identity at each point of time. It has 3 main steps\n",
    "\n",
    "**1. Detect frames on photodiode signal**\n",
    "\n",
    "**2. Cross-correlate frame with sequence to find expected lag**\n",
    "\n",
    "**3. Match cross correlation results to frame logger**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64fe5d3",
   "metadata": {},
   "source": [
    "\n",
    "### Load example data\n",
    "\n",
    "This next section just loads one example recording to be used for the rest of the notebook.\n",
    "\n",
    "Define the session we want:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90e896",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c3cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"hey2_3d-vision_foodres_20220101\"\n",
    "MOUSE = \"PZAH8.2i\"\n",
    "SESSION = \"S20230209\"\n",
    "RECORDING = \"R174123_SpheresPermTubeRewardPlayback\"\n",
    "PROTOCOL = \"SpheresPermTubeReward\"\n",
    "MESSAGES = \"harpmessage.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf24fdb",
   "metadata": {},
   "source": [
    "Load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2857981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import flexiznam as flz\n",
    "from cottage_analysis.io_module import harp\n",
    "\n",
    "flm_sess = flz.get_flexilims_session(PROJECT)\n",
    "sess = flz.get_entity(name=f\"{MOUSE}_{SESSION}\", flexilims_session=flm_sess)\n",
    "recs = flz.get_children(\n",
    "    parent_id=sess.id, flexilims_session=flm_sess, children_datatype=\"recording\"\n",
    ")\n",
    "print(f\"Found {len(recs)} recordings for {MOUSE}_{SESSION}\")\n",
    "recs[recs.protocol == PROTOCOL].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = flz.PARAMETERS[\"data_root\"]\n",
    "recording = flz.get_entity(\n",
    "    name=f\"{MOUSE}_{SESSION}_{RECORDING}\", flexilims_session=flm_sess\n",
    ")\n",
    "\n",
    "harp_message, harp_ds = harp.load_harpmessage(\n",
    "    recording,\n",
    "    flexilims_session=flm_sess,\n",
    "    conflicts=\"skip\",\n",
    "    di_names=None,\n",
    ")\n",
    "harp_csvs = harp_ds.extra_attributes[\"csv_files\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_log = pd.read_csv(harp_ds.path_full / harp_csvs[\"FrameLog\"])\n",
    "expected_sequence = (\n",
    "    pd.read_csv(\n",
    "        harp_ds.path_full / harp_csvs[\"random_sequence_5values_alternate\"], header=None\n",
    "    )\n",
    "    .loc[:, 0]\n",
    "    .values\n",
    ")\n",
    "step_values = frame_log.PhotoQuadColor.unique()\n",
    "ao_time = harp_message[\"analog_time\"]\n",
    "photodiode = harp_message[\"photodiode\"]\n",
    "ao_sampling = 1 / np.mean(np.diff(ao_time))\n",
    "\n",
    "print(\"Data loaded.\")\n",
    "print(\n",
    "    \"Recording is %d s long.\"\n",
    "    % (frame_log.HarpTime.values[-1] - frame_log.HarpTime.values[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63be585",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(harp_messages.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae8f2f",
   "metadata": {},
   "source": [
    "# Normal usage\n",
    "\n",
    "This is using the main master function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cottage_analysis.preprocessing import find_frames\n",
    "\n",
    "processed = flz.get_data_root(\"processed\", project=PROJECT)\n",
    "\n",
    "\n",
    "frame_rate = 144\n",
    "frame_detection_height = 0.1\n",
    "correlation_threshold = 0.8\n",
    "relative_corr_thres = 0.02\n",
    "expected_lag = 2.0 / frame_rate\n",
    "maxlag = 3.0 / frame_rate\n",
    "minimum_lag = 1.0 / frame_rate\n",
    "num_frame_to_corr = 6\n",
    "time_column = \"HarpTime\"\n",
    "sequence_column = \"PhotoQuadColor\"\n",
    "save_folder = (\n",
    "    processed / PROJECT / MOUSE / SESSION / RECORDING / \"diagnostics\" / \"frame_sync\"\n",
    ")\n",
    "print(f\"Saving to {save_folder}\")\n",
    "\n",
    "frames_df, db_dict = find_frames.sync_by_correlation(\n",
    "    frame_log,\n",
    "    ao_time,\n",
    "    photodiode,\n",
    "    time_column=time_column,\n",
    "    sequence_column=sequence_column,\n",
    "    num_frame_to_corr=num_frame_to_corr,\n",
    "    maxlag=maxlag,\n",
    "    expected_lag=expected_lag,\n",
    "    frame_rate=frame_rate,\n",
    "    correlation_threshold=correlation_threshold,\n",
    "    relative_corr_thres=relative_corr_thres,\n",
    "    frame_detection_height=frame_detection_height,\n",
    "    minimum_lag=minimum_lag,\n",
    "    do_plot=True,\n",
    "    verbose=True,\n",
    "    debug=True,\n",
    "    save_folder=save_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e05db",
   "metadata": {},
   "source": [
    "# Detailled description\n",
    "\n",
    "How does it work? The alignment is made in 3 steps:\n",
    "\n",
    "- detect frames\n",
    "- crosscorrelated with expected sequence\n",
    "- align results\n",
    "\n",
    "## Detect frames\n",
    "\n",
    "The frame detection is simple: filter a bit to smooth local extrema, `diff` to find fast changes and detect peaks on that `diff` trace. This should detect all frame borders. In between these borders, look for the `diff` minimum to find the frame peak (be it a maximum or a minium)\n",
    "\n",
    "Detection can be done independently using `detect_frame_onset`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create frame df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "photodiode_time = ao_time\n",
    "pd_sampling = 1 / np.mean(np.diff(photodiode_time))\n",
    "\n",
    "# Normalise photodiode signal\n",
    "normed_pd = np.array(photodiode, dtype=float)\n",
    "normed_pd -= np.quantile(normed_pd, 0.01)\n",
    "normed_pd /= np.quantile(normed_pd, 0.99)\n",
    "photodiode_signal = normed_pd\n",
    "\n",
    "height = frame_detection_height\n",
    "\n",
    "# First step: Frame detection\n",
    "\n",
    "\n",
    "pd_sampling = 1 / np.mean(np.diff(ao_time))\n",
    "out = find_frames.detect_frame_onset(\n",
    "    photodiode=photodiode,\n",
    "    frame_rate=frame_rate,\n",
    "    photodiode_sampling=pd_sampling,\n",
    "    highcut=frame_rate * 3,\n",
    "    debug=True,\n",
    "    height=height,\n",
    ")\n",
    "frame_borders, peak_index, db_dict = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut frame detected before the recording started\n",
    "t0 = frame_log[time_column].iloc[0]\n",
    "to_cut = photodiode_time[frame_borders].searchsorted(t0)\n",
    "print(f\"Cutting {to_cut} frames detected before the recording started\")\n",
    "\n",
    "frame_borders = frame_borders[to_cut:]\n",
    "peak_index = peak_index[to_cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_skip = np.diff(frame_borders) > pd_sampling / frame_rate * 1.5\n",
    "frames_df = pd.DataFrame(\n",
    "    dict(\n",
    "        onset_sample=frame_borders[:-1],\n",
    "        offset_sample=frame_borders[1:],\n",
    "        peak_sample=peak_index,\n",
    "        include_skip=frame_skip,\n",
    "    )\n",
    ")\n",
    "frames_df[\"onset_time\"] = photodiode_time[frames_df.onset_sample]\n",
    "frames_df[\"offset_time\"] = photodiode_time[frames_df.offset_sample]\n",
    "frames_df[\"peak_time\"] = photodiode_time[frames_df.peak_sample]\n",
    "\n",
    "# check if frames are detected after the presentation is over\n",
    "after_last = frames_df[\"onset_time\"] >= frame_log[time_column].iloc[-1]\n",
    "print(f\"Framed detected after the presentation is over: {after_last.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_frames.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_presentation = frame_log[time_column].iloc[-1]\n",
    "last_frame = frames_df[\"onset_time\"].iloc[-1]\n",
    "print(f\"Presentation ends at {end_of_presentation:.2f} s\")\n",
    "print(f\"Last frame detected at {last_frame:.2f} s\")\n",
    "\n",
    "delay = frames_df[\"onset_time\"].iloc[-1] - frame_log[time_column].iloc[-1]\n",
    "print(f\"{after_last.sum()} frames detected after the last render time.\")\n",
    "\n",
    "b, e = photodiode_time.searchsorted([end_of_presentation - 1, last_frame + 20])\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_title(\"Recording ends before last frame detected\")\n",
    "ax.plot(\n",
    "    photodiode_time[b:e] - end_of_presentation,\n",
    "    photodiode_signal[b:e],\n",
    "    label=\"photodiode\",\n",
    ")\n",
    "ax.axvline(0, color=\"k\", label=\"end of presentation\")\n",
    "late_frames = frames_df[frames_df.onset_time > end_of_presentation]\n",
    "ax.scatter(\n",
    "    late_frames.peak_time - end_of_presentation,\n",
    "    photodiode_signal[late_frames.peak_sample],\n",
    "    label=\"late frame\",\n",
    "    color=\"purple\",\n",
    ")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"Time relative to end of presentation (s)\")\n",
    "ax.set_ylabel(\"Photodiode signal (a.u.)\")\n",
    "save_folder = None\n",
    "if save_folder is not None:\n",
    "    fig.savefig(Path(save_folder) / f\"presentation_end_issue.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e02230",
   "metadata": {},
   "source": [
    "You can get an example of detection using `plot_frame_detection_report`. This will give you `num_examples * 2` figures. Half of them are selected on random frames, half are centered around a frame drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_errors = True\n",
    "ndetected = len(frames_df)\n",
    "npresented = len(frame_log)\n",
    "if npresented < ndetected:\n",
    "    msg = (\n",
    "        f\"Detected more frames ({ndetected}) than presented ({npresented})\"\n",
    "        \"\\n Check create_frame_df parameters\"\n",
    "    )\n",
    "elif npresented > ndetected * 2:\n",
    "    msg = (\n",
    "        f\"Dropped more than half of the frames ({npresented - ndetected} dropped)\"\n",
    "        \"\\n Check create_frame_df parameters\"\n",
    "    )\n",
    "else:\n",
    "    msg = None\n",
    "if msg is not None:\n",
    "    if ignore_errors:\n",
    "        warnings.warn(msg)\n",
    "    else:\n",
    "        raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc9fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_window = np.array([-7.5, 7.5]) / frame_rate * pd_sampling\n",
    "figs = find_frames.plot_frame_detection_report(\n",
    "    border_index=frame_borders,\n",
    "    peak_index=peak_index,\n",
    "    debug_dict=db_dict,\n",
    "    num_examples=1,\n",
    "    plot_window=plot_window,\n",
    "    photodiode=photodiode,\n",
    "    frame_rate=frame_rate,\n",
    "    photodiode_sampling=pd_sampling,\n",
    "    highcut=frame_rate * 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e4728",
   "metadata": {},
   "source": [
    "# Crosscorrelation\n",
    "\n",
    "After having detected frames we will try to find where each of them falls in the photodiode sequence. To do that, we start by normalising the photodiode signal between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2637d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_df, db_di = find_frames.run_cross_correlation(\n",
    "    frames_df,\n",
    "    frame_log,\n",
    "    photodiode_time,\n",
    "    normed_pd,\n",
    "    time_column,\n",
    "    sequence_column,\n",
    "    num_frame_to_corr,\n",
    "    maxlag,\n",
    "    expected_lag,\n",
    "    frame_rate,\n",
    "    verbose=True,\n",
    "    debug=True,\n",
    "    pd_sampling=pd_sampling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_df = find_frames._match_fit_to_logger(\n",
    "    frames_df,\n",
    "    correlation_threshold=correlation_threshold,\n",
    "    relative_corr_thres=relative_corr_thres,\n",
    "    minimum_lag=minimum_lag,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Then interpolate the missing frames\n",
    "find_frames.interpolate_sync(frames_df, verbose=True)\n",
    "# and remove the last double detected frames\n",
    "frames_df = find_frames._remove_double_frames(frames_df, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dict.update(db_di)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "find_frames.plot_crosscorr_matrix(\n",
    "    ax, db_dict[\"cc_dict\"], db_dict[\"lags_sample\"], frames_df\n",
    ")\n",
    "ax = fig.add_subplot(2, 1, 2)\n",
    "find_frames.plot_crosscorr_matrix(\n",
    "    ax, db_dict[\"cc_dict\"], db_dict[\"lags_sample\"], frames_df\n",
    ")\n",
    "xl = ax.get_xlim()\n",
    "mid = (xl[0] + xl[1]) / 2\n",
    "ax.set_xlim(mid - 100, mid + 100)\n",
    "fig.savefig(save_folder / \"crosscorr_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(frames_df.closest_frame_log_index.values)\n",
    "\n",
    "ok = frames_df.closest_frame_log_index.values >= (\n",
    "    frames_df.closest_frame_log_index.iloc[-1] - 1\n",
    ")\n",
    "\n",
    "# plt.plot(frames_df.closest_frame_log_index.values)\n",
    "ok.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = frames_df.index[919623 - 150]\n",
    "fig = find_frames.plot_one_frame_check(\n",
    "    frame,\n",
    "    frames_df,\n",
    "    frame_log,\n",
    "    real_time=photodiode_time,\n",
    "    normed_pd=normed_pd,\n",
    "    ideal_time=db_dict[\"ideal_time\"],\n",
    "    ideal_pd=db_dict[\"ideal_photodiode_trace\"],\n",
    "    ideal_seqi=db_dict[\"ideal_seqi_trace\"],\n",
    "    num_frame_to_corr=None,\n",
    ")\n",
    "fig.suptitle(\n",
    "    f\"Frame {frame} matching frame log \"\n",
    "    f\"{frames_df.loc[frame, 'closest_frame']}\\n\"\n",
    "    f\"Is interpolated: {not frames_df.loc[frame, 'interpolation_seeds']}\"\n",
    ")\n",
    "fig.savefig(save_folder / f\"frame_{frame}_check.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54044fba",
   "metadata": {},
   "source": [
    "### Idealised photodiode\n",
    "\n",
    "Then we generate an idealised version of what the photodiode signal should be (had their been no frame drops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b32dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_trace, ideal_pd = find_frames.ideal_photodiode(\n",
    "    time_base=ao_time,\n",
    "    switch_time=frame_log[\"HarpTime\"].values,\n",
    "    sequence=frame_log[\"PhotoQuadColor\"].values,\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "w = np.array([10000, 10100])\n",
    "t0 = ao_time[w[0]]\n",
    "plt.plot(ao_time[slice(*w)] - t0, normed_pd[slice(*w)], label=\"Normed photodiode\")\n",
    "plt.plot(\n",
    "    ao_time[slice(*w)] - t0,\n",
    "    seq_trace[slice(*w)],\n",
    "    label=\"Sequence\",\n",
    "    color=\"grey\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.plot(ao_time[slice(*w)] - t0, ideal_pd[slice(*w)], label=\"Filtered sequence\")\n",
    "l = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75593f0d",
   "metadata": {},
   "source": [
    "### Data chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13059ae",
   "metadata": {},
   "source": [
    "Now we want to run the crosscorrelation around each frame.\n",
    "\n",
    "We need to take a chunk of data that is big enough but short enough. Five or 6 frames seems to get good unique match with the sequence. Use `num_frame_to_corr` to set that.\n",
    "\n",
    "Then we need to shift the photodiode by a given lag and cut the same chunk of data to correlate. There is no point in testing all the shifts, we now it will be about 2 frames. So we have `expected_lag ~= int(2/frame_rate*ao_sampling)` (in samples). \n",
    "\n",
    "To make things reasonably fast we also limit the search to a 3 frames of lag (+/- around expected_lag). With `maxlag ~= int(3/frame_rate*ao_sampling)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frame_to_corr = 6\n",
    "maxlag_samples = int(np.round(3 / frame_rate * ao_sampling))  # make it into samples\n",
    "expected_lag_samples = int(\n",
    "    np.round(2 / frame_rate * ao_sampling)\n",
    ")  # make it into samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db73bc",
   "metadata": {},
   "source": [
    "Finally we need to decide if we take the chunk of data before the frame, centered on the frame or after the frame. The best choice depends on if there was a frame drop recently or not. So let's just do the 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = [\n",
    "    np.array([-1, 1]) * maxlag_samples\n",
    "    + np.array(w * num_frame_to_corr / frame_rate * ao_sampling, dtype=\"int\")\n",
    "    for w in [np.array([-1, 0]), np.array([-0.5, 0.5]), np.array([0, 1])]\n",
    "]\n",
    "# for bef window, we add 1 frame to have the current frame included\n",
    "window[0] += int(1 / frame_rate * ao_sampling)\n",
    "# for center window, we shift by 0.5 frame to center\n",
    "window[1] += int(0.5 / frame_rate * ao_sampling)\n",
    "\n",
    "example_frame = 5234\n",
    "frame_sample = frame_borders[example_frame]\n",
    "lab = [\"bef\", \"center\", \"aft\"]\n",
    "t0 = ao_time[frame_sample]\n",
    "for iw, w in enumerate(window):\n",
    "    part = slice(*w + frame_sample)\n",
    "    plt.plot(\n",
    "        ao_time[part][maxlag_samples : -maxlag_samples + 1] - t0,\n",
    "        normed_pd[part][maxlag_samples : -maxlag_samples + 1] + iw * 0.5,\n",
    "        label=lab[iw],\n",
    "    )\n",
    "plt.axvspan(\n",
    "    ao_time[frame_sample] - t0,\n",
    "    ao_time[frame_borders[example_frame + 1]] - t0,\n",
    "    alpha=0.5,\n",
    ")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5771c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlag_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.hist(\n",
    "    frames_df.lag.values * 1000,\n",
    "    bins=np.arange(frames_df.lag.min() * 1000, frames_df.lag.max() * 1000),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb39769",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = np.diff(frames_df.closest_frame.values) < 1\n",
    "frames_df.iloc[1:][bad]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bca33c3",
   "metadata": {},
   "source": [
    "Add that to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dad53a",
   "metadata": {},
   "source": [
    "## Match cross correlation results to frame logger\n",
    "\n",
    "Ideally, if there is no frame drop, it does not matter if we look at the frames perceeding or following the frame we want to sync. That should be most of the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66934d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db_dict[\"debug_info\"]\n",
    "normed_pd = np.array(photodiode, dtype=float)\n",
    "normed_pd -= np.quantile(normed_pd, 0.01)\n",
    "normed_pd /= np.quantile(normed_pd, 0.99)\n",
    "db.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2de6eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(102)\n",
    "w = frames_df[frames_df.sync_reason == \"photodiode matching\"].index\n",
    "random_select = [w[i] for i in rng.integers(len(w), size=10)]\n",
    "bad = np.diff(frames_df.closest_frame.values) < 1\n",
    "badi = np.where(bad)[0]\n",
    "random_select = frames_df.iloc[badi[20] + np.array([0, 1, 2], dtype=int)].index\n",
    "labels = [\"bef\", \"center\", \"aft\"]\n",
    "num_frame_to_corr = 5\n",
    "maxlag = int(5.0 / frame_rate * ao_sampling)\n",
    "expected_lag = int(2.0 / frame_rate * ao_sampling)\n",
    "window = [\n",
    "    np.array([-1, 1]) * maxlag\n",
    "    + np.array(w * num_frame_to_corr / frame_rate * ao_sampling, dtype=\"int\")\n",
    "    for w in [np.array([-1, 0]), np.array([-0.5, 0.5]), np.array([0, 1])]\n",
    "]\n",
    "seq_trace = db[\"seq_trace\"]\n",
    "\n",
    "for frame in random_select:\n",
    "    # frame = frames_df[~good].index[num]\n",
    "    # frame = frames_df.index[num]\n",
    "    fseries = frames_df.loc[frame]\n",
    "    on_s = fseries.onset_sample\n",
    "    off_s = fseries.offset_sample\n",
    "    on_t = fseries.onset_time\n",
    "    off_t = fseries.offset_time\n",
    "    w = np.array([-50, 50])\n",
    "    vfdf = frames_df[\n",
    "        (frames_df.onset_sample > w[0] + on_s)\n",
    "        & (frames_df.offset_sample < w[1] + off_s)\n",
    "    ]\n",
    "    qc = np.array([fseries[[\"quadcolor_%s\" % w for w in labels]]])\n",
    "    best = fseries.crosscorr_picked\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    plt.gca().get_yaxis().set_visible(False)\n",
    "\n",
    "    col = dict(bef=\"r\", center=\"g\", aft=\"b\")\n",
    "    for i in range(3):\n",
    "        label = \"Photodiode\" if i == 1 else None\n",
    "        plt.plot(\n",
    "            ao_time[slice(*w + on_s)] - on_t,\n",
    "            normed_pd[slice(*w + on_s)] + i,\n",
    "            label=label,\n",
    "            color=\"purple\",\n",
    "        )\n",
    "        label = \"Frame #%d\" % frame if i == 1 else None\n",
    "        plt.axvspan(0, off_t - on_t, color=\"purple\", alpha=0.2, label=label)\n",
    "        plt.plot(fseries.peak_time - on_t, fseries.photodiode, \"o\", color=\"purple\")\n",
    "\n",
    "    vlog = frame_log[\n",
    "        (frame_log.HarpTime > w[0] / ao_sampling + on_t - fseries.lag_bef)\n",
    "        & (frame_log.HarpTime < w[1] / ao_sampling + off_t)\n",
    "    ]\n",
    "    plt.plot(\n",
    "        vlog.HarpTime.values - on_t,\n",
    "        vlog.PhotoQuadColor - 1.5,\n",
    "        drawstyle=\"steps-post\",\n",
    "        label=\"Render frame\",\n",
    "    )\n",
    "\n",
    "    i = 0\n",
    "    for win, lab in zip(window, [\"bef\", \"center\", \"aft\"]):\n",
    "        cut_win = win + maxlag * np.array([1, -1], dtype=int)\n",
    "        l = fseries[\"lag_%s\" % lab]\n",
    "        part = seq_trace[slice(*win + on_s)]\n",
    "        cut_part = seq_trace[slice(*cut_win + on_s)]\n",
    "        x = normed_pd[slice(*win + on_s)][maxlag : -maxlag + 1]\n",
    "\n",
    "        plt.plot(\n",
    "            ao_time[slice(*win + on_s)] - on_t + l,\n",
    "            part + i,\n",
    "            alpha=0.75,\n",
    "            lw=2,\n",
    "            color=col[lab],\n",
    "        )\n",
    "        plt.plot(\n",
    "            ao_time[slice(*win + on_s)][maxlag : -maxlag + 1] - on_t,\n",
    "            x + i,\n",
    "            alpha=0.5,\n",
    "            lw=4,\n",
    "            ls=\"--\",\n",
    "            color=col[lab],\n",
    "        )\n",
    "\n",
    "        cl = fseries[\"closest_frame_%s\" % lab]\n",
    "        plt.plot(\n",
    "            frame_log.iloc[cl].HarpTime - on_t,\n",
    "            frame_log.iloc[cl].PhotoQuadColor - 1.5 + i / 6,\n",
    "            \"o\",\n",
    "            color=col[lab],\n",
    "        )\n",
    "        if lab == best:\n",
    "            plt.plot(\n",
    "                frame_log.iloc[cl].HarpTime - on_t,\n",
    "                frame_log.iloc[cl].PhotoQuadColor - 1.5 + i / 6,\n",
    "                \"o\",\n",
    "                mfc=\"None\",\n",
    "                mec=\"k\",\n",
    "                ms=10,\n",
    "                mew=2,\n",
    "            )\n",
    "            plt.plot(\n",
    "                frame_log.iloc[cl].HarpTime - on_t + l,\n",
    "                frame_log.iloc[cl].PhotoQuadColor + i,\n",
    "                \"o\",\n",
    "                color=\"k\",\n",
    "            )\n",
    "            plt.plot(\n",
    "                ao_time[slice(*cut_win + on_s)] - on_t + l,\n",
    "                cut_part + i,\n",
    "                alpha=1,\n",
    "                lw=1,\n",
    "                color=\"k\",\n",
    "                label=\"Selected match\",\n",
    "            )\n",
    "\n",
    "        i += 1\n",
    "        plt.title(\"%s\" % fseries.onset_sample)\n",
    "\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4230d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77caab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find what is the actual photodiode value and how does it depend on previous value\n",
    "\n",
    "df = pd.DataFrame(frames_df.iloc[1:][[\"quadcolor\", \"photodiode\"]]).reset_index()\n",
    "bef = pd.DataFrame(frames_df.iloc[:-1][[\"quadcolor\", \"photodiode\"]]).reset_index()\n",
    "df[\"quadcolor_before\"] = bef[\"quadcolor\"]\n",
    "df[\"photodiode_before\"] = bef[\"photodiode\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_data = (\n",
    "    df.groupby([\"quadcolor_before\", \"quadcolor\"]).aggregate(np.nanmean).photodiode\n",
    ")\n",
    "n_data = df.groupby([\"quadcolor_before\", \"quadcolor\"]).aggregate(len).photodiode\n",
    "m = mat_data.values.reshape((5, 5))\n",
    "n = n_data.values.reshape((5, 5))\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(m.T, origin=\"lower\")\n",
    "cm = plt.colorbar()\n",
    "plt.xlabel(\"quad n-1\")\n",
    "plt.ylabel(\"quad n\")\n",
    "plt.title(\"Photodiode\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Difference\")\n",
    "plt.imshow((m - np.linspace(0, 1, 5)).T, origin=\"lower\", cmap=\"RdBu_r\")\n",
    "plt.xlabel(\"quad n-1\")\n",
    "plt.ylabel(\"quad n\")\n",
    "cm = plt.colorbar()\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"N transitions\")\n",
    "plt.imshow(n.T, origin=\"lower\")\n",
    "plt.xlabel(\"quad n-1\")\n",
    "plt.ylabel(\"quad n\")\n",
    "cm = plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e70e6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac69145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d09668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7beb44f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c1be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742d37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7864b0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda39a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fd0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362fd146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f489290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebacb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313ba96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914de51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff837f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd1199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8f89f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "fseries = frames_df.loc[4713]\n",
    "fseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = fseries.offset_time\n",
    "frame_log[\"HarpTime\"][1940:1955] - fseries.lag_aft - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b43e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_df.sync_reason.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ac6fb",
   "metadata": {},
   "source": [
    "# Divers stuff\n",
    "\n",
    "Figures to explain things for my lab meeting (09/11/2022)\n",
    "\n",
    "## Sequence principle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "b = 1000\n",
    "w = 100\n",
    "shift = 1\n",
    "0\n",
    "seq = seq_trace[b : b + w]\n",
    "bad_seq = np.array(seq_trace[b + shift : b + w + shift])\n",
    "bad_seq[int(w / 3) : int(w / 3 + w / 3 * 0.6)] = bad_seq[int(w / 3)]\n",
    "ax.plot((ao_time[b : b + w] - ao_time[b]) * 1e3, bad_seq)\n",
    "ax.plot((ao_time[b : b + w] - ao_time[b]) * 1e3, seq + 1)\n",
    "ax.set_xlabel(\"Time (ms)\")\n",
    "ax.yaxis.set_visible(False)\n",
    "for w in [\"top\", \"left\", \"right\"]:\n",
    "    ax.spines[w].set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
