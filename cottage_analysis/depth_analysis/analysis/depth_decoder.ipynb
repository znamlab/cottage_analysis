{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import defopt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import wilcoxon\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42 # save text as text not outlines\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import cottage_analysis as cott\n",
    "from cottage_analysis.depth_analysis.filepath.generate_filepaths import *\n",
    "from cottage_analysis.imaging.common import find_frames\n",
    "from cottage_analysis.imaging.common import imaging_loggers_formatting as format_loggers\n",
    "from cottage_analysis.stimulus_structure import sphere_structure as vis_stim_structure\n",
    "from cottage_analysis.depth_analysis.plotting.plotting_utils import *\n",
    "from cottage_analysis.depth_analysis.plotting.basic_vis_plots import *\n",
    "from cottage_analysis.depth_analysis.depth_preprocess.process_params import *\n",
    "from cottage_analysis.depth_analysis.depth_preprocess.process_trace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace_arrs(roi, dffs, depth_list, stim_dict,\n",
    "                                            mode='sort_by_depth', protocol='fix_length',\n",
    "                                            blank_period=5, frame_rate=15):\n",
    "    # Trace array of dFF\n",
    "    trace_arr, _ = create_trace_arr_per_roi(roi, dffs, depth_list, stim_dict,\n",
    "                                            mode='sort_by_depth', protocol='fix_length',\n",
    "                                            blank_period=blank_period, frame_rate=frame_rate)\n",
    "    # trace_arr_mean = np.nanmean(trace_arr, axis=1)\n",
    "    trace_arr_noblank, _ = create_trace_arr_per_roi(roi, dffs, depth_list, stim_dict,\n",
    "                                                    mode='sort_by_depth', protocol='fix_length',\n",
    "                                                    blank_period=0, frame_rate=frame_rate)\n",
    "    # trace_arr_noblank_mean = np.nanmean(trace_arr_noblank, axis=1)\n",
    "    trace_arr_blank, _ = create_trace_arr_per_roi(roi, dffs, depth_list, stim_dict,\n",
    "                                                  mode='sort_by_depth', protocol='fix_length',\n",
    "                                                  isStim=False, blank_period=0,\n",
    "                                                  frame_rate=frame_rate)\n",
    "    return trace_arr_noblank, trace_arr_blank\n",
    "\n",
    "\n",
    "def train_test_split_trials(p_test, xarr1, xarr2, yarr):\n",
    "    test_trials_no = random.sample(range(xarr1.shape[1]),int(xarr1.shape[1]*p_test))\n",
    "    train_trials_no = list(set(range(xarr1.shape[1]))-set(test_trials_no))\n",
    "    \n",
    "    X_train1 = xarr1[:,train_trials_no,:]\n",
    "    X_test1 = xarr1[:,test_trials_no,:]\n",
    "    \n",
    "    # trace with all rois\n",
    "    X_train2 = xarr2[:,:,train_trials_no,:]\n",
    "    X_test2 = xarr2[:,:,test_trials_no,:]\n",
    "    \n",
    "    y_train = yarr[:,train_trials_no]\n",
    "    y_test = yarr[:,test_trials_no]\n",
    "\n",
    "    return X_train1, X_test1, X_train2, X_test2, y_train, y_test\n",
    "\n",
    "    \n",
    "# Seperate data based on running speed bins\n",
    "def separate_data_into_bins(arr, bin_arr, bins, shape):\n",
    "    if arr.ndim > 1:\n",
    "        # it's a trace array\n",
    "        _,_,bin_no = scipy.stats.binned_statistic(x=bin_arr, values=arr[0], statistic='mean', bins=bins, range=None)\n",
    "        arr_all_bins = []\n",
    "        for i in range(np.nanmin(bin_no),np.nanmax(bin_no)+1): \n",
    "            arr_all_bins.append((arr.reshape(shape))[:,np.where(bin_no==i)[0]])\n",
    "    else:\n",
    "        # it's a param array\n",
    "        _,_,bin_no = scipy.stats.binned_statistic(x=bin_arr, values=arr, statistic='mean', bins=bins, range=None)\n",
    "        arr_all_bins = []\n",
    "        for i in range(np.nanmin(bin_no),np.nanmax(bin_no)+1): \n",
    "            arr_all_bins.append((arr)[np.where(bin_no==i)[0]])\n",
    "    return arr_all_bins\n",
    "\n",
    "\n",
    "def train_classifier(X_train, X_test, X_val, y_train, y_test, y_val, param_grid, class_labels, kernel='linear'):\n",
    "    # Train the SVM classifier and tune hyperparameters\n",
    "    best_acc=0\n",
    "    best_params = {'C': param_grid['C'][0], 'gamma': param_grid['gamma'][0]}\n",
    "    for C in param_grid['C']:\n",
    "        for gamma in param_grid['gamma']:\n",
    "            clf = SVC(C=C, gamma=gamma, kernel=kernel)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_val)\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            \n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_params = {'C': C, 'gamma': gamma}\n",
    "            print(C,gamma, flush=True)\n",
    "\n",
    "    # Train the final classifier on the best hyperparameters\n",
    "    clf = SVC(C=best_params['C'], gamma=best_params['gamma'], kernel=kernel)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Evaluate the accuracy of the classifier\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    conmat = confusion_matrix(y_test, y_pred, labels=class_labels)\n",
    "\n",
    "    # print(\"Accuracy: {:.2f}%\".format(acc * 100))\n",
    "    return acc, conmat, clf\n",
    "\n",
    "\n",
    "def downsample_2darray(arr, window, mode='average'):\n",
    "    mode = 'average'\n",
    "    end =  window * int(arr.shape[1]/window)\n",
    "    if mode=='average':\n",
    "        arr_crop = arr[:,:end].reshape(arr.shape[0], -1, window)\n",
    "        arr_mean = np.mean(arr_crop,axis=2).reshape(arr.shape[0],-1)\n",
    "    return arr_mean\n",
    "    \n",
    "    \n",
    "def rolling_average_2darray(arr, axis, window):\n",
    "     result = pd.DataFrame(arr).rolling(window,axis=axis, min_periods=1).mean(skipna=True).values\n",
    "     return result\n",
    " \n",
    " \n",
    "def plot_confusion_matrix(conmat, fontsize_dict):\n",
    "    plt.imshow(conmat, interpolation='nearest', cmap='Blues')\n",
    "    fmt = 'd'\n",
    "    thresh = conmat.max() / 2.\n",
    "    for i, j in itertools.product(range(conmat.shape[0]), range(conmat.shape[1])):\n",
    "        plt.text(j, i, str(int(conmat[i, j])),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if conmat[i, j] > thresh else \"black\", fontsize=fontsize_dict['text'])\n",
    "    plt.xlabel('Predicted depth class',fontsize=fontsize_dict['xlabel'])\n",
    "    plt.ylabel('True depth class',fontsize=fontsize_dict['ylabel'])\n",
    "    \n",
    "    \n",
    "MIN_SIGMA=0.5\n",
    "\n",
    "def gaussian_func(x, a, x0, log_sigma,b):\n",
    "    a = a\n",
    "    sigma = np.exp(log_sigma)+MIN_SIGMA\n",
    "    return (a * np.exp(-(x - x0) ** 2) / (2 * sigma ** 2))+b\n",
    "\n",
    "def plot_depth_tuning_curve(trace_arr_noblank,\n",
    "                            depth_list,\n",
    "                            plot_rows, plot_cols, fontsize_dict,\n",
    "                            grid=True,\n",
    "                            this_depth=None,\n",
    "                           gaussian_fit=False, popt = [],\n",
    "                           title='Depth tuning', ylabel='dF/F', xlabel='Depth (cm)', linewidth=1):\n",
    "    # --- Plot 4 (0,3): Depth tuning curve ---\n",
    "    trace_arr_noblank_cp = trace_arr_noblank.copy()\n",
    "#     trace_arr_noblank_cp[speed_arr_noblank < speed_thr_cal] = np.nan\n",
    "    trace_arr_mean_eachtrial = np.nanmean(trace_arr_noblank_cp, axis=2)\n",
    "    CI_lows = np.zeros(len(depth_list))\n",
    "    CI_highs = np.zeros(len(depth_list))\n",
    "    for idepth in range(len(depth_list)):\n",
    "        CI_lows[idepth], CI_highs[idepth] = get_confidence_interval(\n",
    "            trace_arr_mean_eachtrial[idepth, :],\n",
    "            mean_arr=np.nanmean(trace_arr_mean_eachtrial, axis=1)[idepth].reshape(-1, 1))\n",
    "        \n",
    "    if gaussian_fit:\n",
    "        if (this_depth == None) or (this_depth!=len(depth_list)): # we can't plot the tuning curve for non-depth-selective neurons\n",
    "            plot_line_with_error(arr=np.nanmean(trace_arr_mean_eachtrial, axis=1), CI_low=CI_lows,\n",
    "                                 CI_high=CI_highs, linecolor='b', fontsize_dict=fontsize_dict, linewidth=linewidth)\n",
    "\n",
    "            trace_arr_mean_eachtrial = np.nanmean(trace_arr_noblank, axis=2)\n",
    "            x = np.log(np.repeat(np.array(depth_list), trace_arr_mean_eachtrial.shape[1]))\n",
    "            roi_number = np.where(depth_neurons == roi)[0][0]\n",
    "            \n",
    "            plt.plot(np.linspace(0, len(depth_list) - 1, 100),\n",
    "                     gaussian_func(np.linspace(np.log(depth_list[0]*100), np.log(depth_list[-1]*100), 100), *popt), 'gray', linewidth=3)\n",
    "            plt.xticks(np.arange(len(depth_list)), (np.array(depth_list) * 100).astype('int'),\n",
    "                       fontsize=fontsize_dict['xticks'])\n",
    "            plt.yticks(fontsize=fontsize_dict['yticks'])\n",
    "            plt.ylabel(ylabel, fontsize=fontsize_dict['title'])\n",
    "            plt.xlabel(xlabel, fontsize=fontsize_dict['xlabel'])\n",
    "            plt.title(title, fontsize=fontsize_dict['ylabel'])\n",
    "            plot_frame_off()\n",
    "    else:\n",
    "        plot_line_with_error(arr=np.nanmean(trace_arr_mean_eachtrial, axis=1), CI_low=CI_lows,\n",
    "                             CI_high=CI_highs, linecolor='royalblue', fontsize_dict=fontsize_dict, linewidth=linewidth)\n",
    "        plt.xticks(np.arange(len(depth_list)), (np.array(depth_list) * 100).astype('int'),\n",
    "                   fontsize=fontsize_dict['xticks'])\n",
    "        plt.ylabel(ylabel, fontsize=fontsize_dict['ylabel'])\n",
    "        plt.xlabel(xlabel, fontsize=fontsize_dict['xlabel'])\n",
    "        plt.title(title, fontsize=fontsize_dict['title'])\n",
    "        plot_frame_off()\n",
    "        \n",
    "        \n",
    "def plot_line_with_error(arr, CI_low, CI_high, linecolor, fontsize_dict, \n",
    "                         label=None, marker='-', markersize=None, xarr=[], xlabel=None, ylabel=None, \n",
    "                         title_on=False, title=None, suffix=None, linewidth=0.5, rasterized=False):\n",
    "    if len(xarr) == 0:\n",
    "        plt.plot(arr, marker, c = linecolor, linewidth=linewidth, label=label, alpha = 1, markersize=markersize, rasterized=rasterized)\n",
    "        plt.fill_between(np.arange(len(arr)), CI_low, CI_high, color=linecolor, alpha=0.3, edgecolor=None, rasterized=rasterized)\n",
    "    else:\n",
    "        plt.plot(xarr, arr, marker, c = linecolor, linewidth=linewidth, label=label, alpha = 1, markersize=markersize, rasterized=rasterized)\n",
    "        plt.fill_between(xarr, CI_low, CI_high, color=linecolor, alpha=0.3, edgecolor=None, rasterized=rasterized)\n",
    "    plt.xlabel(xlabel, fontsize=fontsize_dict['xlabel'])\n",
    "    plt.ylabel(ylabel, fontsize=fontsize_dict['ylabel'])\n",
    "    if title_on:\n",
    "        plt.title(title+' '+suffix, fontsize=fontsize_dict['title'])\n",
    "    else:\n",
    "        plt.title(suffix, fontsize=fontsize_dict['title'])\n",
    "        \n",
    "        \n",
    "# get rid of nan values from trace arr and the corresponding rs/depth\n",
    "def remove_nan(xarr, arr1, arr2, shape):\n",
    "    nanidx = np.isnan(xarr[0])\n",
    "    arr1 = arr1[~nanidx]\n",
    "    arr2 = arr2[~nanidx]\n",
    "    xarr = xarr[~np.isnan(xarr)].reshape(shape)\n",
    "\n",
    "    return xarr, arr1, arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load openloop data (speed_arr, of_arr)\n",
    "# ----- SETUPS -----\n",
    "project = 'hey2_3d-vision_foodres_20220101'\n",
    "mouse = 'PZAH6.4b'\n",
    "session = 'S20220524'\n",
    "protocol = 'SpheresPermTubeRewardPlayback'\n",
    "\n",
    "rawdata_root = '/camp/lab/znamenskiyp/data/instruments/raw_data/projects/'\n",
    "root = '/camp/lab/znamenskiyp/home/shared/projects/'\n",
    "depth_list = [0.06, 0.19, 0.6, 1.9, 6]\n",
    "choose_trials = 50\n",
    "frame_rate = 15\n",
    "# speed_thr_cal = 0.2  # m/s, threshold for running speed when calculating depth neurons\n",
    "# speed_thr = 0.01  #m/s\n",
    "\n",
    "cmap = cm.cool.reversed()\n",
    "line_colors = []\n",
    "norm = matplotlib.colors.Normalize(vmin=np.log(min(depth_list)), vmax=np.log(max(depth_list)))\n",
    "for depth in depth_list:\n",
    "    rgba_color = cmap(norm(np.log(depth)),bytes=True)\n",
    "    rgba_color = tuple(it/255 for it in rgba_color)\n",
    "    line_colors.append(rgba_color)\n",
    "    \n",
    "# ----- STEP1: Generate file path -----\n",
    "print('---START STEP 1---', '\\n', 'Getting data filepaths...', flush=True)\n",
    "rawdata_folder, protocol_folder, analysis_folder, suite2p_folder, trace_folder = generate_file_folders(\n",
    "    project=project,\n",
    "    mouse=mouse,\n",
    "    session=session,\n",
    "    protocol=protocol,\n",
    "    rawdata_root=rawdata_root,\n",
    "    root=root)\n",
    "print(rawdata_folder, protocol_folder, analysis_folder, suite2p_folder, trace_folder, flush=True)\n",
    "\n",
    "if 'Playback' in protocol:\n",
    "    rawdata_folder_closeloop, protocol_folder_closeloop, analysis_folder_closeloop, suite2p_folder_closeloop, trace_folder_closeloop = generate_file_folders(project=project,\n",
    "                                                                                                    mouse=mouse,\n",
    "                                                                                                    session=session,\n",
    "                                                                                                    protocol=protocol.replace('Playback',''),\n",
    "                                                                                                    rawdata_root=rawdata_root,\n",
    "                                                                                                    root=root)\n",
    "    print(rawdata_folder_closeloop, protocol_folder_closeloop, analysis_folder_closeloop, suite2p_folder_closeloop, trace_folder_closeloop, flush=True)\n",
    "    assert(os.path.exists(analysis_folder_closeloop/'plane0/'))\n",
    "\n",
    "print('---STEP 1 FINISHED.---', '\\n', flush=True)\n",
    "\n",
    "\n",
    "\n",
    "# ----- STEP2: Load files -----\n",
    "print('---START STEP 2---', '\\n', 'Load files...', flush=True)\n",
    "# Load suite2p files\n",
    "ops = np.load(suite2p_folder/'ops.npy', allow_pickle=True)\n",
    "ops = ops.item()\n",
    "iscell = np.load(suite2p_folder/'iscell.npy', allow_pickle=True)[:, 0]\n",
    "F = np.load(trace_folder/'F.npy', allow_pickle=True)\n",
    "Fast = np.load(trace_folder/'Fast.npy', allow_pickle=True)\n",
    "# Fneu = np.load(trace_folder + 'Fneu.npy', allow_pickle=True)\n",
    "# spks = np.load(trace_folder + 'spks.npy', allow_pickle=True)\n",
    "\n",
    "# All_rois\n",
    "which_rois = (np.arange(F.shape[0]))[iscell.astype('bool')]\n",
    "\n",
    "# dffs_ast\n",
    "dffs = np.load(trace_folder/'dffs_ast.npy')\n",
    "\n",
    "\n",
    "# Load stim structure file\n",
    "with open(protocol_folder/'img_VS.pickle', 'rb') as handle:\n",
    "    img_VS = pickle.load(handle)\n",
    "if os.path.exists(protocol_folder/'stim_dict.pickle'):\n",
    "    with open(protocol_folder/'stim_dict.pickle', 'rb') as handle:\n",
    "        stim_dict = pickle.load(handle)\n",
    "else:\n",
    "    stim_dict = vis_stim_structure.create_stim_dict(depth_list=depth_list, img_VS=img_VS, choose_trials=choose_trials)\n",
    "\n",
    "\n",
    "# Load depth neurons\n",
    "if 'Playback' in protocol:\n",
    "    depth_neurons = np.load(analysis_folder_closeloop/'plane0/depth_neurons.npy')\n",
    "    max_depths = np.load(analysis_folder_closeloop/'plane0/max_depths_index.npy')\n",
    "else:\n",
    "    depth_neurons = np.load(analysis_folder/'plane0/depth_neurons.npy')\n",
    "    max_depths = np.load(analysis_folder/'plane0/max_depths_index.npy')\n",
    "\n",
    "print('---STEP 2 FINISHED.---', '\\n', flush=True)\n",
    "\n",
    "# ----- STEP3: Plotting -----\n",
    "print('---START STEP 3---', '\\n', 'Plotting...', flush=True)\n",
    "# SETUP\n",
    "dff_plot_max = 3\n",
    "blank_period = 5\n",
    "fontsize = 20\n",
    "speed_bins = 15\n",
    "frame_rate = 15\n",
    "speed_thr = 0.01\n",
    "\n",
    "# PARAMS PROCESSING\n",
    "# Speed array\n",
    "# Running speed is thresholded with a small threshold to get rid of non-zero values (default threshold 0.01)\n",
    "speeds = img_VS.MouseZ.diff() / img_VS.HarpTime.diff()  # with no playback. EyeZ and MouseZ should be the same.\n",
    "speeds[0] = 0\n",
    "speeds = thr(speeds, speed_thr)\n",
    "# speed_arr, _ = create_speed_arr(speeds, depth_list, stim_dict, mode='sort_by_depth', protocol='fix_length',\n",
    "#                                 blank_period=0, frame_rate=frame_rate)\n",
    "# speed_arr_mean = np.nanmean(speed_arr,axis=1)\n",
    "speed_arr_noblank,_ = create_speed_arr(speeds, depth_list, stim_dict, mode='sort_by_depth', protocol='fix_length', blank_period=0, frame_rate=frame_rate)\n",
    "speed_arr =  speed_arr_noblank\n",
    "\n",
    "# speed_arr_noblank_mean = np.nanmean(speed_arr_noblank,axis=1)\n",
    "# speed_arr_blank,_ = create_speed_arr(speeds, depth_list, stim_dict, mode='sort_by_depth', protocol='fix_length', isStim=False, blank_period=0, frame_rate=frame_rate)\n",
    "# frame_num_pertrial_max = speed_arr_noblank.shape[2]\n",
    "# total_trials = speed_arr_noblank.shape[1]\n",
    "\n",
    "# OF (Unit: rad/s)\n",
    "if 'Playback' in protocol:\n",
    "    speeds_eye = img_VS.EyeZ.diff() / img_VS.HarpTime.diff()  # EyeZ is how the perspective of animal moves\n",
    "    speeds_eye[0] = 0\n",
    "    speeds_eye = thr(speeds_eye, speed_thr)\n",
    "    speed_eye_arr_noblank,_ = create_speed_arr(speeds_eye, depth_list, stim_dict, mode='sort_by_depth', protocol='fix_length', blank_period=0, frame_rate=frame_rate)\n",
    "\n",
    "    optics = calculate_OF(rs=speeds_eye, img_VS=img_VS, mode='no_RF')\n",
    "\n",
    "else:\n",
    "    optics = calculate_OF(rs=speeds, img_VS=img_VS, mode='no_RF')\n",
    "\n",
    "of_arr, _ = create_speed_arr(optics, depth_list, stim_dict, mode='sort_by_depth',\n",
    "                    protocol='fix_length', blank_period=blank_period)\n",
    "of_arr_mean = np.nanmean(of_arr, axis=1)\n",
    "of_arr_noblank, _ = create_speed_arr(optics, depth_list, stim_dict, mode='sort_by_depth',\n",
    "                                    protocol='fix_length', blank_period=0)\n",
    "of_arr_noblank_mean = np.nanmean(of_arr_noblank, axis=1)\n",
    "of_arr_blank, _ = create_speed_arr(optics, depth_list, stim_dict, mode='sort_by_depth',\n",
    "                                protocol='fix_length', isStim=False, blank_period=0)\n",
    "\n",
    "\n",
    "# Transform speed into cm/s, optic flow into degrees/s\n",
    "speed_arr_noblank = speed_arr_noblank*100\n",
    "\n",
    "of_arr_noblank = np.degrees(of_arr_noblank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling average and downsample of running speeds\n",
    "window_period = 0.5\n",
    "frame_rate = 15\n",
    "window = int(window_period*frame_rate)\n",
    "speed_arr_smoothed = rolling_average_2darray(arr=speed_arr_noblank.reshape(-1,speed_arr_noblank.shape[2]), axis=1, window=window).reshape(speed_arr_noblank.shape)\n",
    "speed_arr_downsam = downsample_2darray(arr=speed_arr_smoothed.reshape(-1,speed_arr_smoothed.shape[2]), window=window, mode='average').reshape(speed_arr_smoothed.shape[0],speed_arr_smoothed.shape[1],-1)\n",
    "\n",
    "# concat all cells dffs\n",
    "all_trace_downsam = np.zeros((len(which_rois),speed_arr_downsam.shape[0],speed_arr_downsam.shape[1],speed_arr_downsam.shape[2]))\n",
    "\n",
    "for iroi, roi in enumerate(which_rois):\n",
    "    \n",
    "    # Load trace_arr\n",
    "    trace_arr_noblank, _ = get_trace_arrs(roi=roi, dffs=dffs, \n",
    "                                                    depth_list=depth_list, stim_dict=stim_dict,\n",
    "                                mode='sort_by_depth', protocol='fix_length',\n",
    "                                blank_period=0, frame_rate=frame_rate)\n",
    "    # Downsample trace\n",
    "    trace_arr_downsam = downsample_2darray(arr=trace_arr_noblank.reshape(-1,trace_arr_noblank.shape[2]), window=window, mode='average').reshape(trace_arr_noblank.shape[0],trace_arr_noblank.shape[1],-1)\n",
    "    all_trace_downsam[iroi, :] = trace_arr_downsam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test on all data\n",
    "# train test split\n",
    "depth_labels = np.repeat(np.repeat(np.arange(len(depth_list)),speed_arr_downsam.shape[1]),speed_arr_downsam.shape[2]).reshape(len(depth_list),speed_arr_downsam.shape[1],speed_arr_downsam.shape[2])\n",
    "bin_number=10\n",
    "speed_min = 1\n",
    "speed_max= 150\n",
    "\n",
    "rs_train, rs_test, trace_train, trace_test, depth_train, depth_test = train_test_split_trials(p_test=0.4, \n",
    "                                                                                            xarr1=speed_arr_downsam, \n",
    "                                                                                            xarr2=all_trace_downsam,\n",
    "                                                                                            yarr=depth_labels)\n",
    "rs_test, rs_val, trace_test, trace_val, depth_test, depth_val = train_test_split_trials(p_test=0.5, \n",
    "                                                                                        xarr1=rs_test, \n",
    "                                                                                        xarr2=trace_test,\n",
    "                                                                                        yarr=depth_test)\n",
    "\n",
    "# Log all data and flatten array\n",
    "[rs_train, \n",
    "    rs_test, \n",
    "    rs_val,\n",
    "    trace_train, \n",
    "    trace_test, \n",
    "    trace_val,\n",
    "    depth_train, \n",
    "    depth_test, \n",
    "    depth_val,\n",
    "    ] = [np.log10(rs_train).flatten(), \n",
    "        np.log10(rs_test).flatten(), \n",
    "        np.log10(rs_val).flatten(),\n",
    "        (trace_train).reshape(len(which_rois),-1), \n",
    "        (trace_test).reshape(len(which_rois),-1), \n",
    "        (trace_val).reshape(len(which_rois),-1),\n",
    "        (depth_train).flatten(), \n",
    "        (depth_test).flatten(), \n",
    "        (depth_val).flatten()]\n",
    "    \n",
    "\n",
    "nanidx = np.isnan(trace_train[0])\n",
    "depth_train = depth_train[~nanidx]\n",
    "trace_train = trace_train[~np.isnan(trace_train)].reshape(len(which_rois),-1)\n",
    "nanidx = np.isnan(trace_test[0])\n",
    "depth_test = depth_test[~nanidx]\n",
    "trace_test= trace_test[~np.isnan(trace_test)].reshape(len(which_rois),-1)\n",
    "nanidx = np.isnan(trace_val[0])\n",
    "depth_val = depth_val[~nanidx]\n",
    "trace_val= trace_val[~np.isnan(trace_val)].reshape(len(which_rois),-1)\n",
    "\n",
    "    \n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01]} \n",
    "acc_same, conmat_same, clf = train_classifier(X_train=trace_train.T, \n",
    "                X_test=trace_test.T, \n",
    "                X_val = trace_val.T,\n",
    "                y_train=depth_train, \n",
    "                y_test=depth_test, \n",
    "                y_val=depth_val, \n",
    "                param_grid=param_grid,\n",
    "                class_labels=np.arange(len(depth_list)))\n",
    "\n",
    "fontsize_dict = {\n",
    "    'xlabel':20,\n",
    "    'ylabel':20,\n",
    "    'xticks':20,\n",
    "    'yticks':20,\n",
    "    'title':20,\n",
    "    'legend':15,\n",
    "    'text':15\n",
    "    }\n",
    "plot_confusion_matrix(conmat=conmat_same, fontsize_dict=fontsize_dict)\n",
    "acc_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test on 50% of data, validate on 10% of data, test on 40% of data with splitting into different running bins\n",
    "depth_labels = np.repeat(np.repeat(np.arange(len(depth_list)),speed_arr_downsam.shape[1]),speed_arr_downsam.shape[2]).reshape(len(depth_list),speed_arr_downsam.shape[1],speed_arr_downsam.shape[2])\n",
    "bin_number=10\n",
    "speed_min = 1\n",
    "speed_max= 150\n",
    "speed_arr_downsam_log = np.log10(speed_arr_downsam)\n",
    "rs_classifier, rs_rest, trace_classifier, trace_rest, depth_classifier, depth_rest = train_test_split_trials(p_test=0.4, \n",
    "                                                                                            xarr1=speed_arr_downsam_log, \n",
    "                                                                                            xarr2=all_trace_downsam,\n",
    "                                                                                            yarr=depth_labels)\n",
    "rs_classifier_train_test, rs_val, trace_classifier_train_test, trace_val, depth_classifier_train_test, depth_val = train_test_split_trials(p_test=0.1, \n",
    "                                                                                        xarr1=rs_classifier, \n",
    "                                                                                        xarr2=trace_classifier,\n",
    "                                                                                        yarr=depth_classifier)\n",
    "rs_train, rs_classifier_test, trace_train, trace_classifier_test, depth_train, depth_classifier_test = train_test_split_trials(p_test=0.1, \n",
    "                                                                                        xarr1=rs_classifier_train_test, \n",
    "                                                                                        xarr2=trace_classifier_train_test,\n",
    "                                                                                        yarr=depth_classifier_train_test)\n",
    "\n",
    "\n",
    "bin_number=10\n",
    "trace_rest, depth_rest, rs_rest = remove_nan(xarr=trace_rest, arr1=depth_rest, arr2=rs_rest, shape=(len(which_rois),-1))\n",
    "trace_train, depth_train, rs_train = remove_nan(xarr=trace_train, arr1=depth_train, arr2=rs_train, shape=(len(which_rois),-1))\n",
    "trace_val, depth_val, rs_val = remove_nan(xarr=trace_val, arr1=depth_val, arr2=rs_val, shape=(len(which_rois),-1))\n",
    "trace_classifier_test, depth_classifier_test, rs_classifier_test = remove_nan(xarr=trace_classifier_test, arr1=depth_classifier_test, arr2=rs_classifier_test, shape=(len(which_rois),-1))\n",
    "\n",
    "rs_bins = np.linspace(np.nanmin(speed_arr_downsam_log),np.nanmax(speed_arr_downsam_log),bin_number+1)\n",
    "trace_all_bins = separate_data_into_bins(arr=trace_rest, bin_arr=rs_rest, bins=rs_bins, shape=(len(which_rois),-1))\n",
    "rs_all_bins = separate_data_into_bins(arr=rs_rest, bin_arr=rs_rest, bins=rs_bins, shape=(len(which_rois),-1))\n",
    "depth_all_bins = separate_data_into_bins(arr=depth_rest, bin_arr=rs_rest, bins=rs_bins, shape=(len(which_rois),-1))\n",
    "# Train classifier with all data\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01]} \n",
    "acc_same, conmat_same, clf = train_classifier(X_train=trace_train.T, \n",
    "                X_test=trace_classifier_test.T, \n",
    "                X_val = trace_val.T,\n",
    "                y_train=depth_train, \n",
    "                y_test=depth_classifier_test, \n",
    "                y_val=depth_val, \n",
    "                param_grid=param_grid,\n",
    "                class_labels=np.arange(len(depth_list)))\n",
    "\n",
    "fontsize_dict = {\n",
    "    'xlabel':20,\n",
    "    'ylabel':20,\n",
    "    'xticks':20,\n",
    "    'yticks':20,\n",
    "    'title':20,\n",
    "    'legend':15,\n",
    "    'text':15\n",
    "    }\n",
    "plot_confusion_matrix(conmat=conmat_same, fontsize_dict=fontsize_dict)\n",
    "\n",
    "all_accs = []\n",
    "all_con_mats = []\n",
    "# Evaluate the accuracy of the classifier\n",
    "for i in range(bin_number):\n",
    "    y_pred = clf.predict(trace_all_bins[i].T)\n",
    "    acc = accuracy_score(depth_all_bins[i], y_pred)\n",
    "    conmat = confusion_matrix(depth_all_bins[i], y_pred, labels=np.arange(len(depth_list)))\n",
    "    all_accs.append(acc)\n",
    "    all_con_mats.append(conmat)\n",
    "    print(i, flush=True)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(conmat=conmat, fontsize_dict=fontsize_dict)\n",
    "    plt.title(f'sample size{len(y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2p_analysis_cottage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "939011bda4ac240316e77cd9a4c8c2e7aac31e833bb6d639fcef3c3405a9852b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
