{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example step by step DLC\n",
    "\n",
    "First create the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No modules loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"module load cuDNN/8.1.1.33-CUDA-11.2.1\")\n",
    "os.system(\"module list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/camp/home/blota/.conda/envs/dlc_nogui/lib/\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/camp/home/blota/.conda/envs/dlc_nogui/lib/\"\n",
    "print(os.environ[\"LD_LIBRARY_PATH\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 23:12:47.744197: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-24 23:13:01.157307: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /camp/home/blota/.conda/envs/dlc_nogui/lib/\n",
      "2023-01-24 23:13:01.157468: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /camp/home/blota/.conda/envs/dlc_nogui/lib/\n",
      "2023-01-24 23:13:01.157478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 23:13:16.143232: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-24 23:13:16.705573: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /camp/home/blota/.conda/envs/dlc_nogui/lib/\n",
      "2023-01-24 23:13:16.705642: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "\n",
    "print(get_available_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 2.2.3...\n",
      "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n",
      "/camp/lab/znamenskiyp/home/shared/projects/DLC_models/all_eyes_2023/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/camp/home/blota/.conda/envs/dlc_nogui/lib/python3.8/site-packages/deeplabcut/__init__.py:81: UserWarning: \n",
      "        As PyTorch is not installed, unsupervised identity learning will not be available.\n",
      "        Please run `pip install torch`, or ignore this warning.\n",
      "        \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "from pathlib import Path\n",
    "\n",
    "config = Path(\"/camp/lab/znamenskiyp/home/shared/projects/DLC_models/\")\n",
    "config /= \"all_eyes_2023/config.yaml\"\n",
    "\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flexiznam as flm\n",
    "\n",
    "project = \"hey2_3d-vision_foodres_20220101\"\n",
    "flm_sess = flm.get_flexilims_session(project_id=project)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of videos to add\n",
    "session_dict = {\n",
    "    \"PZAH6.4b\": [\"S20220506\", \"S20220526\"],\n",
    "    \"PZAG3.4f\": [\"S20220421\", \"S20220517\"],\n",
    "}\n",
    "from flexiznam.schema import Dataset\n",
    "import shutil\n",
    "tmp_folder = Path('/nemo/lab/znamenskiyp/home/shared/projects/DLC_models/tmp')\n",
    "tmp_folder.mkdir(exist_ok=True)\n",
    "vids = []\n",
    "\n",
    "for mouse, sessions in session_dict.items():\n",
    "    for sess in sessions:\n",
    "        fl_sess = flm.get_entity(name=f\"{mouse}_{sess}\", flexilims_session=flm_sess)\n",
    "        recs = flm.get_children(\n",
    "            parent_id=fl_sess[\"id\"],\n",
    "            children_datatype=\"recording\",\n",
    "            flexilims_session=flm_sess,\n",
    "        )\n",
    "        recs = [r for n, r in recs.iterrows() if \"Spheres\" in n]\n",
    "        rec = recs[0]\n",
    "        ds = flm.get_children(\n",
    "            parent_id=rec[\"id\"], flexilims_session=flm_sess, children_datatype=\"dataset\"\n",
    "        )\n",
    "        eye_ds = [r for n, r in ds.iterrows() if \"eye_camera\" in n]\n",
    "        for ds in eye_ds:\n",
    "            ds = Dataset.from_flexilims(data_series=ds)\n",
    "            vid = ds.path_full / ds.extra_attributes['video_file']\n",
    "            target = tmp_folder / f\"{mouse}_{sess}_{rec.name}_{vid.name}\"\n",
    "            if not target.exists():\n",
    "                shutil.copy(vid, target)\n",
    "            vids.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/nemo/lab/znamenskiyp/home/shared/projects/DLC_models/tmp/PZAH6.4b_S20220506_PZAH6.4b_S20220506_R120732_SpheresPermTubeReward_left_eye_camera_data.mp4'),\n",
       " PosixPath('/nemo/lab/znamenskiyp/home/shared/projects/DLC_models/tmp/PZAH6.4b_S20220506_PZAH6.4b_S20220506_R120732_SpheresPermTubeReward_right_eye_camera_data.mp4'),\n",
       " PosixPath('/nemo/lab/znamenskiyp/home/shared/projects/DLC_models/tmp/PZAH6.4b_S20220526_PZAH6.4b_S20220526_R184428_SpheresPermTubeReward_right_eye_camera_data.mp4'),\n",
       " PosixPath('/nemo/lab/znamenskiyp/home/shared/projects/DLC_models/tmp/PZAH6.4b_S20220526_PZAH6.4b_S20220526_R184428_SpheresPermTubeReward_left_eye_camera_data.mp4'),\n",
       " PosixPath('/nemo/lab/znamenskiyp/home/shared/projects/DLC_models/tmp/PZAG3.4f_S20220421_PZAG3.4f_S20220421_R165922_SpheresPermTubeReward_right_eye_camera_data.avi'),\n",
       " PosixPath('/nemo/lab/znamenskiyp/home/shared/projects/DLC_models/tmp/PZAG3.4f_S20220421_PZAG3.4f_S20220421_R165922_SpheresPermTubeReward_left_eye_camera_data.mp4'),\n",
       " PosixPath('/nemo/lab/znamenskiyp/home/shared/projects/DLC_models/tmp/PZAG3.4f_S20220517_PZAG3.4f_S20220517_R140651_SpheresPermTubeReward_left_eye_camera_data.mp4'),\n",
       " PosixPath('/nemo/lab/znamenskiyp/home/shared/projects/DLC_models/tmp/PZAG3.4f_S20220517_PZAG3.4f_S20220517_R140651_SpheresPermTubeReward_right_eye_camera_data.mp4')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add these videos (do it only once)\n",
    "if False:\n",
    "    deeplabcut.add_new_videos(config, videos=vids, extract_frames=True, copy_videos=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 264.75  seconds.\n",
      "Extracting and downsampling... 15867  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15867it [00:06, 2557.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 222.74  seconds.\n",
      "Extracting and downsampling... 13349  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13349it [00:05, 2626.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 319.88  seconds.\n",
      "Extracting and downsampling... 19148  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19148it [00:09, 2072.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 281.8  seconds.\n",
      "Extracting and downsampling... 16861  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16861it [00:08, 2052.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 155.37  seconds.\n",
      "Extracting and downsampling... 9296  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9296it [00:04, 2175.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 5030.11  seconds.\n",
      "Extracting and downsampling... 150904  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150904it [27:19, 92.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 5030.11  seconds.\n",
      "Extracting and downsampling... 150904  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150904it [27:13, 92.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 2024.07  seconds.\n",
      "Extracting and downsampling... 60723  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60723it [10:56, 92.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 2024.07  seconds.\n",
      "Extracting and downsampling... 60723  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60723it [11:03, 91.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 2162.51  seconds.\n",
      "Extracting and downsampling... 64876  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64876it [11:45, 91.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 2162.51  seconds.\n",
      "Extracting and downsampling... 64876  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64876it [11:49, 91.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 2683.59  seconds.\n",
      "Extracting and downsampling... 80508  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80508it [14:37, 91.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 2683.59  seconds.\n",
      "Extracting and downsampling... 80508  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80508it [14:39, 91.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Frames were successfully extracted, for the videos listed in the config.yaml file.\n",
      "\n",
      "You can now label the frames using the function 'label_frames' (Note, you should label frames extracted from diverse videos (and many videos; we do not recommend training on single videos!)).\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.extract_frames(config=config, userfeedback=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit config file manually to add the skeleton and body parts you want. Then head to the\n",
    "VM to label manually. Copy the labelled data back to camp and edit config.yaml to change\n",
    "paths if needed.\n",
    "\n",
    "Then check labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by Antonin Blot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 32.33it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 43.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.check_labels(config, visualizeindividuals=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.95,\n",
       "  1,\n",
       "  (array([ 4, 28, 29, 33, 34, 25, 10, 22, 11, 27, 18, 15,  2, 38, 20, 36, 16,\n",
       "          35,  8, 13,  5, 17, 14, 32,  7, 31,  1, 26, 12, 30, 24,  6, 23, 21,\n",
       "          19,  9, 37]),\n",
       "   array([3, 0])))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(config, augmenter_type=\"imgaug\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the network (unsing the sbatch script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  DLC_resnet50_eye_trackingDec1shuffle1_1030000  with # of training iterations: 1030000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/camp/home/blota/.conda/envs/dlc_nogui/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "2022-12-06 13:14:54.497433: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [00:03, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-1030000\n",
      "Results for 1030000  training iterations: 95 1 train error: 0.8 pixels. Test error: 2.55  pixels.\n",
      "With pcutoff of 0.6  train error: 0.8 pixels. Test error: 2.55 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Plotting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:04<00:00,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANwAAADcCAYAAAAbWs+BAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAACeklEQVR4nO3TMQEAIAzAMMC/5+GiHCQK+nTPzAIa53UA/MRwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchAwHIcNByHAQMhyEDAchw0HIcBAyHIQMByHDQchwEDIchC6yugS1re5aPgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o = deeplabcut.evaluate_network(config, Shuffles=[1], plotting=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/camp/home/blota/.conda/envs/dlc_nogui/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving plots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "scm = deeplabcut.extract_save_all_maps(config, shuffle=1, Indices=[0, 5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('dlc_nogui')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51121e9e3696cd87f3edc342e0110e901d85e752c5f0b51abca873b31e6cd586"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
