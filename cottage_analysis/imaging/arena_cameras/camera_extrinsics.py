"""
This is code to calibrate the arena cameras in the freely-moving setup. Today, it assumes that the arena was calibrated with
El Calibrador. It also contains useful code to make movies combining Lighthouse/BNO and camera iput. 

Author: Antonio ColÃ¡s Nieto
Creation: Aug 2023

You can find an example implementation of this code in scratch.py, or in plot.position.ipynb

ISSUES:

- The camera metadata conversion to ONIX time still relies on the old method, and not yet the MasterFile.bonsai workflow
- There is no programatic way to estimate the offset in orientation. Defaults to the offset in COWBOY

"""

# Standard Library Imports
from pathlib import Path
import math

# Third-Party Imports
import cv2
import numpy as np
from matplotlib import pyplot as plt

# Local/Application Specific Imports
import cottage_analysis.io_module.onix as onix
import cottage_analysis.ephys.preprocessing as prp
import cottage_analysis.utilities.plot_utils as plut

"""
LOADING FUNCTIONS



"""

def load_extrinsics(extrinsics_path):
    """
    Useful to load the extrinsic calibration file from a camera. The camera should already be calibrated with the code 
    in Calibrations, and the results of the calibration should be in NEMO. This function is only useful to programatically 
    read the output generated there

    Args: 
        extrinsics_path (Path or str): The complete path to the folder in which extrinsic calibration of a camera is stored. 

    Returns: 
        extrinsics (dict): A dictionary with a key for each found marker (e.g, 'marker1', 'marker2'). Inside, a dictionary with
        ['rvec', 'tvec', 'corners', 'r2m_rvec', 'r2m_tvec', 'm2r_rvec', 'm2r_tvec']. To find out the exact meaning of these objects, 
        please read Calibrations or the opencv documentation.    
     """
    extrinsics_path = Path(extrinsics_path)
    marker_files = list(extrinsics_path.glob('*.npz'))
    
    # Error handling for no files 
    if len(marker_files) == 0:
        raise FileNotFoundError(f"No .npz files found in the directory: {extrinsics_path}")

    extrinsics = dict()
    for i, marker in enumerate(marker_files): 
        marker_path = extrinsics_path / marker
        extrinsics[f'{str(marker.name)[0:7]}'] = np.load(marker_path)
    return extrinsics

def load_intrinsics(intrinsics_path):
    """
    Useful to load the intrinsic calibration file from a camera. The camera should already be calibrated with the code 
    in Calibrations, and the results of the calibration should be NEMO. This function is only useful to programatically 
    read the output generated there

    Args: 
        intrinsics_path (Path or str): The complete path to the folder in which intrinsic calibration of a camera is stored. 

    Returns: 
        intrinsics (dict): A dictionary with ['ret', 'mtx', 'dist', 'rvecs', 'tvecs', 'mean_error']. Since they're commonly used, 
        it may be useful to point out that cameraMatrix is 'mtx' and the distCoeffs is 'dist'. To find out the exact meaning of these 
        objects, please read Calibrations or the opencv documentation.    
     """
    intrinsics_path = Path(intrinsics_path)
    files = list(intrinsics_path.glob('*.npz'))
    
    # Error handling for no files or multiple files
    if len(files) == 0:
        raise FileNotFoundError(f"No .npz files found in the directory: {intrinsics_path}")
    elif len(files) > 1:
        raise ValueError(f"Multiple .npz files found in the directory: {intrinsics_path}. Ensure there's only one.")
    
    intrinsics = np.load(files[0])
    return intrinsics


"""
VIDEO AND PICTURE ENTRY POINTS


"""

def plot_onix_time(onix_time, video_path, trans_data, cam_metadata, rvec, tvec, cameraMatrix, distCoeffs):
    """
    Plots the mouse position overlaid on the camera frame at a specific onix time. 

    Args:
        onix_time(float): the onix time at which you want to plot a frame
        video_path(string): a complete path to the video file
        trans_data(pd): the Lighthouse transformed positions as generated by transform_data() in Calibrations
        cam_metadata(dict): a cam_metadata dict as generated from dio_conversion() or equivalent way of handling metadata
        rvec, tvec, cameraMatrix, distCoeffs: OpenCV objects generated in the extrinsic calibration of the camera and generated
        in load_extrinsics and load_intrinsics. 

    Out: 
        fig(fig): a matplotlib figure object
    """
    #obtaining the closest frame
    frame_start = get_onix_frame(video_path, onix_time, cam_metadata)
    #transforming coordinates
    proj_data = lighthouse2aruco(trans_data)
    #selecting index
    lighthouse_index = _find_nearest(proj_data['clock'], onix_time)
    index = proj_data[proj_data['clock']==lighthouse_index].index[0]
    objectPoints = np.array([proj_data['x'][index], proj_data['y'][index], proj_data['z'][index]])
    objectPoints = np.transpose(objectPoints)
    #transforming and plotting
    imagePoints, jacobian = cv2.projectPoints(objectPoints, rvec, tvec, cameraMatrix, distCoeffs)
    coordinates = imagePoints[0][0]
    fig, axs = plt.subplots(1, 1, figsize=(50, 50))
    axs.imshow(frame_start)
    axs.scatter(coordinates[0], coordinates[1], marker='+', s=15e3)
    return fig

def plot_interval_s(start, end, video_path, trans_data, cam_metadata, rvec, tvec, cameraMatrix, distCoeffs):
    """
    Makes a video of the mouse position overlaid on camera data for a certain time interval

    Args:
        start(float): start time in s
        end(float): end time in s
        video_path(string): a complete path to the video file
        trans_data(pd): the Lighthouse transformed positions as generated by transform_data() in Calibrations
        cam_metadata(dict): a cam_metadata dict as generated from dio_conversion() or equivalent way of handling metadata
        rvec, tvec, cameraMatrix, distCoeffs: OpenCV objects generated in the extrinsic calibration of the camera and generated
        in load_extrinsics and load_intrinsics. 

    Out: 
        Nothing. It generates a file on your working directory called output_video_start_end.mp4
    """

    frame_times_s = np.arange(start, end, (1/15))

    # Define the output file name, codec, frame rate, and dimensions
    output_file = f'output_video_{str(start)}_{str(end)}.mp4'
    codec = cv2.VideoWriter_fourcc(*'mp4v')  # Choose the appropriate codec
    frame_rate = 15  # Desired frame rate
    frame_width = 3600 #as measured from current matplotlib output, maybe better to change programatically to ensure robustness when changing
    #plotting function?
    frame_height = 3600

    # Create the video writer
    video_writer = cv2.VideoWriter(output_file, codec, frame_rate, (frame_width, frame_height))

    proj_data = lighthouse2aruco(trans_data)

    for i in frame_times_s:
        time_in_s = i
        onix_time = proj_data['clock'][0]+(time_in_s*250e6)
        fig = _plot_onix_time(onix_time, video_path, proj_data, cam_metadata, rvec, tvec, cameraMatrix, distCoeffs)
        image = plut.get_img_from_fig(fig)
        plt.close(fig)
        bgr_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        # Write the frame to the video
        video_writer.write(bgr_image)

    video_writer.release()
    cv2.destroyAllWindows()

def plot_onix_time_orientation(onix_time, video_path, trans_data, cam_metadata, processed_bno, heading, rvec, tvec, cameraMatrix, distCoeffs):
    """
    Makes a video of the mouse position overlaid on camera data for a certain time interval

    Args:
        onix_time(float): the onix time at which you want to plot a frame
        video_path(string): a complete path to the video file
        trans_data(pd): the Lighthouse transformed positions as generated by transform_data() in Calibrations
        cam_metadata(dict): a cam_metadata dict as generated from dio_conversion() or equivalent way of handling metadata
        processed_bno(dict): a processed_bno dictionary as outputted from onix.load_bno055()
        heading(np.array): the heading orientation in radians, as obtained in get_heading()
        rvec, tvec, cameraMatrix, distCoeffs: OpenCV objects generated in the extrinsic calibration of the camera and generated
        in load_extrinsics and load_intrinsics. 

    Out: 
        fig(fig): a matplotlib figure object
    """
    #obtaining the closest frame
    frame_start = get_onix_frame(video_path, onix_time, cam_metadata)
    #transforming coordinates
    proj_data = lighthouse2aruco(trans_data)
    #selecting index
    lighthouse_time = _find_nearest(proj_data['clock'], onix_time)
    index = proj_data[proj_data['clock']==lighthouse_time].index[0]
    objectPoints = np.array([proj_data['x'][index], proj_data['y'][index], proj_data['z'][index]])
    objectPoints = np.transpose(objectPoints)
    #Obtaining heading
    heading_time = _find_nearest(processed_bno['onix_time'], onix_time)
    heading_index = processed_bno['onix_time'].index[processed_bno['onix_time'].eq(heading_time)][0]
    current_heading = heading[heading_index]
    #Obtaining heading point
    heading_point_x = objectPoints[0]+100*math.cos(current_heading)
    heading_point_y = objectPoints[1]+100*math.sin(current_heading)
    heading_point = np.transpose(np.array([heading_point_x, heading_point_y, 0]))
    #transforming and plotting
    imagePoints, jacobian = cv2.projectPoints(objectPoints, rvec, tvec, cameraMatrix, distCoeffs)
    headingPoints, jacobian = cv2.projectPoints(heading_point, rvec, tvec, cameraMatrix, distCoeffs)
    coordinates, heading_coordinates = imagePoints[0][0], headingPoints[0][0]
    fig, axs = plt.subplots(1, 1, figsize=(50, 50))
    axs.imshow(frame_start)
    axs.scatter(coordinates[0], coordinates[1], marker='+', s=15e3)
    print(heading_coordinates)
    axs.plot([coordinates[0], heading_coordinates[0]], [coordinates[1], heading_coordinates[1]], lw=12, linestyle='solid', color = 'red')
    return fig

def plot_interval_orientation_s(start, end, video_path, trans_data, cam_metadata, processed_bno, heading, rvec, tvec, cameraMatrix, distCoeffs, normalise=False, offset=(math.pi+0.236332)):
    """
    Makes a video of the mouse position and orientation overlaid on camera data for a certain time interval

    Args:
        start(float): start time in s
        end(float): end time in s
        video_path(string): a complete path to the video file
        trans_data(pd): the Lighthouse transformed positions as generated by transform_data() in Calibrations
        cam_metadata(dict): a cam_metadata dict as generated from dio_conversion() or equivalent way of handling metadata
        processed_bno(dict): a processed_bno dictionary as outputted from onix.load_bno055()
        heading(np.array): the heading orientation in radians, as obtained in get_heading()
        rvec, tvec, cameraMatrix, distCoeffs: OpenCV objects generated in the extrinsic calibration of the camera and generated
        in load_extrinsics and load_intrinsics. 
        normalise(bool): whether or not to normalise the video to the brightness average of the first frame. 
        offset(float): the constant diference between the direction of heading indicated by the BNO and the direction of heading of the mouse. 
        Obviously, a different number for each implant. Defaults to the value of COWBOY in the helper function _plot_onix_time_orientation()

    Out: 
        Nothing. It generates a file on your working directory called output_orient_video_start_end.mp4
    """

    frame_times_s = np.arange(start, end, (1/15))

    # Define the output file name, codec, frame rate, and dimensions
    output_file = f'output_orient_video_{str(start)}_{str(end)}.mp4'
    codec = cv2.VideoWriter_fourcc(*'mp4v')  # Choose the appropriate codec
    frame_rate = 15  # Desired frame rate
    frame_width = 3600 #as measured from current matplotlib output, maybe better to change programatically to ensure robustness when changing
    #plotting function?
    frame_height = 3600
    avg_first_frame = get_avg_first_frame(video_path)

    # Create the video writer
    video_writer = cv2.VideoWriter(output_file, codec, frame_rate, (frame_width, frame_height))

    proj_data = lighthouse2aruco(trans_data)

    for i in frame_times_s:
        time_in_s = i
        onix_time = proj_data['clock'][0]+(time_in_s*250e6)
        fig = _plot_onix_time_orientation(onix_time, video_path, avg_first_frame, proj_data, cam_metadata, processed_bno, heading, rvec, tvec, cameraMatrix, distCoeffs, normalise = normalise, offset=offset)
        image = plut.get_img_from_fig(fig)
        plt.close(fig)
        bgr_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        # Write the frame to the video
        video_writer.write(bgr_image)

    video_writer.release()
    cv2.destroyAllWindows()

"""
FUNCTIONS USED TO ANALYSE HEADSTAGE INPUT


"""

def dio_conversion(camera_id, camera_metadata, data_path):
    """
    Converts the camera timestamps in the metadata of one camera to onix timestamps using the DIO inputs that are generated
    every time that such camera is triggered. This is the dirty way of converting camera timestamps to onix timestamps, before
    we had a master workflow that is able to timestamp every frame to the HARP. Kept here in case is needed again in the future. 

    Args: 
        camera_id(str): a camera id, such as 'cam1_camera'
        camera_metadata(dict): the metadata of the cameras in an acquisition, as generated by load_camera_times() at io-module.onix
        data_path(str or Path): the complete path to the ephys folder as generated by a recording. 
    Out:
        cam_metadata(dict): a dictionary, identical to the ones in camera_metadata but including another item, 'onix_timestamp'. 
    """
    data_path = Path(data_path)
    #import breakout board input
    processed_breakout = onix.load_breakout(data_path)
    dio = processed_breakout['dio']
    #filter dio input
    filtered_dio = dict()
    filtered_dio['Clock'], filtered_dio['DI0'] = prp.clean_di_channel(dio['Clock'], dio['DI0'])
    #import camera metadata
    cam_metadata = camera_metadata[camera_id]
    #evaluate difference between frames in breakout and frames in camera
    camera_frames = len(cam_metadata['frame_id'])
    breakout_frames = (len(filtered_dio['DI0']))/2
    print(f'cam2 has {camera_frames} frames')
    print(f'The breakout board accounts for {breakout_frames} frames')
    #write converted onix timestamp. 
    cam_metadata['onix_timestamp'] = _cam2onix(filtered_dio, cam_metadata)
    return cam_metadata


def lighthouse2aruco(trans_data):
    """
    Transforms data from the Lighthouse positioning system to coordinates usable in Aruco. Mostly, converts cm to mm and then substract the
    distance from the (0, 0, 0) of the lighthouse to the (0, 0, 0) of the Aruco. 

    Args:
        trans_data(pd): the Lighthouse transformed positions as generated by transform_data() in Calibrations
    Out:
        proj_data(pd): the Lighthouse position in mm and compensating for the center of the Aruco
    """
    #transforming coordinates
    proj_data = trans_data.copy(deep=True)
    proj_data['x'] = (trans_data['x']*10)
    proj_data['y'] = ((trans_data['y'])*10)+(44.5) #25 for half an aruco, 25 for the other half, 15 until the connector, 4.5 for one connector and a half
    proj_data['z'] = trans_data['z']*10 
    return proj_data

def get_heading(processed_bno):
    """
    Get the heading direction from the quaternion in radians. 

    Args:
        processed_bno(dict): a processed_bno dictionary as outputted from onix.load_bno055()
        
    Out:
        heading(np.array): the heading direction in radians
    """
    quaternion = processed_bno['quaternion']
    Qw = np.transpose(quaternion[:,0])
    Qx = np.transpose(quaternion[:,1])
    Qy = np.transpose(quaternion[:,2])
    Qz = np.transpose(quaternion[:,3])
    heading = np.zeros(len(Qw))
    for i in np.arange(0, len(Qw)):
        ith_quaternion = [[Qw[i], Qx[i], Qy[i], Qz[i]]]
        norm_quaternion = ith_quaternion / np.linalg.norm([Qw[i], Qx[i], Qy[i], Qz[i]])
        norm_quaternion = norm_quaternion[0]
        heading[i] = _get_heading(norm_quaternion[0], norm_quaternion[1],norm_quaternion[2],norm_quaternion[3])
    return heading










"""
HELPER FUNCTIONS


"""

def get_avg_first_frame(video_path):
    """
    Gets the average luminance value of the first frame of a video. Useful for luminance normalisation (when needed). 
    Normalisation never worked great for me, I include these functions for future reference. 

    Args:
        video_path(str): the complete path to the video
    Out:
        avg_first_frame(float): the average luminance on the first frame
    """
    video = cv2.VideoCapture(video_path)
    video.set(cv2.CAP_PROP_POS_FRAMES, 1)
    ret, frame_start = video.read()
    frame_start = cv2.cvtColor(frame_start, cv2.COLOR_BGR2GRAY)
    avg_first_frame = frame_start.mean()
    return(avg_first_frame)


def get_onix_frame(video_path, onix_time, cam_metadata, avg_first_frame=None, show=False, normalise = False):
    """
    Gets the camera frame for a corresponding onix time. Note that there must be some kind of camera to onix conversion
    in the cam_metadata object, like the one produced by dio_conversion() or some more recent method.

    Args:
        video_path(str): a complete path to the video. 
        onix_time(float): a time coordinate as produced by the onix clock. 
        cam_metadata(dict):  the metadata of the cameras in an acquisition, as generated by load_camera_times() at io-module.onix. 
        avg_first_frame(float): the average luminance value of the first frame of the video. Only needed if normalise==True. 
        show(Boole): wether or not to plot the frame once extracted. 
        normalise(Bool): whether or not to normalise the frame to the luminance of the first frame. 
    Out:
        frame_start(ndarray): a frame as read by VideoCapture.read() method
    
    """
    start_frame_onix = _find_nearest(cam_metadata['onix_timestamp'], onix_time)
    start_frame = cam_metadata[cam_metadata['onix_timestamp']==start_frame_onix]
    start_frame = start_frame.index[0]
    video = cv2.VideoCapture(video_path)
    video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
    ret, frame_start = video.read()
    if normalise==True:
        frame_start = frame_start*(avg_first_frame / frame_start.mean())#This never worked well
    if show==True:
        fig, axs = plt.subplots(1, 1, figsize=(50, 50))
        fig.suptitle(f'Frame {start_frame}', size = 40)
        axs.imshow(frame_start)
    return frame_start


def _cam2onix(filtered_dio, cam2_metadata):
    """
    Helper function to compute the conversion from cam time to onix time. 
    Takes the first and last DIO pulses and the first and last frames to estimate a linear relationship between
    camera timestamps and onix timestamps. 
    """

    onix_first = filtered_dio['Clock'][filtered_dio['DI0']==True][0]
    onix_last = filtered_dio['Clock'][filtered_dio['DI0']==True][-1]
    camera_first = cam2_metadata['camera_timestamp'].iloc[0]
    camera_last = cam2_metadata['camera_timestamp'].iloc[-1] 

    #y=intercept+slope*x

    intercept = onix_first
    slope = (onix_last-onix_first)/(camera_last-camera_first)

    onix_timestamp = intercept+slope*(cam2_metadata['camera_timestamp']-camera_first)

    return onix_timestamp

def _find_nearest(Array, x):
    dif_Array = np.absolute(Array-x) # use of absolute() function to find the difference 
    index = dif_Array.argmin() # find the index of minimum difference element
    return Array[index]

def _get_heading(Qw, Qx, Qy, Qz):
    heading = math.atan2(2.0 * (Qz * Qw + Qx * Qy) , - 1.0 + 2.0 * (Qw * Qw + Qx * Qx))
    return heading

def _plot_onix_time(onix_time, video_path, proj_data, cam_metadata, rvec, tvec, cameraMatrix, distCoeffs):
    """
    Same as plot_onix_time, but for efficiency I took out the transformation step to use it in generating the videos, so it works directly with
    proj_data. 
    """

    #obtaining the closest frame
    frame_start = get_onix_frame(video_path, onix_time, cam_metadata)
    #selecting index
    lighthouse_index = _find_nearest(proj_data['clock'], onix_time)
    index = proj_data[proj_data['clock']==lighthouse_index].index[0]
    objectPoints = np.array([proj_data['x'][index], proj_data['y'][index], proj_data['z'][index]])
    objectPoints = np.transpose(objectPoints)
    #transforming and plotting
    imagePoints, jacobian = cv2.projectPoints(objectPoints, rvec, tvec, cameraMatrix, distCoeffs)
    coordinates = imagePoints[0][0]
    fig, axs = plt.subplots(1, 1, figsize=(50, 50))
    axs.imshow(frame_start)
    axs.scatter(coordinates[0], coordinates[1], marker='+', s=15e3)
    return fig

def _plot_onix_time_orientation(onix_time, video_path, avg_first_frame, proj_data, cam_metadata, processed_bno, heading, rvec, tvec, cameraMatrix, distCoeffs, normalise=False, offset=(math.pi+0.236332)):
    """
    Same as plot_onix_time, but for efficiency I took out the transformation step to use it in generating the videos, so it works directly with
    proj_data. 
    """

    #obtaining the closest frame
    frame_start = get_onix_frame(video_path, onix_time, cam_metadata, avg_first_frame = avg_first_frame, normalise=normalise)
    #selecting index
    lighthouse_index = _find_nearest(proj_data['clock'], onix_time)
    index = proj_data[proj_data['clock']==lighthouse_index].index[0]
    objectPoints = np.array([proj_data['x'][index], proj_data['y'][index], proj_data['z'][index]])
    objectPoints = np.transpose(objectPoints)
    objectPoints[2]==0
    #Obtaining heading
    heading_time = _find_nearest(processed_bno['onix_time'], onix_time)
    heading_index = processed_bno['onix_time'].index[processed_bno['onix_time'].eq(heading_time)][0]
    if heading[heading_index]>0:
        current_heading = heading[heading_index]-(offset)#The offset looks like pi and 12 degrees, by eye
    else:
        current_heading = heading[heading_index]+(offset)
    #Obtaining heading point
    heading_point_x = objectPoints[0]+100*math.cos(current_heading)
    heading_point_y = objectPoints[1]+100*math.sin(current_heading)
    heading_point = np.transpose(np.array([heading_point_x, heading_point_y, 0]))
    #transforming and plotting
    imagePoints, jacobian = cv2.projectPoints(objectPoints, rvec, tvec, cameraMatrix, distCoeffs)
    headingPoints, jacobian = cv2.projectPoints(heading_point, rvec, tvec, cameraMatrix, distCoeffs)
    coordinates, heading_coordinates = imagePoints[0][0], headingPoints[0][0]
    fig, axs = plt.subplots(1, 1, figsize=(50, 50))
    axs.imshow(frame_start)
    axs.scatter(coordinates[0], coordinates[1], marker='+', s=15e3)
    axs.plot([coordinates[0], heading_coordinates[0]], [coordinates[1], heading_coordinates[1]], lw=12, linestyle='solid', color = 'red')
    return fig
